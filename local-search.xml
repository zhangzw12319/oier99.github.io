<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>论文十问:GASN(ECCV2022)</title>
    <link href="/2022/10/30/%E8%AE%BA%E6%96%87%E5%8D%81%E9%97%AE-GASN-ECCV2022/"/>
    <url>/2022/10/30/%E8%AE%BA%E6%96%87%E5%8D%81%E9%97%AE-GASN-ECCV2022/</url>
    
    <content type="html"><![CDATA[<h1 id="GASN-ECCV-2022"><a href="#GASN-ECCV-2022" class="headerlink" title="GASN(ECCV 2022)"></a>GASN(ECCV 2022)</h1><blockquote><p>论文标题: Efficient Point Cloud Segmentation with Geometry-aware Sparse Networks<br><br>论文地址: <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136990193.pdf">PDF</a><br><br>作者单位: HKUST, DeepRoute<br>代码地址: <a href="https://github.com/ItIsFriday/PcdSeg">https://github.com/ItIsFriday/PcdSeg</a></p></blockquote><h2 id="Q1-论文试图解决什么问题？"><a href="#Q1-论文试图解决什么问题？" class="headerlink" title="Q1 论文试图解决什么问题？"></a>Q1 论文试图解决什么问题？</h2><p>本文提出一个新的3D稀疏网络框架，可用于室外场景大规模自动驾驶数据集，在满足SOTA精度的前提下，有非常快的速度，更少的内存消耗，满足更好的实时性。</p><h2 id="Q2-这是否是一个新的问题？"><a href="#Q2-这是否是一个新的问题？" class="headerlink" title="Q2 这是否是一个新的问题？"></a>Q2 这是否是一个新的问题？</h2><p>不是，很多工作都在尝试做这个</p><h2 id="Q3-这篇文章要验证一个什么科学假设？"><a href="#Q3-这篇文章要验证一个什么科学假设？" class="headerlink" title="Q3 这篇文章要验证一个什么科学假设？"></a>Q3 这篇文章要验证一个什么科学假设？</h2><ol><li>本文认为保证精度的关键在于对点云multi-scale特征的充分利用</li><li>本文认为保证速度并且降低显存的关键，是放弃基于点级别的操作(pooling), 网络设计全部基于sparse voxel-based representation。比如稀疏卷积，池化，多特征融合，MLP均在该稀疏特征层次上完成。</li></ol><h2 id="Q4-有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？"><a href="#Q4-有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？" class="headerlink" title="Q4 有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？"></a>Q4 有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？</h2><ol><li>基于Raw Point的网络: PointNet, PointNet++ (maintain and utilize the pointwise geometry).</li><li>通过采样降低点的数量，提高效率: RandLA， KPConv.（但是采样会导致极大的信息损失，这些网络在室外场景精度表现不高）</li><li>划分&#x2F;投影到预先设定的Grids里(如2D, 3D, Sparse 3D)，再进行卷积&#x2F;稀疏卷积操作：AF2S3Net, Cylinder3D, SECOND, PointPillar…4.Point, Voxel, Range等多种representation融合感知的网络: PVCNN, SPVCNN, DRINet, RPVNet</li><li>基于图的(主要室内用得多，大规模室外计算速度&#x2F;显存开销偏大)</li><li>基于transformer的</li></ol><h2 id="Q5-论文中提到的解决方案之关键是什么？"><a href="#Q5-论文中提到的解决方案之关键是什么？" class="headerlink" title="Q5 论文中提到的解决方案之关键是什么？"></a>Q5 论文中提到的解决方案之关键是什么？</h2><ol><li>整体网络由Sparse Feature Encoder(SFE)和Sparse Geometry Feature Enhancement(SGFE)两个模块组成。前者用稀疏卷积提取特征后送入SGFE； SGFE设计了多尺度稀疏特征投影模块(Multi-scale Sparse Projection)来增强对几何信息的提取，将多尺度融合后的特征送到下一层的SFE。</li><li>相比于PVCNN，SPVCNN, DRINet等point+voxel的表征方式，作者只保留了sparse-voxel representation送入SFE和SGFE，不需要point-wise representation，大大减小了计算开销与显存开销。</li><li>point-wise representation 能完整地提取点云的局部几何信息。为了弥补放弃使用point representation带来的几何信息感知缺失， 作者设计了多尺度稀疏特征投影模块(Multi-scale Sparse Projection)来增强对几何信息的提取。认为不同的尺度下提取的特征是对点云结构的先验知识学习，在不同层次上获取并编码点云的几何特征。</li></ol><h2 id="Q6-论文中的实验是如何设计的？"><a href="#Q6-论文中的实验是如何设计的？" class="headerlink" title="Q6 论文中的实验是如何设计的？"></a>Q6 论文中的实验是如何设计的？</h2><p>主要做的语义分割实验，比较mIoU, 参数量，显存占用和速度</p><h2 id="Q7-用于定量评估的数据集是什么？代码有没有开源？"><a href="#Q7-用于定量评估的数据集是什么？代码有没有开源？" class="headerlink" title="Q7 用于定量评估的数据集是什么？代码有没有开源？"></a>Q7 用于定量评估的数据集是什么？代码有没有开源？</h2><p>nuScenes, semanticKitti.</p><h2 id="Q8-论文中的实验及结果有没有很好地支持需要验证的科学假设？"><a href="#Q8-论文中的实验及结果有没有很好地支持需要验证的科学假设？" class="headerlink" title="Q8 论文中的实验及结果有没有很好地支持需要验证的科学假设？"></a>Q8 论文中的实验及结果有没有很好地支持需要验证的科学假设？</h2><p>SemanticKitti和nuScenes，在mIoU和Cylinder3D, RPVNet, AF2S3net等SOTA网络基本持平的基础上，速度快2-5倍，显存占用量少2-3倍，参数量少2-10倍，仅一张2080Ti能跑的很好，性能优异</p><h2 id="Q9-这篇论文到底有什么贡献？"><a href="#Q9-这篇论文到底有什么贡献？" class="headerlink" title="Q9 这篇论文到底有什么贡献？"></a>Q9 这篇论文到底有什么贡献？</h2><p>提出了一个性能好，速度快，显存占用少，适用于大规模室外点云数据集的网络</p><h2 id="Q10-下一步呢？有什么工作可以继续深入？"><a href="#Q10-下一步呢？有什么工作可以继续深入？" class="headerlink" title="Q10 下一步呢？有什么工作可以继续深入？"></a>Q10 下一步呢？有什么工作可以继续深入？</h2><p>别卷了，卷不动了，有点强</p>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
      <category>点云</category>
      
      <category>通用点云理解与网络设计</category>
      
      <category>2022</category>
      
    </categories>
    
    
    <tags>
      
      <tag>论文十问系列</tag>
      
      <tag>ECCV2022</tag>
      
      <tag>Efficiency</tag>
      
      <tag>Sparse Voxel-Based Rrepresentation</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2022/10/30/hello-world/"/>
    <url>/2022/10/30/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p><p>推荐资料：</p><ol><li>如何搭配Fluid优雅的写一篇文档：<a href="https://hexo.fluid-dev.com/posts/fluid-write/">https://hexo.fluid-dev.com/posts/fluid-write/</a></li><li>使用ECharts插件绘制炫酷图表： <a href="https://hexo.fluid-dev.com/posts/hexo-echarts/">https://hexo.fluid-dev.com/posts/hexo-echarts/</a></li><li>给博客文章迁入PPT演示：<a href="https://hexo.fluid-dev.com/posts/hexo-nodeppt/">https://hexo.fluid-dev.com/posts/hexo-nodeppt/</a></li></ol><p>优雅，实在是太优雅了！</p><hr><p>下面我们开始不正经：</p><p>再次感谢Fluid官方和作者大大，为表感谢直接把你们的网站强行搬来套娃了😁</p><p>官方示例:</p><iframe src="https://hexo.fluid-dev.com/" width="100%" height="500" name="topFrame" scrolling="yes"  noresize="noresize" frameborder="0" id="topFrame"></iframe><hr><p>作者：</p><iframe src="https://zkqiang.cn/" width="100%" height="500" name="topFrame" scrolling="yes"  noresize="noresize" frameborder="0" id="topFrame"></iframe>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>LaTeX讲解系列：常用数学符号大全</title>
    <link href="/2022/05/24/LaTEX-math-symbols/"/>
    <url>/2022/05/24/LaTEX-math-symbols/</url>
    
    <content type="html"><![CDATA[<h1 id="LaTex-讲解系列：常用数学符号大全"><a href="#LaTex-讲解系列：常用数学符号大全" class="headerlink" title="LaTex 讲解系列：常用数学符号大全"></a>LaTex 讲解系列：常用数学符号大全</h1><div class="note note-primary">            <p>本文整理过程中着重参考了：</p><p><a href="https://www.cnblogs.com/yalphait/articles/8685586.html%EF%BC%88LaTex">https://www.cnblogs.com/yalphait/articles/8685586.html（LaTex</a> 命令符号大全）</p><p><a href="https://blog.csdn.net/u012684062/article/details/78398191">https://blog.csdn.net/u012684062/article/details/78398191</a> （LaTex 所有常用数学符号整理）</p><p>这两份非常全面，非常感谢！尤其第一份大佬的，还介绍了不同符号在数学，物理计算机等不同学科中用法(比如方程组中用希腊小写字母，向量用粗体小写，矩阵用粗体大写等)，还有很多美观排版的建议与事例，强烈安利阅读一番！</p><p>除此之外整合了其他网站提到的常用的公式命令，一并列在最后<a href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99">参考资料</a>中。</p>          </div><h2 id="不同的数学模式"><a href="#不同的数学模式" class="headerlink" title="不同的数学模式"></a>不同的数学模式</h2><h3 id="重音符号"><a href="#重音符号" class="headerlink" title="重音符号"></a>重音符号</h3><table><thead><tr><th>$\hat{a}$  <code>\hat&#123;a&#125;</code></th><th>$\check{a}$  <code>\check&#123;a&#125;</code></th><th>$\tilde{a}$  <code>\tilde&#123;a&#125;</code></th></tr></thead><tbody><tr><td>$\grave{a}$  <code>\grave&#123;a&#125;</code></td><td>$\dot{a}$  <code>\dot&#123;a&#125;</code></td><td>$\ddot{a}$  <code>\ddot&#123;a&#125;</code></td></tr><tr><td>$\bar{a}$  <code>\bar&#123;a&#125;</code></td><td>$\vec{a}$  <code>\vac&#123;a&#125;</code></td><td>$\widehat{A}$  <code>\widehat&#123;A&#125;</code></td></tr><tr><td>$\acute{a}$  <code>\acute&#123;a&#125;</code></td><td>$\breve{a}$ <code> \breve&#123;a&#125;</code></td><td>$\widetilde{A}$  <code>\widetilde&#123;A&#125;</code></td></tr></tbody></table><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><table><thead><tr><th>$\mathbf{E}$  <code>\mathbf&#123;E&#125;</code></th><th>$\mathbb{R}$ <code> \mathbb&#123;R&#125;</code></th><th>$\mathit{ABC}$  <code>\mathit&#123;ABC&#125;</code></th></tr></thead><tbody><tr><td>$\mathrm{ABC}$  <code>\mathrm&#123;ABC&#125;</code></td><td>$\mathfrak{ABC}$ <code>\mathfrak&#123;ABC&#125;</code></td><td></td></tr></tbody></table><h2 id="希腊字母"><a href="#希腊字母" class="headerlink" title="希腊字母"></a>希腊字母</h2><table><thead><tr><th>$\alpha$  <code>\alpha</code></th><th>$\theta$  <code>\theta</code></th><th>$o$  <code> o</code></th><th>$\upsilon$  <code>\upsilon</code></th></tr></thead><tbody><tr><td>$\beta$  <code>\beta</code></td><td>$\vartheta$  <code>\vartheta</code></td><td>$\pi$ <code> \pi</code></td><td>$\phi$  <code>\phi</code></td></tr><tr><td>$\gamma$  <code>\gamma</code></td><td>$\iota$  <code>\iota</code></td><td>$\varpi$  <code>\varpi</code></td><td>$\varphi$  <code>\varphi</code></td></tr><tr><td>$\delta$  <code>\delta</code></td><td>$\kappa$ <code> \kappa</code></td><td>$\rho$ <code> \rho</code></td><td>$\chi$  <code>\chi</code></td></tr><tr><td>$\epsilon$  <code>\epsilon</code></td><td>$\lambda$ <code> \lambda</code></td><td>$\varrho$  <code>\varrho</code></td><td>$\psi$  <code>\psi</code></td></tr><tr><td>$\varepsilon$  <code>\varepsilon</code></td><td>$\mu$  <code>\mu</code></td><td>$\sigma$  <code>\sigma</code></td><td>$\omega$  <code>\omega</code></td></tr><tr><td>$\zeta$  <code>\zeta</code></td><td>$\nu$  <code>\nu</code></td><td>$\varsigma$  <code>\varsigma</code></td><td></td></tr><tr><td>$\eta$  <code>\eta</code></td><td>$\xi$  <code>\xi</code></td><td>$\tau$  <code>\tau</code></td><td></td></tr><tr><td>$\Gamma$  <code>\Gamma</code></td><td>$\Lambda$ <code> \Lambda</code></td><td>$\Sigma$  <code>\Sigma</code></td><td>$\Psi$  <code>\Psi</code></td></tr><tr><td>$\Theta$  <code>\Theta</code></td><td>$\Pi$ <code> \Pi</code></td><td>$\Phi$  <code>\Phi</code></td><td></td></tr></tbody></table><h2 id="运算符"><a href="#运算符" class="headerlink" title="运算符"></a>运算符</h2><h3 id="关系符号"><a href="#关系符号" class="headerlink" title="关系符号"></a>关系符号</h3><p>可以在下列符号的相应命令前加上<code>\not</code> 命令，得到其否定形式，例如$\in$ 与$\not \in$ 。</p><table><thead><tr><th>$\leq$  <code>\le</code></th><th>$\ge$  <code>\ge</code></th><th>$\equiv$  <code>\equiv</code></th><th>$\ll$  <code>\ll</code></th></tr></thead><tbody><tr><td>$\gg$  <code>\gg</code></td><td>$\doteq$  <code>\doteq</code></td><td>$\prec$  <code>\prec</code></td><td>$\succ$  <code>\succ</code></td></tr><tr><td>$\simeq$  <code>\simeq</code></td><td>$\subset$  <code>\subset</code></td><td>$\supset$  <code>\supset</code></td><td>$\approx$  <code>\approx</code></td></tr><tr><td>$\subseteq$ <code> \subseteq</code></td><td>$\supseteq$  <code>\supseteq</code></td><td>$\cong$  <code>\cong</code></td><td>$\in$  <code>\in</code></td></tr><tr><td>$\vdash$  <code>\vdash</code></td><td>$\dashv$  <code>\dashv</code></td><td></td><td></td></tr></tbody></table><h3 id="运算符-1"><a href="#运算符-1" class="headerlink" title="运算符"></a>运算符</h3><table><thead><tr><th>$\pm$  <code>\pm</code></th><th>$\mp$  <code>\mp</code></th><th>$\cdot$  <code>\cdot</code></th><th>$\div$  <code>\div</code></th></tr></thead><tbody><tr><td>$\times$  <code>\times</code></td><td>$\setminus$  <code>\setminus</code></td><td>$\star$  <code>\star</code></td><td>$\cup$  <code>\cup</code></td></tr><tr><td>$\cap$  <code>\cap</code></td><td>$\ast$  <code>\ast</code></td><td>$\circ$  <code>\circ</code></td><td>$\lor$ <code> \lor</code></td></tr><tr><td>$\land$  <code>\land</code></td><td>$\oplus$  <code>\oplus</code></td><td>$\ominus$  <code>\ominus</code></td><td>$\diamond$  <code>\diamond</code></td></tr><tr><td>$\otimes$  <code>\otimes</code></td><td>$\odot$  <code>\odot</code></td><td>$\oslash$  <code>\oslash</code></td><td>$\amalg$ <code> \amalg</code></td></tr><tr><td>$\bigtriangleup$  <code>\bigtrianglep</code></td><td>$\bigtriangledown$  <code>\bigtriangledown</code></td><td>$\dagger$  <code>\dagger</code></td><td>$\ddagger$  <code>\ddagger</code></td></tr><tr><td>$\wr$  <code>\wr</code></td><td>$\lnot$ <code>\not</code></td><td></td><td></td></tr></tbody></table><h3 id="”大“运算符"><a href="#”大“运算符" class="headerlink" title="”大“运算符"></a>”大“运算符</h3><table><thead><tr><th>$\sum_i^j$  <code>\sum_i^&#123;j&#125;</code></th><th>$\prod$ <code> \prod</code></th><th>$\bigcup$  <code>\bigcup</code></th><th>$\bigcap$  <code>\bigcap</code></th></tr></thead><tbody><tr><td>$\bigvee$  <code>\bigvee</code></td><td>$\bigwedge$ <code>\bigwedge</code></td><td>$\int_a^b$  <code>\int_a^b</code></td><td>$\oint$  <code>\oint</code></td></tr><tr><td>$\bigoplus$  <code>\bigoplus</code></td><td>$\bigotimes$  <code>\bigotimes</code></td><td>$\bigodot$  <code>\bigodot</code></td><td></td></tr></tbody></table><h3 id="箭头"><a href="#箭头" class="headerlink" title="箭头"></a>箭头</h3><table><thead><tr><th>$\leftarrow$  <code>\leftarrow</code></th><th>$\longleftarrow$  <code>\longleftarrow</code></th><th>$\rightarrow$  <code>\rightarrow</code></th><th>$\longrightarrow$  <code>\longrightarrow</code></th></tr></thead><tbody><tr><td>$\leftrightarrow$  <code>\leftrightarrow</code></td><td>$\longleftrightarrow$  <code>\longleftrightarrrow</code></td><td>$\Leftarrow$ <code> \Leftarrow</code></td><td>$\Rightarrow$  <code>\Rightarrow</code></td></tr><tr><td>$\Leftrightarrow$  <code>\Leftrightarrow</code></td><td>$\iff$  <code>\iff(bigger spaces)</code></td><td>$\mapsto$ <code>\mapsto</code></td><td>$\rightleftharpoons$  <code>\rightleftharpoons</code></td></tr><tr><td>$\leadsto$  <code>\leadsto</code></td><td></td><td></td><td></td></tr></tbody></table><h3 id="常用函数符号"><a href="#常用函数符号" class="headerlink" title="常用函数符号"></a>常用函数符号</h3><table><thead><tr><th>$\sin \theta$  <code>\sin \theta</code></th><th>$\cos\theta$  <code>\cos\theta</code></th><th>$\tan\theta$  <code>\tan\theta</code></th><th>$\arcsin$ <code>\arcsin</code></th></tr></thead><tbody><tr><td>$\arccos$ <code>\arccos</code></td><td>$\cosh$  <code>\cosh</code></td><td>$\tanh$  <code>\tanh</code></td><td>$\limsup$ <code>\limsup</code></td></tr><tr><td>$f’(x) &#x3D; \lim_{\Delta x \rightarrow 0} \frac{f(x + \Delta x) - f(x)}{\Delta x}$  <code>\lim_&#123;\Delta x \rightarrow 0&#125; \frac&#123;f(x + \Delta x) - f(x)&#125;&#123;\Delta x&#125;</code></td><td>$\max$  <code>\max</code></td><td>$\min$ <code> \min</code></td><td>$\inf$  <code>\inf</code></td></tr><tr><td>$\log$  <code>\log</code></td><td>$\ln$  <code>\ln</code></td><td>$\ker$  <code>\ker</code></td><td>$\Pr$  <code>\Pr</code></td></tr><tr><td>$\dim$  <code>\dim</code></td><td>$\det$  <code>\det</code></td><td>$\exp$  <code>\exp</code></td><td></td></tr></tbody></table><h3 id="其他-1"><a href="#其他-1" class="headerlink" title="其他"></a>其他</h3><table><thead><tr><th>$\lvert a \rvert$  <code>\lvert a \rvert</code></th><th>$\lVert a \rVert$ <code> \lVert a \rVert</code></th><th>$\dots$  <code>\dots</code></th><th>$\cdots$  <code>\cdots</code>(for matrix)</th></tr></thead><tbody><tr><td>$\vdots$  <code>\vdots</code></td><td>$\ddots$  <code>\ddots</code></td><td>$\forall$  <code>\forall</code></td><td>$\exists$  <code>\exists</code></td></tr><tr><td>$\partial$  <code>\partial</code></td><td>$\nabla$  <code>\nabla</code></td><td>$\bot$  <code>\bot</code></td><td>$\top$  <code>\top</code></td></tr><tr><td>$\angle$  <code>\angle</code></td><td>$\surd$  <code>\surd</code></td><td>$\emptyset$  <code>\emptyset</code></td><td>$\ell$  <code>\ell</code></td></tr><tr><td>$\infty$ <code>\infty</code></td><td>$\heartsuit$  <code>\heartsuit</code></td><td>$\clubsuit$  <code>\clubsuit</code></td><td>$\spadesuit$  <code>\spadesuit</code></td></tr><tr><td>$\therefore$  <code>\therefore</code></td><td>$\because$  <code>\because</code></td><td>$\mathop{\arg\min}_{\theta}$   <code>\mathop&#123;\arg\min&#125;_&#123;\theta&#125;</code>(套在LaTeX equation环境下，下标$\theta$能自动在字母下面)</td><td></td></tr></tbody></table><h2 id="矩阵表示"><a href="#矩阵表示" class="headerlink" title="矩阵表示"></a>矩阵表示</h2><p>1. </p><p>$$<br>\begin{align}<br>    \left[ \begin{matrix}<br>  a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n} \<br>  a_{21} &amp; a_{22} &amp; \cdots &amp; a_{25} \<br>  \vdots &amp; \vdots &amp; \ddots &amp; \vdots \<br>  a_{n1} &amp; a_{n2} &amp; \cdots &amp; a_{nn}<br>  \end{matrix}\right]<br>\end{align}<br>$$</p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs latex"><span class="hljs-keyword">\begin</span>&#123;align&#125; <span class="hljs-comment">% 推荐用align或equation的数学环境</span><br><span class="hljs-keyword">\left</span>[ <span class="hljs-keyword">\begin</span>&#123;matrix&#125; <span class="hljs-comment">% 推荐使用matrix的环境, 如果使用array环境需要指定&#123;ccc&#125;列数</span><br>  a<span class="hljs-built_in">_</span>&#123;11&#125; <span class="hljs-built_in">&amp;</span> a<span class="hljs-built_in">_</span>&#123;12&#125; <span class="hljs-built_in">&amp;</span> <span class="hljs-keyword">\cdots</span> <span class="hljs-built_in">&amp;</span> a<span class="hljs-built_in">_</span>&#123;1n&#125; <span class="hljs-keyword">\\</span><br>  a<span class="hljs-built_in">_</span>&#123;21&#125; <span class="hljs-built_in">&amp;</span> a<span class="hljs-built_in">_</span>&#123;22&#125; <span class="hljs-built_in">&amp;</span> <span class="hljs-keyword">\cdots</span> <span class="hljs-built_in">&amp;</span> a<span class="hljs-built_in">_</span>&#123;25&#125; <span class="hljs-keyword">\\</span><br>  <span class="hljs-keyword">\vdots</span> <span class="hljs-built_in">&amp;</span> <span class="hljs-keyword">\vdots</span> <span class="hljs-built_in">&amp;</span> <span class="hljs-keyword">\ddots</span> <span class="hljs-built_in">&amp;</span> <span class="hljs-keyword">\vdots</span> <span class="hljs-keyword">\\</span><br>  a<span class="hljs-built_in">_</span>&#123;n1&#125; <span class="hljs-built_in">&amp;</span> a<span class="hljs-built_in">_</span>&#123;n2&#125; <span class="hljs-built_in">&amp;</span> <span class="hljs-keyword">\cdots</span> <span class="hljs-built_in">&amp;</span> a<span class="hljs-built_in">_</span>&#123;nn&#125;<br>  <span class="hljs-keyword">\end</span>&#123;matrix&#125;<span class="hljs-keyword">\right</span>] <span class="hljs-comment">% \left[ 与\right]配对，[还可以换成&#123;, (等 </span><br><span class="hljs-keyword">\end</span>&#123;align&#125;<br><br><span class="hljs-comment">% 如使用nicematrix宏包，需要在完整版的LaTex环境中引用该包</span><br><span class="hljs-comment">% 详见官方连接https://ctan.org/pkg/nicematrix</span><br></code></pre></td></tr></table></figure><ol start="2"><li>上下大括号</li></ol><p>$$<br>\begin{align}<br>&amp;\begin{matrix} 5050 \ \overbrace{ 1+2+\cdots+100 }\end{matrix} \<br>\<br>&amp;\begin{matrix} \underbrace{ a+b+\cdots+z } \ 26\end{matrix}<br>\end{align}<br>$$</p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs latex"><span class="hljs-keyword">\begin</span>&#123;align&#125;<br><span class="hljs-built_in">&amp;</span><span class="hljs-keyword">\begin</span>&#123;matrix&#125; 5050 <span class="hljs-keyword">\\</span> <span class="hljs-keyword">\overbrace</span>&#123; 1+2+<span class="hljs-keyword">\cdots</span>+100 &#125;<span class="hljs-keyword">\end</span>&#123;matrix&#125; <span class="hljs-keyword">\\</span><br><span class="hljs-keyword">\\</span><br><span class="hljs-built_in">&amp;</span><span class="hljs-keyword">\begin</span>&#123;matrix&#125; <span class="hljs-keyword">\underbrace</span>&#123; a+b+<span class="hljs-keyword">\cdots</span>+z &#125; <span class="hljs-keyword">\\</span> 26<span class="hljs-keyword">\end</span>&#123;matrix&#125;<br><br><span class="hljs-keyword">\end</span>&#123;align&#125;<br></code></pre></td></tr></table></figure><ol start="3"><li>方程组&#x2F;分段函数</li></ol><h1 id=""><a href="#" class="headerlink" title="$$"></a>$$</h1><p>\begin{cases}<br>3x + 5y +  z, &amp; x+y+z &lt;1\<br>7x - 2y + 4z, &amp; 1\le x+y+z &lt;5\<br>-6x + 3y + 2z, &amp; x+y+z &gt; 1<br>\end{cases}<br>$$</p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs latex">=<br><span class="hljs-keyword">\begin</span>&#123;cases&#125;<br>3x + 5y +  z, <span class="hljs-built_in">&amp;</span> x+y+z &lt;1<span class="hljs-keyword">\\</span><br>7x - 2y + 4z, <span class="hljs-built_in">&amp;</span> 1<span class="hljs-keyword">\le</span> x+y+z &lt;5<span class="hljs-keyword">\\</span><br>-6x + 3y + 2z, <span class="hljs-built_in">&amp;</span> x+y+z &gt; 1 <br><span class="hljs-keyword">\end</span>&#123;cases&#125;<br></code></pre></td></tr></table></figure><ol start="4"><li>数组</li></ol><p>$$<br>\begin{array}{|c|c||c|} a &amp; b &amp; S \<br>\hline<br>0&amp;0&amp;1\<br>0&amp;1&amp;1\<br>1&amp;0&amp;1\<br>1&amp;1&amp;0\<br>\end{array}<br>$$</p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs latex"><span class="hljs-keyword">\begin</span>&#123;array&#125;&#123;|c|c||c|&#125; a <span class="hljs-built_in">&amp;</span> b <span class="hljs-built_in">&amp;</span> S <span class="hljs-keyword">\\</span><br><span class="hljs-keyword">\hline</span><br>0<span class="hljs-built_in">&amp;</span>0<span class="hljs-built_in">&amp;</span>1<span class="hljs-keyword">\\</span><br>0<span class="hljs-built_in">&amp;</span>1<span class="hljs-built_in">&amp;</span>1<span class="hljs-keyword">\\</span><br>1<span class="hljs-built_in">&amp;</span>0<span class="hljs-built_in">&amp;</span>1<span class="hljs-keyword">\\</span><br>1<span class="hljs-built_in">&amp;</span>1<span class="hljs-built_in">&amp;</span>0<span class="hljs-keyword">\\</span><br><span class="hljs-keyword">\end</span>&#123;array&#125;<br></code></pre></td></tr></table></figure><p>(未完待续，动态补充)</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="https://www.cnblogs.com/yalphait/articles/8685586.html">https://www.cnblogs.com/yalphait/articles/8685586.html</a></li><li><a href="https://zhuanlan.zhihu.com/p/266267223">https://zhuanlan.zhihu.com/p/266267223</a></li><li><a href="https://blog.csdn.net/u012684062/article/details/78398191">https://blog.csdn.net/u012684062/article/details/78398191</a></li></ol>]]></content>
    
    
    <categories>
      
      <category>LaTex学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LaTex</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文十问:Map-view Transformer(CVPR2022)</title>
    <link href="/2022/05/24/%E8%AE%BA%E6%96%87%E5%8D%81%E9%97%AE-map-view-transformer-CVPR2022/"/>
    <url>/2022/05/24/%E8%AE%BA%E6%96%87%E5%8D%81%E9%97%AE-map-view-transformer-CVPR2022/</url>
    
    <content type="html"><![CDATA[<h1 id="Cross-view-Transformers-for-real-time-Map-view-Semantic-Segmentation-CVPR-2022-2"><a href="#Cross-view-Transformers-for-real-time-Map-view-Semantic-Segmentation-CVPR-2022-2" class="headerlink" title="Cross-view Transformers for real-time Map-view Semantic Segmentation(CVPR 2022)[2]"></a>Cross-view Transformers for real-time Map-view Semantic Segmentation(CVPR 2022)[2]</h1><blockquote><p>论文标题：Cross-view Transformers for real-time Map-view Semantic Segmentation(CVPR 2022)<br><br>论文地址：<a href="https://arxiv.org/abs/2205.02833">https://arxiv.org/abs/2205.02833</a><br><br>作者单位：The Chinese University of Hong Kong<br><br>代码地址：<a href="https://github.com/bradyz/cross_view_transformers">https://github.com/bradyz/cross_view_transformers</a><br><br>一句话读论文：Our architecture implicitly learns a mapping from individual camera views into a canonical map-view representation using a camera-aware cross-view attention mechanism.</p></blockquote><h2 id="Q1"><a href="#Q1" class="headerlink" title="Q1"></a>Q1</h2><p>论文试图解决什么问题？</p><p>做图像特征与地图特征的融合(“model geometry and relationships between different view and a canonical map representation”)</p><h2 id="Q2"><a href="#Q2" class="headerlink" title="Q2"></a>Q2</h2><p>这是否是一个新的问题？</p><p>利用地图信息作为query，参与语义分割网络的跨视图融合，是一个有新意的做法</p><h2 id="Q3"><a href="#Q3" class="headerlink" title="Q3"></a>Q3</h2><p>这篇文章要验证一个什么科学假设？</p><h2 id="Q4"><a href="#Q4" class="headerlink" title="Q4"></a>Q4</h2><p>有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？</p><p>做俯视图语义分割，已有方法大致可归为如下两类(包含其存在的问题)<br>Image-based depth estimation are error-prone.<br>Depth-based projections are a fairly inflexible and rigid bottleneck to map between views.</p><p>附一份知乎笔记连接：<a href="https://zhuanlan.zhihu.com/p/511477453">https://zhuanlan.zhihu.com/p/511477453</a></p><h2 id="Q5"><a href="#Q5" class="headerlink" title="Q5"></a>Q5</h2><p>论文中提到的解决方案之关键是什么？</p><p>通过cross-view transoformer来做Camera View到Map View的融合。相比于已有方法基于显式地几何关系地映射，这种融合的方式是一种隐式函数的映射(“learn any geometric transformation implicitly and directly from data”)。此外，transformer需要positional embedding来区分不同空间位置的特征。本文因此设计了camera-aware和map-view两类positional embedding。</p><h2 id="Q6"><a href="#Q6" class="headerlink" title="Q6"></a>Q6</h2><p>论文中的实验是如何设计的？</p><h2 id="Q7"><a href="#Q7" class="headerlink" title="Q7"></a>Q7</h2><p>用于定量评估的数据集是什么？代码有没有开源？</p><p>nuScenes, 已经开源</p><h2 id="Q8"><a href="#Q8" class="headerlink" title="Q8"></a>Q8</h2><p>论文中的实验及结果有没有很好地支持需要验证的科学假设？</p><p>在俯视图的语义分割中是SOTA(37.5% mIoU), 和基于深度估计与投影等已有方法相比comaprable。但是整个赛道与图像语义分割与3D语义分割结果相比(70-80mIoU)，整体后面还有挖掘空间</p><h2 id="Q9"><a href="#Q9" class="headerlink" title="Q9"></a>Q9</h2><p>这篇论文到底有什么贡献？</p><p>1）利用了地图信息，这是一个比较有新意的setting.<br>2）注意力系数计算方式比较有新意，可以参考拓展</p><h2 id="Q10"><a href="#Q10" class="headerlink" title="Q10"></a>Q10</h2><p>下一步呢？有什么工作可以继续深入？</p>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
      <category>点云</category>
      
      <category>多模态融合</category>
      
      <category>2022</category>
      
    </categories>
    
    
    <tags>
      
      <tag>一句话读论文系列</tag>
      
      <tag>CVPR2022</tag>
      
      <tag>点云语义分割</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>一句话读论文:Map-view Transformer(CVPR2022)</title>
    <link href="/2022/05/24/%E4%B8%80%E5%8F%A5%E8%AF%9D%E8%AF%BB%E8%AE%BA%E6%96%87-map-view-transformer-CVPR2022/"/>
    <url>/2022/05/24/%E4%B8%80%E5%8F%A5%E8%AF%9D%E8%AF%BB%E8%AE%BA%E6%96%87-map-view-transformer-CVPR2022/</url>
    
    <content type="html"><![CDATA[<h1 id="Cross-view-Transformers-for-real-time-Map-view-Semantic-Segmentation-CVPR-2022-1"><a href="#Cross-view-Transformers-for-real-time-Map-view-Semantic-Segmentation-CVPR-2022-1" class="headerlink" title="Cross-view Transformers for real-time Map-view Semantic Segmentation(CVPR 2022)[1]"></a>Cross-view Transformers for real-time Map-view Semantic Segmentation(CVPR 2022)[1]</h1><blockquote><p>论文标题：Cross-view Transformers for real-time Map-view Semantic Segmentation(CVPR 2022)<br><br>论文地址：<a href="https://arxiv.org/abs/2205.02833">https://arxiv.org/abs/2205.02833</a><br><br>作者单位：The Chinese University of Hong Kong<br><br>代码地址：[<a href="https://github.com/bradyz/">https://github.com/bradyz/</a> cross_view_transformers<br>](<a href="https://github.com/bradyz/">https://github.com/bradyz/</a> cross_view_transformers)<br>一句话读论文：Our architecture implicitly learns a mapping from individual camera views into a canonical map-view representation using a camera-aware cross-view attention mechanism. </p></blockquote><h2 id="网络框架："><a href="#网络框架：" class="headerlink" title="网络框架："></a>网络框架：</h2><p><img src="https://s2.loli.net/2022/05/24/okSpGdcwzAjVeiu.png" alt="image-20220523111833587"></p><h2 id="核心内容："><a href="#核心内容：" class="headerlink" title="核心内容："></a><strong>核心内容：</strong></h2><p>​Motivation: 想做图像特征与地图特征的融合(“model geometry and relationships between different view and a canonical map representation”)</p><p>​已有方法的问题：</p><ul><li>Image-based depth estimation are error-prone.</li><li>Depth-based projections are a fairly inflexible and rigid bottleneck to map between views.</li></ul><p>​本文的思路：</p><p>​是通过cross-view transoformer来做Map View 到Camera View的融合。相比于已有方法基于显式地几何关系地映射，这种融合的方式是一种隐式函数的映射(“learn any geometric transformation implicitly and directly from data”)。此外，transformer需要positional embedding来区分不同空间位置的特征。</p><p>​注意力系数的计算方式比较巧妙：<br>$$<br>x^{(I)} \simeq K_k R_k(x^{(W)} - t_k)<br>$$<br>​上述公式展示了世界坐标系$x^{(W)}$与相机坐标系$x^{(I)}$的映射关系，其中$t_k$是车辆在行驶时的平移，$K_k$与$R_k$分别是相机内参矩阵和外参旋转矩阵(相机位姿相对于LiDAR传感器位姿)。<br>$$<br>sim_k(x^{(I)}, x^{(W)}) &#x3D; \frac{(R_K^{-1}K_K^{-1}x^{(I)})\cdot (x^{(W)} - t_k)}{\Vert (R_K^{-1}K_K^{-1}x^{(I)})\Vert \Vert (x^{(W)} - t_k)\Vert}<br>$$<br>​这样将3D坐标点和2D图像对应的坐标联系起来，计算其相似度系数。由于本文中没有3D LiDAR点云数据参与，所以从地图中只能获得xy坐标的信息，无法获得高度(深度)信息。所以这里用的是经过MLP后提取点特征代替了直接使用坐标。</p><blockquote><p>几何意义：The uprojected image coordinate $d_{k,i} &#x3D; R_k^{-1} K_k^{-1 }x_i^{(I)} $ for each image coordinate $x_i^{(I)}$ described a direction vector from the origin $t_k$ of camera $k$ to the image plane at depth 1.</p><p>代码实现：1)We encode this direction vector $d_{k,i}$ using an MLP(shared across k views) into a D-dimensional positional embedding $\delta_{k,i} \in \mathbb{R}^D$… We combine this positional embedding with image features $\phi_{k,i}$ in the keys of our cross-view attention mechanism.</p><p>2)$x^{W}$ 从地图获得，没有高度信息怎么办：We start with a learned positional encoding $c^{(0)}\in \mathbb{R}^{w \times h \times D}$. We build the map-view representation up over multiple iterations in our transformer… Each positional embedding is better able to project the map-view coordinates into a proxy of the 3D environment.</p></blockquote><p>基于上述几何映射的注意力系数计算方式，论文做如下改动作为实际的计算方法：</p><p>$$<br>sim(\delta_{k,i}, \phi_{k,i}, c_j^{n}, \tau_k) &#x3D; \frac{(\delta_{k,i}+ \phi_{k,i})\cdot(c_j^{(n)} - \tau_k)}{\Vert \delta_{k,i}+ \phi_{k,i} \Vert \Vert c_j^{(n)} - \tau_k \Vert }<br>$$</p><p>​ $\delta_{k,j}$表示图像的Camera-view positional embedding; $\phi_{k,i} &#x3D; MLP[d_{k,i}]&#x3D;MLP[(R_K^{-1}K_K^{-1}x^{(I)})]$表示经过MLP映射后的$D$维向量。相比于原始的计算方式，这里把positional embedding和图像feature加在一起参与运算。$c_j^{(n)}$和$\tau_k$意义和原始公式相同，不同之处是他们也是经过Transforemr和MLP映射到$D$维向量。</p><h2 id="实验结果："><a href="#实验结果：" class="headerlink" title="实验结果："></a><strong>实验结果：</strong></h2><p><img src="https://s2.loli.net/2022/05/23/sk2fGHMORJ8UjWX.png" alt="image-20220523135309538"></p><p><img src="https://s2.loli.net/2022/05/23/wdDPYJWxlhzFv8s.png" alt="image-20220523135440759"></p><p><img src="https://s2.loli.net/2022/05/23/Y84bdTZ75nNxlJ2.png" alt="image-20220523135515717"></p><p><img src="https://s2.loli.net/2022/05/23/z4uAlfWCjPGHJ7v.png" alt="image-20220523140115644"></p><h2 id="Related-Work-可选"><a href="#Related-Work-可选" class="headerlink" title="Related Work(可选):"></a><strong>Related Work(可选):</strong></h2><p>- </p><h2 id="你认为优点-x2F-不足-x2F-可以拓展改进的地方-可选"><a href="#你认为优点-x2F-不足-x2F-可以拓展改进的地方-可选" class="headerlink" title="你认为优点&#x2F;不足&#x2F;可以拓展改进的地方(可选):"></a><strong>你认为优点&#x2F;不足&#x2F;可以拓展改进的地方(可选):</strong></h2><p>优点：</p><ul><li>利用了地图信息，这是一个比较有新意的setting.</li><li>提出的transformer不是简单的高维特征计算cos复杂度。从理论上来说，本文是巧妙利用三维点与二维相机视角存在的仿射变换的关系构建的计算方法。原始公式中只需要坐标信息 ，然后在此基础上拓展了图像和地图的positional embedding以及各自的特征信息(如transformer得到的特征向量，图像颜色，点云的极坐标信息等)，可拓展性强。</li></ul><p>不足：</p><ul><li>本文中没有用到LiDAR点云数据。讲道理从LiDAR数据中可以直接获得准确的高度信息，为什么不用呢(是为了提高模型推理速度，避免使用点云数据?)。这样的话从地图中经过多层transformer来估计特征向量的方式总觉得有瑕疵。</li><li>实验评测在语义分割上做的，但是做的是俯视图语义分割而不是3D语义分割。实际应用中3D语义分割和2D图像语义分割比俯视图语义分割实用很多。俯视图语义分割的重要性有待商榷。此外实验结果虽然和传统方法comparable证明这种利用transformer的新技术路线是有用的，但是30多的mIoU和现在3D语义分割中78的mIoU相比仍然逊色很多。</li></ul><p>可以拓展改进的地方：</p><ul><li>后面利用这套同样的idea，去做一做3D语义分割或2D图像分割。这样能挖掘一个更有意思的课题，我从地图中学习到的信息对CameraView和3D点云分割能起到什么帮助？</li></ul>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
      <category>点云</category>
      
      <category>多模态融合</category>
      
      <category>2022</category>
      
    </categories>
    
    
    <tags>
      
      <tag>一句话读论文系列</tag>
      
      <tag>CVPR2022</tag>
      
      <tag>点云语义分割</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文十问:DeepFusion(CVPR2022)</title>
    <link href="/2022/05/24/%E8%AE%BA%E6%96%87%E5%8D%81%E9%97%AE-DeepFusion-CVPR2022/"/>
    <url>/2022/05/24/%E8%AE%BA%E6%96%87%E5%8D%81%E9%97%AE-DeepFusion-CVPR2022/</url>
    
    <content type="html"><![CDATA[<h1 id="DeepFusion-CVPR-2022"><a href="#DeepFusion-CVPR-2022" class="headerlink" title="DeepFusion(CVPR 2022)"></a>DeepFusion(CVPR 2022)</h1><blockquote><p>论文标题：DeepFusion: Lidar-Camera Deep Fusion for Multi-Modal 3D Object Detection<br><br>论文地址：<a href="https://arxiv.org/abs/2203.08195">https://arxiv.org/abs/2203.08195</a><br><br>作者单位：Johns Hopkins University, Google<br><br>代码地址：<a href="https://github.com/tensorflow/lingvo">https://github.com/tensorflow/lingvo</a><br><br>一句话读论文：This study points out the key role of transformed feature alignment process that plays in multi-modal fusion module, and thus proposes InverseAug and LearnableAlign module to overcome incosistency between data augmentation and multi-modal fusion.</p></blockquote><h2 id="Q1"><a href="#Q1" class="headerlink" title="Q1"></a>Q1</h2><p>论文试图解决什么问题？</p><p>本工作试图解决RGB图像-LiDAR点云网络，由于图像和点云分别进行数据增强操作(如随机旋转)，而导致模态特征之间的对应关系被破坏，使得多模态学习带来的增益被削弱的问题。</p><h2 id="Q2"><a href="#Q2" class="headerlink" title="Q2"></a>Q2</h2><p>这是否是一个新的问题？</p><p>这是一个比较有意思的，也很实用的新问题<br>1.因为大规模LiDAR点云数据需要大量的数据增强操作利于刷榜，如果多模态与之有冲突，那么提点效果会被大幅降低。<br>2.2020年前的多模态融合多是RGB图像-点云伪图像(环形投影，BeV, 透视投影等)的融合，本质是2D-2D网络间的融合。而2021年开始有更多3D-2D网络的多模态模型，因此在这个背景下考虑数据增强和特征对齐的冲突问题，还是比较有意思的</p><h2 id="Q3"><a href="#Q3" class="headerlink" title="Q3"></a>Q3</h2><p>这篇文章要验证一个什么科学假设？</p><p>1.特征对齐对多模态发挥效果很重要(参考Table1,9) -&gt; 提出InverseAug<br>\2. 深层网络的特征向量在不同模态间对齐，对模型学习语义信息有利 -&gt; 提出基于注意力机制的融合方式</p><h2 id="Q4"><a href="#Q4" class="headerlink" title="Q4"></a>Q4</h2><p>有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？</p><p>\1. Input-Level Decoration:<br>在原始数据层面,利用图形学中地仿射变换与投影，将LiDAR点与Pixel对应。相关工作有：<br>PointPainting(CVPR 2021),<br>PointAugmentation(CVPR 2021),<br>PMF(ICCV 2021)</p><p>\2. Mid-Level Fusion:<br>在网络结构层面，对每一层的特征向量隐式融合。相关工作有：<br>Deep Continuous Fusion(ECCV 2018)<br>EP-Net(CVPR 2018),<br>4D-Net(ICCV 2021),<br>Cross view Transformer for real-time Map-view Semantic Segmentation(CVPR 2022)<br>TransFusion(CVPR 2022)等</p><h2 id="Q5"><a href="#Q5" class="headerlink" title="Q5"></a>Q5</h2><p>论文中提到的解决方案之关键是什么？</p><p>\1. InverseAug: 把图像和点云分支的数据增强操作取逆，再用仿射变换投影。</p><p>\2. LearnableAlign: 比较常见的Attention的融合方式</p><h2 id="Q6"><a href="#Q6" class="headerlink" title="Q6"></a>Q6</h2><p>论文中的实验是如何设计的？</p><p>略………………</p><h2 id="Q7"><a href="#Q7" class="headerlink" title="Q7"></a>Q7</h2><p>用于定量评估的数据集是什么？代码有没有开源？</p><p>Waymo 。开源。</p><h2 id="Q8"><a href="#Q8" class="headerlink" title="Q8"></a>Q8</h2><p>论文中的实验及结果有没有很好地支持需要验证的科学假设？</p><p>有，比较好。<br>1.实验效果比较solid: 在各大主流目标检测方法上，提升了6-8%左右(LEVEL 2)。<br>2.可视化结果也很好地验证科学猜想。</p><h2 id="Q9"><a href="#Q9" class="headerlink" title="Q9"></a>Q9</h2><p>这篇论文到底有什么贡献？</p><p>同时考虑多模态融合与数据增强之间的协同作用，并且从原始数据和网络结构两个层面，提出简单有效的方法，较好地解决冲突并验证猜想。<br>Motivation有新意，实验效果比较solid, 通用价值大。</p><h2 id="Q10"><a href="#Q10" class="headerlink" title="Q10"></a>Q10</h2><p>下一步呢？有什么工作可以继续深入？</p><p>\1. 对于不可逆的数据增强操作，比如用于图像的RandomCrop, RandomErasing, 必然造成图像与点云在几何层面无法对应。原文中只能忽略这些增强操作，或者等对齐模块后单独加入不可逆的数据增强操作。</p><p>\2. 如实例分割、全景分割任务中，有一些对于实例的数据增强操作，如从其他场景中随机复制一些实例点到另外的场景，那么图像中是否需要对应生成一些假的物体pixel呢？若存在遮挡关系又如何处理？</p><p>\3. 方法相对简单一些，后续工作或许能继续深入挖掘。</p>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
      <category>点云</category>
      
      <category>多模态融合</category>
      
      <category>2022</category>
      
    </categories>
    
    
    <tags>
      
      <tag>CVPR2022</tag>
      
      <tag>论文十问系列</tag>
      
      <tag>多模态，点云目标检测</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>一句话读论文:Panoptic-PHNet(CVPR2022)</title>
    <link href="/2022/05/24/%E4%B8%80%E5%8F%A5%E8%AF%9D%E8%AF%BB%E8%AE%BA%E6%96%87-Panoptic-PHNet-CVPR2022/"/>
    <url>/2022/05/24/%E4%B8%80%E5%8F%A5%E8%AF%9D%E8%AF%BB%E8%AE%BA%E6%96%87-Panoptic-PHNet-CVPR2022/</url>
    
    <content type="html"><![CDATA[<h1 id="PNF-CVPR-2022"><a href="#PNF-CVPR-2022" class="headerlink" title="PNF(CVPR 2022)"></a>PNF(CVPR 2022)</h1><blockquote><p>论文标题：Panoptic Neural Fields: A Semantic Object-Aware Neural Scene Representation(CVPR 2022)<br><br>论文地址：<a href="https://arxiv.org/abs/2205.04334">https://arxiv.org/abs/2205.04334</a><br><br>作者单位：Google Research, Georgia Tech, Simon Fraser University, Stanford University<br><br>代码地址：暂无<br><br>一句话读论文：”We present Panoptic Neural Fields(PMF), and object-aware neural scene representation that decomposes a scene into a set of objects(things) and background(stuff).”</p></blockquote><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=520 height=86 src="///music.163.com/outchain/player?type=2&id=22707001&auto=0&height=66"></iframe><h2 id="网络框架："><a href="#网络框架：" class="headerlink" title="网络框架："></a><strong>网络框架：</strong></h2><p><img src="https://s2.loli.net/2022/05/23/AiBE8Xg3cu5ql1w.png" alt="image-20220523191212464"></p><center style="color:#C0C0C0;text-decoration:underline">图1 Overview of Panoptic Neural Field  </center><p><img src="https://s2.loli.net/2022/05/23/3rnZVSYfXNpMeib.png" alt="image-20220523191253241"></p><center style="color:#C0C0C0;text-decoration:underline">图2 Dynamic Challenging 3D Scenes Description</center><h2 id="核心内容："><a href="#核心内容：" class="headerlink" title="核心内容："></a><strong>核心内容：</strong></h2><p>大佬们请收下我的膝盖！！！</p><p>Motivation: </p><ul><li><p>把多用于室内场景的Nerf首次应用到室外自动驾驶场景中</p></li><li><p>Nerf原先多用于View Synthesis ，图形学渲染，重建。本篇工作窥得大佬们的野心：想在室外自动驾驶场景中，把分类、语义分割、目标检测、目标追踪、全景分割、三维重建、深度估计、场景编辑与生成等一系列任务全部做到SOTA指标，从而让Nerf一统2D-3D视觉任务的天下。虽然本篇工作是初步的尝试，但是开辟了一个新的研究领域。</p></li><li><p>相比于之前在Nerf基础上各种incremental类型的工作，本篇工作提出一种室外场景通用类型的Nerf框架。主要分为stuff类别和thing类别，除了分别学习传统Nerf模型所需要的color, pose，density 等信息，还加入语义信息，最后将stuff类别与thing类别共同合成panoptic radiance field，用于各类下游任务。</p></li><li><p>在已有的语义分支+Nerf, Dynamics+Nerf等各种变体基础上，取消了共享的MLP网络，而是为每一种类别的物体instance设计小的MLP网络；此外在初始化上引入类别的先验信息，设计了category-specific meta-learned initialization</p><p>  本文的方法：在原版Nerf基础上，做如下变化</p></li><li><p>Things类别：</p><ul><li>首先用RGB-only 3D Object Detector&amp; Tracker 得到Bounding box track $T_k$(由一系列仿射变换矩阵组成)和语义类别$k$.</li><li>对每个物体实例，用标准的Nerf网络提取特征，该网络是由time-invariant MLP组成(不是随时间变化而变化的RNN时序网络),得到包括color, pose, density等参数信息</li><li>损失函数共同优化Nerf网络和$T_k$</li></ul></li><li><p>Stuff 类别：</p><ul><li>用单一的Nerf网络提取Stuff类别，此外还有网络分支学习每个Stuff pixel的语义类别</li></ul></li><li><p>Panoptic-Radiance Field</p><ul><li><p>对color,densiy等通道采取如下融合方式</p></li><li><p>$$<br>  c(x| \theta) &#x3D; \mathbb{1}s(x)c_x(x|\theta) + \sum_kc_k(T^{-1}x|\theta)<br>  $$</p></li></ul></li><li><p>Render Panoptic-Radiance Fields</p><ul><li>$$<br>  C(r|\theta) \sim \sum_{i&#x3D;1}^Nw(t_i)f(\mathbf{r}(t_i)|\theta)<br>  $$</li></ul></li><li><p>Nerf中权重先验的获取</p><ul><li>Bias initalization(设置stuff MLP的bias为-5，thing MLP的bias为0.1，因为真实室外场景中stuff volume大多数是空的，而thing volume大多数非空) 和 Meta-learn的方式(<code>FedAvg</code> 算法)</li></ul></li></ul><h2 id="贡献点-x2F-创新性："><a href="#贡献点-x2F-创新性：" class="headerlink" title="贡献点&#x2F;创新性："></a><strong>贡献点&#x2F;创新性：</strong></h2><ul><li>见Motivation的第1-3条</li></ul><h2 id="实验结果："><a href="#实验结果：" class="headerlink" title="实验结果："></a><strong>实验结果：</strong></h2><p><img src="https://s2.loli.net/2022/05/23/xYOCDHST3kIguh9.png" alt="image-20220523194617216"><center style="color:#C0C0C0;text-decoration:underline">图5 实验结果1 </center></p><p><img src="https://s2.loli.net/2022/05/23/sowYT8tfqBILU1b.png" alt="image-20220523194641296"></p><center style="color:#C0C0C0;text-decoration:underline">图6 实验结果2 </center><p><img src="https://s2.loli.net/2022/05/23/gkLWepDuo41ZQmO.png" alt="image-20220523194752533"></p><center style="color:#C0C0C0;text-decoration:underline">图7 实验结果3 </center><p><img src="https://s2.loli.net/2022/05/23/xG91WXNegAr2Pj7.png" alt="image-20220523194819545"></p><center style="color:#C0C0C0;text-decoration:underline">图8 实验结果4 </center><p><img src="https://s2.loli.net/2022/05/23/ZLzw3K8InV62dcv.png" alt="image-20220523194847881"></p><center style="color:#C0C0C0;text-decoration:underline">图9 实验结果5 </center><h2 id="Related-Work-可选-后续再补充"><a href="#Related-Work-可选-后续再补充" class="headerlink" title="Related Work(可选, 后续再补充):"></a><strong>Related Work(可选, 后续再补充):</strong></h2><ul><li>Nerfs</li><li>Nerfs with Semantics</li><li>Nerfs with dynamics</li><li>Nerfs with object decompositions</li><li>Conditional NeRFs</li><li>MVS</li><li>SLAM</li></ul><h2 id="你认为优点-x2F-不足-x2F-可以拓展改进的地方-可选"><a href="#你认为优点-x2F-不足-x2F-可以拓展改进的地方-可选" class="headerlink" title="你认为优点&#x2F;不足&#x2F;可以拓展改进的地方(可选):"></a><strong>你认为优点&#x2F;不足&#x2F;可以拓展改进的地方(可选):</strong></h2><p>优点：</p><ul><li>太多了吐槽不完</li></ul><p>缺点：</p><ul><li>虽然很大一统，但是整个框架挺复杂，目前只能在离线的训练和推理，不太容易直接应用在实时场景下。</li></ul><h2 id="其他笔记"><a href="#其他笔记" class="headerlink" title="其他笔记:"></a><strong>其他笔记:</strong></h2><ul><li>CVer 计算机视觉：<a href="https://zhuanlan.zhihu.com/p/513499887">https://zhuanlan.zhihu.com/p/513499887</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
      <category>点云</category>
      
      <category>全景分割</category>
      
      <category>2022</category>
      
    </categories>
    
    
    <tags>
      
      <tag>一句话读论文系列</tag>
      
      <tag>CVPR2022</tag>
      
      <tag>点云全景分割</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>课程笔记-矩阵理论4-4</title>
    <link href="/2021/11/24/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-%E7%9F%A9%E9%98%B5%E7%90%86%E8%AE%BA4-4/"/>
    <url>/2021/11/24/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-%E7%9F%A9%E9%98%B5%E7%90%86%E8%AE%BA4-4/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>课程笔记：矩阵理论3-4——盖尔圆定理与特征值估计</title>
    <link href="/2021/11/24/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-%E7%9F%A9%E9%98%B5%E7%90%86%E8%AE%BA3-4/"/>
    <url>/2021/11/24/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-%E7%9F%A9%E9%98%B5%E7%90%86%E8%AE%BA3-4/</url>
    
    <content type="html"><![CDATA[<h1 id="课程笔记：矩阵理论3-4——盖尔圆定理与特征值估计"><a href="#课程笔记：矩阵理论3-4——盖尔圆定理与特征值估计" class="headerlink" title="课程笔记：矩阵理论3-4——盖尔圆定理与特征值估计"></a>课程笔记：矩阵理论3-4——盖尔圆定理与特征值估计</h1><h2 id="矩阵的盖尔圆盘"><a href="#矩阵的盖尔圆盘" class="headerlink" title="矩阵的盖尔圆盘"></a>矩阵的盖尔圆盘</h2><p>$N \times M$阶矩阵，每一行可以定义一个盖尔圆盘，每一列也可以定义一个盖尔圆盘。这里先讨论行的情况(关于行的盖尔圆盘)。<br>$$<br>D_i(A) &#x3D; { \mathbf{x \in \mathbb{C}: |x - a_{ii}| \le \sum_{i \ne j}|a_{ij}|} }<br>$$<br>圆的半径是除对角元素外所有元素的<strong>模</strong>的和。圆心是对角线元素(由于是复数，在复平面上有实轴和虚轴坐标)。</p><p>这些圆盘区域的并能够得到盖尔区域。</p><h2 id="盖尔圆盘定理"><a href="#盖尔圆盘定理" class="headerlink" title="盖尔圆盘定理"></a>盖尔圆盘定理</h2><p>盖尔圆盘定理: 设$$A$$是n阶矩阵，矩阵的所有特征值一定会落在某个圆盘里。<br>$$<br>|\lambda_i - a_{ii}| \le \sum_{i \ne j}^n |a_{ij}|, \exist i \in {1,2,\dots, n }<br>$$</p><p>Proof:<br>$$<br>\begin{align*}<br>&amp;\mathbf{A}x &#x3D; \lambda x \<br>\Leftrightarrow &amp;(\mathbf{A} - \lambda I)x &#x3D; 0 \<br>\Leftrightarrow &amp;\sum_{i \ne j} a_{ij}x_j + (a_{ii} - \lambda)x_i &#x3D; 0 , , i&#x3D;1,2,\dots,n\<br>\Leftrightarrow&amp; |\lambda - a_{ii}||x_i| &#x3D; |\sum_{i \ne j}a_{ij}x_j| \le \sum_{i \ne j}|a_{ij}||x_j| \<br>\Leftrightarrow &amp; |\lambda - a_{ii}| \le \sum_{i \ne j}|a_{ij}| \frac{|x_j|}{|x_{max}|} \le \sum_{i \ne j}|a_{ij}| \<br>\end{align*}<br>$$<br> 从第四行，挑选特征向量最大的分量$x_{max}$，并把该分量index 作为选取的矩阵的行。也就是说特征值满足的不等式，是在特征向量中分量最大的位置对应的行。</p><Details>  <summary>例3.4.4</summary>  <img src="https://i.loli.net/2021/11/19/7Ms9ftkrDhiBCzU.png" alt="image-20211119213910167" style="zoom:80%;" /></details><div class="note note-warning">            <p>注意：有的圆盘可能有多个特征值，有的圆盘可能没有特征值。如果需要精细的描述，使用精细圆盘定理，一个联通区域有几块圆盘，那么该区域必须<strong>恰好</strong>有相同个数的特征值</p>          </div><details>  <summary>例3.4.5</summary>  <img src="https://i.loli.net/2021/11/19/3CJzTgvXesfNMFQ.png" alt="image-20211119221047116" style="zoom:50%;" /></details><p>Note : 圆盘定理不能保证每个圆盘一定有特征值。但是下面精细圆盘定理更详细的说明了特征值分布特点：K个圆盘的并集联通区域一定恰好有K个特征值。</p><h2 id="精细圆盘定理"><a href="#精细圆盘定理" class="headerlink" title="精细圆盘定理"></a>精细圆盘定理</h2><p>精细圆盘定理：设$$C$$是盖尔区域的一个由$k$个圆盘组成的连通分量，则$C$恰好有$k$个特征值。</p><p>证明思路: 设矩阵$$\mathbf{A} &#x3D; \mathbf{D} + \epsilon \mathbf{B}$$, 其中$$\mathbf{D}$$是对角阵，$$\mathbf{B}$$是对角线为0的剩余部分。$$\epsilon \in [0,1]$$,这样矩阵$$\mathbf{A}$$从对角阵连续变化到原矩阵。对角阵的圆盘半径为0，圆心就是特征值，因此每个圆盘恰好有1个特征值，圆盘外面有n-1个特征值。在$$\epsilon$$连续变化增长到1的过程中，圆盘半径逐渐变大，并且部分圆开始相交，k个圆盘相交，k个圆盘区域外有n-k个特征值。由于矩阵的特征多项式的根是其系数的连续函数，随意特征值也是从圆心出发连续游走的，不会出现跳跃的情况，且增长过程中一直被圆盘包着，所以保证了联通的k个盖尔圆一定恰好有k个特征值。</p><h2 id="调节圆盘大小的技巧"><a href="#调节圆盘大小的技巧" class="headerlink" title="调节圆盘大小的技巧"></a>调节圆盘大小的技巧</h2><p>如何调整矩阵盖尔圆的半径，从而将特征值分离出来(比喻：就好像化学试剂提纯，分离似的)</p><p>原理如下：<br>$$<br>\begin{align*}<br>\mathbf{D}^{-1}\mathbf{A}\mathbf{D} &amp;&#x3D; \text{Diag}(\frac{1}{d_1}, \frac{1}{d_2}, \dots, \frac{1}{d_n}) \mathbf{A} \text{Diag}(d_1, d_2, \dots, d_n) \<br>&amp;&#x3D; (\frac{d_j}{d_i}a_{ij})<br>\end{align*}<br>$$</p><p>首先看出对角线元素是不受影响的，非对角先元素会被放缩，放缩系数由该元素的行与列(对应的是其他行)的系数决定</p><p>某一行的放缩系数$$d_i$$和其余行系数$$d_k$$的关系如下：</p><p>若$$d_i &gt; d_k$$，为了简便起见，我们设除了第i行的其他行均为1。第i行的非对角元素放缩系数分母比分子大，故圆盘被缩小；其他行的第i列元素分子比分母大，会被放大，除第i列外的其他非对角元素不变，总体圆盘会被放大。</p><p>若$$d_i &lt; d_k$$，类似的，设置除了第i行的其他行均为1。只有第i行的圆盘会被放大，其余圆盘均缩小。</p><h2 id="利用圆盘定理估计谱半径"><a href="#利用圆盘定理估计谱半径" class="headerlink" title="利用圆盘定理估计谱半径"></a>利用圆盘定理估计谱半径</h2><p>矩阵的谱半径 $$\le \underset{i}{\max} \sum_{j} a_{ij}$$ , 并记录$$v &#x3D;  \underset{i}{\max} \sum_{j} a_{ij}$$</p><p>证明思路: 因为根据圆盘定理，矩阵的每个特征值都一定落在某个圆盘(比如$$a_{ii}$$)里。<br>$$<br>|\lambda| &#x3D; |\lambda - a_{ii} + a_{ii}| \le \sum_{j} a_{ij}<br>$$<br>同理，矩阵的谱半径小于某个“最大”的列$$v’ &#x3D; \underset{j}{\max} \sum_{i} a_{ij}$$</p><h2 id="其他估计"><a href="#其他估计" class="headerlink" title="其他估计"></a>其他估计</h2><ol><li><p>Ostrwoski 圆盘定理</p></li><li><p>Brauer定理(Cassini卵形)</p></li></ol>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
      <category>矩阵理论</category>
      
    </categories>
    
    
    <tags>
      
      <tag>课程笔记</tag>
      
      <tag>特征值</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>课程笔记-统计学习理论与方法-ELS-Chap7</title>
    <link href="/2021/11/11/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA%E4%B8%8E%E6%96%B9%E6%B3%95-ELS-Chap7/"/>
    <url>/2021/11/11/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA%E4%B8%8E%E6%96%B9%E6%B3%95-ELS-Chap7/</url>
    
    <content type="html"><![CDATA[<h1 id="课程笔记：统计学习理论与方法（ELS-Chap7）"><a href="#课程笔记：统计学习理论与方法（ELS-Chap7）" class="headerlink" title="课程笔记：统计学习理论与方法（ELS_Chap7）"></a>课程笔记：统计学习理论与方法（ELS_Chap7）</h1><p>📚书籍：《Elements of Statistical Learning》Chap 7</p><h2 id="两个概念"><a href="#两个概念" class="headerlink" title="两个概念"></a>两个概念</h2><ol><li>模型选择(Model Selection):  模型由参数控制的，模型选择既包括模型类型选择，也包括参数的控制。后面讲贝叶斯信息准则时(BIC)会讲到。</li><li>模型评估(Model Assessment):  给定一个模型，评估其训练误差(In-sample Error)，测试误差(Extra-sample Error)，泛化误差(Generalization Error)。评估训练误差与测试误差之间的差距Bound.</li></ol><p>两者都是侧重对模型的泛化误差进行评估，而非模型的训练误差。泛化误差是整个统计学习中最核心的关注问题。</p><h2 id="训练误差与泛化误差"><a href="#训练误差与泛化误差" class="headerlink" title="训练误差与泛化误差"></a>训练误差与泛化误差</h2><p>训练误差：<br>$$<br>\overline{\text{err}} &#x3D; \frac{1}{N} \sum_{i&#x3D;1}^N L(y_i, \hat{f}(x_i))<br>$$<br>泛化误差：<br>$$<br>\text{Err}(y_i, \hat{f}(x_i)) &#x3D; E_{(X,Y) \sim \tau}(L(y_i, \hat{f}(x_i)) |\tau)<br>$$<br>其中损失函数通常是:</p><ul><li>均方误差$$L &#x3D; (y_i - \hat{f}(x_i))^2 $$</li><li>对数极大似然函数【用于多分类】(严格来说这不算损失函数，但是极大似然最大等价于加符号最小，所以可以写成相同形式):$$L &#x3D; -2\cdot[\sum_{k&#x3D;1}^K I(G&#x3D;k)\log(\hat{\Pr}(\hat{G}&#x3D;k)] $$.其中$$I(G&#x3D;k)$$相当于真实标签(Ground Truth).</li><li>对数极大似然的特殊情况【0-1分类】： $$L &#x3D; I(G&#x3D;1)\log(\hat{\Pr}) + (1-I(G&#x3D;1))\log(1-\hat{\Pr})$$</li></ul><div class="note note-primary">            <p>📢注意，我们算误差时的Ground Truth(GT) $$y$$并不一定是真实的标签，因为获取数据的过程中无法避免有噪声存在(就比如用精密物理仪器测量的结果总会有无法避免的系统误差)。我们的损失函数是让模型的估计值$$\hat{f}$$和GT算误差，不是和$$f(x)$$算误差。</p>          </div><h2 id="Bias-variance-分解"><a href="#Bias-variance-分解" class="headerlink" title="Bias-variance 分解"></a>Bias-variance 分解</h2><p>假设：$${(x_i,y_i)}$$ 是从某个数据分布中采样得到的某个数据集$$\tau$$。数据的真实分布是$$y &#x3D; f(x) + \epsilon$$，数据的真实标签是$$f(x)$$，但是我们获取数据集时总会有噪声$$\epsilon$$干扰。通常假设噪声服从均值为0的某个高斯分布。$$\epsilon \sim N(0, \sigma_{\epsilon}^2)$$</p><p>考察在$$X&#x3D;x_0$$一点上的单点泛化误差，其中求期望操作$$E$$是对所有的数据集$$\tau$$求的期望(Expectation over all the dataset $$\tau$$).<a name="t1">也就是说在这里，$\hat{f}(x_0)$也是随机变量，它的随机性来源于可能选取不同的数据集$\tau$ ；而$f(x_0)$的随机性只来源于$\epsilon$，二者独立无关联</a>。<br>$$<br>\begin{align*}<br>E(L(Y, \hat{f}(X))|X&#x3D;x_0, Y&#x3D;y_0 )&amp;&#x3D; (y_0 - f(x_0)) ,,,\text{(这里采用均方误差分析，其他同理)}\<br>&amp;&#x3D; E(f(x_0) - \hat{f}(x_0) + \epsilon)^2 \<br>&amp;&#x3D; E(f(x_0) - \hat{f}(x_0))^2 + E(\epsilon)^2 + 2E(f(x_0) - \hat{f}(x_0))E(\epsilon),,,\text{($\epsilon$ 与 $f(x_0) - \hat{f(x_0)}$ 独立)} \<br>&amp;&#x3D;  E(f(x_0) - \hat{f}(x_0))^2 + \sigma_{\epsilon}^2 + 0 \<br>&amp;&#x3D; E(f(x_0) - E\hat{f}(x_0) + E\hat{f}(x_0) - \hat{f}(x_0))^2 + \sigma_{\epsilon}^2 \<br>&amp;&#x3D; E(f(x_0) - E\hat{f}(x_0))^2 + E(\hat{f}(x_0) - \hat{f}(x_0))^2 + 2E(f(x_0) - E\hat{f}(x_0))\cdot 0 + \sigma_{\epsilon}^2\<br>&amp;&#x3D; E(f(x_0) - E\hat{f}(x_0))^2 + E(\hat{f}(x_0) - \hat{f}(x_0))^2  + \sigma_{\epsilon}^2 \<br>&amp;&#x3D; \text{bias}^2 + \text{variance} + \sigma_{\epsilon}^2<br>\end{align*}<br>$$</p><div class="note note-info">            <p>关于期望与方差的常用性质，请参见这篇<a href="https://www.oier99.cn/2021/11/10/%E9%9A%8F%E7%AC%94-%E6%9C%9F%E6%9C%9B%E4%B8%8E%E6%96%B9%E5%B7%AE%E7%9A%84%E6%80%A7%E8%B4%A8/">博客</a>.</p>          </div><h3 id="KNN的例子"><a href="#KNN的例子" class="headerlink" title="KNN的例子"></a>KNN的例子</h3><p>KNN的模型是$$\hat{f}(x_0) &#x3D; \frac{1}{K}\sum_{i&#x3D;1}^K y_i,, x_i \in n_K(x_0)$$.<br>$$<br>\begin{align*}<br>\hat{f}(x_0) &amp;&#x3D; \frac{1}{K}\sum_{i&#x3D;1}^K y_i \<br>&amp;&#x3D; \frac{1}{K} \sum_{i&#x3D;1}^K(f(x_i) + \epsilon_i ) \<br>&amp;\approx \frac{1}{K} \sum_{i&#x3D;1}^K(f(x_0) + \epsilon_i ) \<br>&amp;&#x3D; f(x_0) + \frac{1}{K} \sum_{i&#x3D;1}^K \epsilon_i<br>\end{align*}<br>$$<br>为了计算方差方便， 在理论推导这里做了一个重要近似：因为KNN取的是K个近邻点来插值估计，所以假设认为他们的“本源”$$f(x_i) \approx f(x_0)$$.这样，KNN的方差可以如下计算:<br>$$<br>\begin{align*}<br>\text{Var}(\hat{f}(x_0)) &amp;&#x3D; \text{Var} \frac{1}{K} \sum_{i&#x3D;1}^K \epsilon_i \<br>&amp;&#x3D; \frac{1}{K^2} \text{Var}\sum_{i&#x3D;1}^K \epsilon_i \<br>&amp;&#x3D; \frac{1}{K^2} \sum_{i&#x3D;1}^K \text{Var}(\epsilon_i) \<br>&amp;&#x3D; \frac{1}{K^2} \sum_{i&#x3D;1}^K \sigma_{\epsilon}^2 \<br>&amp;&#x3D; \frac{1}{K} \sigma_{\epsilon}^2<br>\end{align*}<br>$$<br>故泛化误差拆分为:<br>$$<br>Err(y_0, \hat{f}(x_0)) \approx \sigma_{\epsilon}^2 + E(f(x_0) - \frac{1}{K}\sum_{i&#x3D;1}^K y_i )^2 + \frac{1}{K} \sigma_{\epsilon}^2<br>$$<br>可以看出$$K$$越小，模型复杂度越大，方差越大，bias应该会越小。</p><h3 id="线性回归的例子"><a href="#线性回归的例子" class="headerlink" title="线性回归的例子"></a>线性回归的例子</h3><p>对于线性回归函数$$\hat{f}(x) &#x3D; \hat{\beta}^T x $$, 解最小二乘误差下的最佳近似参数是$$\hat{\beta} &#x3D; (X^TX)^{-1}X^Ty$$</p><p>故$$\hat{f}(x) &#x3D; y^T X(X^TX)^{-1}x &#x3D; x^T X(X^TX)^-1X^Ty &#x3D; h(x)y$$.可以看出$$\hat{f}(x)$$的随机性来源于y，即来源于$$\epsilon$$ .（这与<a href="#t1">刚才</a>说法不矛盾。因为之前对$$\hat{f}$$的分析是抽象的符号，而这里是对线性回归具体公式分析）<br>$$<br>\text{var}(\hat{f}(x_0)) &#x3D; ||h(x_0)||^2 \sigma_{\epsilon}^2<br>$$<br>故<br>$$<br>Err(y_0, \hat{f}(x_0)) &#x3D; \sigma_{\epsilon}^2 + E(f(x_0) - \hat{\beta}^Tx_0 )^2 + ||h(x_0)||^2 \sigma_{\epsilon}^2<br>$$</p><p>训练误差:<br>$$<br>\frac{1}{N}\sum_{i&#x3D;1}^N Err(y_0, \hat{f}(x_0)) &#x3D; \sigma_{\epsilon}^2 + \sum_{i&#x3D;1}^NE(f(x_0) - \hat{\beta}^Tx_0 )^2 + \frac{p}{N}\sigma_{\epsilon}^2<br>$$<br>可以看出模型复杂度由数据量$$N$$和参数量$$p$$共同控制。</p><div class="note note-primary">            <p>（这部分的推导需要用到矩阵的迹的性质）</p><p>$$\text{tr}(ABC) &#x3D; \text{tr}(BCA) &#x3D; \text{tr}(CAB)$$</p><p>$$\sum_{i}(x_i y_i) &#x3D; x^Ty &#x3D; \text{tr} (xy^T)$$</p>          </div><h2 id="乐观度"><a href="#乐观度" class="headerlink" title="乐观度"></a>乐观度</h2><p>乐观度(Optimisim)的概念主要是为了比较模型训练误差与泛化误差之间的差距，或者如何用训练误差去估计泛化误差。</p><p>直接计算训练误差和泛化误差的差有困难，因为模型输入$$x$$都不固定。所以可以采用重采样的技术，原先训练集$$(x,y)$$,是训练时采样得到的标签，记为$$y$$；重采样是对相同的$$x$$, 再次采样$$y^{\text{New}}$$。<br>$$<br>\text{Err}<em>{in}(y_i, \hat{f}(x_i)) &#x3D; \frac{1}{N}\sum</em>{i&#x3D;1}^N E_{Y^{\text{new}}}[L(Y_i^{\text{New}}, \hat{f}(x_i))]<br>$$</p><p>$$<br>\overline{err} &#x3D; \frac{1}{N} \sum_{i&#x3D;1}^N L(y, \hat{f}(x_i))<br>$$</p><p>结论<br>$$<br>\begin{align*}<br>    w &amp;&#x3D; E_y(op) \<br>    &amp;&#x3D; \frac{2}{N}\text{Cov}(y, \hat{y})<br>\end{align*}<br>$$<br>证明过程后续再补。</p><p>若$$\hat{y}$$是由参数量为$$d$$线性回归得到的估计，那么<br>$$<br>\text{Cov}(y, \hat{y}) &#x3D; d \sigma_{\epsilon}^2<br>$$<br>可得到<br>$$<br>E_y(\text{Err}<em>{in}) &#x3D; E_y(\overline{\text{err}}) + 2\cdot\frac{d}{N} \sigma</em>{\epsilon}^2<br>$$</p><h2 id="有效参数量"><a href="#有效参数量" class="headerlink" title="有效参数量"></a>有效参数量</h2><p>$$<br>\hat{y} &#x3D; Sy<br>$$</p><p>有效参数数量(Effective Number of Parameters):<br>$$<br>\text{df}(S) &#x3D; \text{tr}(S)<br>$$<br>对于线性回归，有如下关系：<br>$$<br>\sum_{i&#x3D;1} ^N \text{Cov}(y_i, \hat{y}<em>i) &#x3D; \text{df}(\hat{y}) \cdot  \sigma</em>{\epsilon}^2<br>$$</p><h2 id="贝叶斯信息量（BIC）"><a href="#贝叶斯信息量（BIC）" class="headerlink" title="贝叶斯信息量（BIC）"></a>贝叶斯信息量（BIC）</h2><p>题外话：前面的部分主要讲述的模型评价(Model Assesment)部分，贝叶斯信息量主要讲述的是模型选择的部分</p><p>BIC for 极大似然回归<br>$$<br>BIC &#x3D; -2 \cdot loglik + (\log N) \cdot d<br>$$<br>BIC under Gaussian Model<br>$$<br>BIC &#x3D; \frac{N}{\sigma_{\epsilon}^2}[\overline{\text{err}}+ (\log N)\cdot \frac{d}{N} \sigma_{\epsilon}^2]<br>$$<br>他的Motivation是源自贝叶斯派的思想(下面手抄草稿，由于过程有点难，整理后还有很多错误比如符号上下标对不上)</p><p><img src="https://i.loli.net/2021/11/10/9qVUZlvD2MgstSo.jpg" alt="1"></p><p><img src="https://i.loli.net/2021/11/10/yYAO547q1Nf39cM.jpg" alt="2"></p><p><img src="https://i.loli.net/2021/11/10/uskaSvyOilJT8mU.jpg" alt="3"></p><h2 id="交叉验证-以后有空再补"><a href="#交叉验证-以后有空再补" class="headerlink" title="交叉验证(以后有空再补)"></a>交叉验证(以后有空再补)</h2>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
      <category>统计学习理论与方法</category>
      
      <category>教材笔记</category>
      
      <category>Elements of Statistical Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>课程笔记</tag>
      
      <tag>教材笔记</tag>
      
      <tag>统计学习理论与方法</tag>
      
      <tag>ELS</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔-期望与方差的性质</title>
    <link href="/2021/11/10/%E9%9A%8F%E7%AC%94-%E6%9C%9F%E6%9C%9B%E4%B8%8E%E6%96%B9%E5%B7%AE%E7%9A%84%E6%80%A7%E8%B4%A8/"/>
    <url>/2021/11/10/%E9%9A%8F%E7%AC%94-%E6%9C%9F%E6%9C%9B%E4%B8%8E%E6%96%B9%E5%B7%AE%E7%9A%84%E6%80%A7%E8%B4%A8/</url>
    
    <content type="html"><![CDATA[<h1 id="随笔：期望与方差的常用性质"><a href="#随笔：期望与方差的常用性质" class="headerlink" title="随笔：期望与方差的常用性质"></a>随笔：期望与方差的常用性质</h1><h2 id="数学期望的性质"><a href="#数学期望的性质" class="headerlink" title="数学期望的性质"></a>数学期望的性质</h2><h2 id="方差的性质"><a href="#方差的性质" class="headerlink" title="方差的性质"></a>方差的性质</h2><ul><li><p>常数的方差为0：</p><p>  $$\text{Var}(C) &#x3D; 0$$</p><p>  逆命题$$\text{Var}(X)&#x3D;0$$的充要条件是$$X$$以概率1取常数$$\Pr{X&#x3D;C} &#x3D; 1$$</p></li><li><p>$$\text{Var}(CX) &#x3D; C^2 \text{Var}(X)$$</p></li><li><p>若随机变量$$X,Y$$<strong>相互独立</strong>，而且$$\text{Var}(X),\text{Var}(Y)$$存在, $$C_1， C_2$$是常数，，则有<br>  $$\text{Var}(C_1X + C_2Y) &#x3D; C_1^2 \text{Var}(X) + C_2^2 \text{Var}(Y)$$</p></li></ul><h2 id="期望的性质"><a href="#期望的性质" class="headerlink" title="期望的性质"></a>期望的性质</h2><ul><li>常数的期望是本身：<br>  $$ E(C) &#x3D; 0$$</li><li><strong>任意</strong>两个随机变量$$X,Y$$和常数$$C_1, C_2$$有<br>  $$ E(C_1X + C_2Y) &#x3D; C_1E(X) + C_2E(Y)$$</li><li>若随机变量$$X,Y$$<strong>相互独立</strong>，则有<br>  $$E(XY) &#x3D; E(X)E(Y)$$</li><li>Note: $$E(X^2)$$和$$E^2(X)$$不是一回事</li></ul>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
      <category>其他</category>
      
    </categories>
    
    
    <tags>
      
      <tag>概率统计</tag>
      
      <tag>期望方差</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔-每日bug系列（2021-11-05）</title>
    <link href="/2021/11/05/%E9%9A%8F%E7%AC%94-%E6%AF%8F%E6%97%A5bug%E7%B3%BB%E5%88%97-2021-11-05/"/>
    <url>/2021/11/05/%E9%9A%8F%E7%AC%94-%E6%AF%8F%E6%97%A5bug%E7%B3%BB%E5%88%97-2021-11-05/</url>
    
    <content type="html"><![CDATA[<h1 id="随笔-每日bug系列-2021-11-05"><a href="#随笔-每日bug系列-2021-11-05" class="headerlink" title="随笔-每日bug系列_2021-11-05"></a>随笔-每日bug系列_2021-11-05</h1><h2 id="Bug-1"><a href="#Bug-1" class="headerlink" title="Bug 1"></a>Bug 1</h2><p>计算矩阵特征值，求行列式$|\lambda I_n - A|$时，一定要写成<br>$$<br>\left [<br>\begin{array}{cccc}<br>\lambda - a_{11} &amp; -a_{12} &amp; \cdots&amp; -a_{1n}\<br>-a_{21} &amp; \lambda -a_{22} &amp; \cdots &amp; -a_{2n}\<br>\vdots &amp; \vdots &amp; \vdots &amp; \vdots<br>\end{array}</p><p>\right ]<br>$$<br>的形式，否则展开求解时极容易出错(今天计算行列式看着原矩阵脑补写特征多项式一连写错五次的痛，对自己的算术能力表示深刻怀疑)</p><h2 id="Bug-2"><a href="#Bug-2" class="headerlink" title="Bug 2"></a>Bug 2</h2><p>记住 $$A + 2&#x3D; A + 2I_n$$，是加在对角线元素上的，不是每个位置都加2。这个惯性错误犯了好多次了！！</p><h2 id="Bug-3"><a href="#Bug-3" class="headerlink" title="Bug 3"></a>Bug 3</h2><p>相似矩阵有相同特征值，但逆命题不成立(即有相同特征值，包括每个特征值的代数重数也相同的两个矩阵不一定相似)。因为可能有不同的Jordan标准型形式。比如一个矩阵可以对角化，相似于某个对角阵；而另一个矩阵无法对角化，只能相似于某个Jordan标准型，相同特征值个数的Jordan标准型又有很多种，所以导致这两个矩阵无法相似。</p>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
      <category>每日bug系列</category>
      
    </categories>
    
    
    <tags>
      
      <tag>特征值</tag>
      
      <tag>矩阵理论</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>课程专题-矩阵理论之矩阵对角化的条件汇总</title>
    <link href="/2021/11/04/%E8%AF%BE%E7%A8%8B%E4%B8%93%E9%A2%98-%E7%9F%A9%E9%98%B5%E7%90%86%E8%AE%BA%E4%B9%8B%E7%9F%A9%E9%98%B5%E5%AF%B9%E8%A7%92%E5%8C%96%E7%9A%84%E6%9D%A1%E4%BB%B6%E6%B1%87%E6%80%BB/"/>
    <url>/2021/11/04/%E8%AF%BE%E7%A8%8B%E4%B8%93%E9%A2%98-%E7%9F%A9%E9%98%B5%E7%90%86%E8%AE%BA%E4%B9%8B%E7%9F%A9%E9%98%B5%E5%AF%B9%E8%A7%92%E5%8C%96%E7%9A%84%E6%9D%A1%E4%BB%B6%E6%B1%87%E6%80%BB/</url>
    
    <content type="html"><![CDATA[<h1 id="课程专题-矩阵理论之矩阵对角化的条件汇总"><a href="#课程专题-矩阵理论之矩阵对角化的条件汇总" class="headerlink" title="课程专题-矩阵理论之矩阵对角化的条件汇总"></a>课程专题-矩阵理论之矩阵对角化的条件汇总</h1><p>题记：整个初等矩阵理论中要考虑最重要的事情，就是如何尽可能将矩阵化简、压缩。不论是化简为最普世的Jordan标准型，还是更为特殊的矩阵能化简为对角矩阵，以至到后续的各种矩阵分解，都是在研究这一问题。</p><h2 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h2><ol><li>定义了特征值，特征向量，特征多项式，特征子空间，相似变换等概念(自我回顾一下)。一些比较重要的性质（常识）</li></ol><ul><li>$$\tr(A) &#x3D; \sum \lambda_i$$（包括多重次数的特征值），$$|A|&#x3D;\Pi \lambda_i$$</li><li>相似矩阵的特征值相同；但特征值相同不一定相似，因为两个矩阵特征值对应的特征子空间不一定一样。</li><li>属于不同特征值的特征向量必不可能相同;进一步，属于不同特征值的特征向量必线性无关。</li><li>$$kA$$,$$A^m$$,$$A^{-1}$$(若$A$可逆)特征值分别是$$k\lambda$$, $$\lambda^m$$,$$\lambda^{-1}$$</li><li>实矩阵的特征值&#x2F;特征向量有可能是复数，因此特征子空间的数域一般定义在复数上</li><li>特征子空间：特征子空间本质上是$$(\lambda I- A)x&#x3D;0$$的解空间(除0以外)，若$$x_1$$和$$x_2$$都是$$A$$的属于$$\lambda$$的特征向量，则$$k_1x_1+ k_2x_2$$($$\ne 0$$)也是属于$$\lambda$$的特征向量。</li></ul><ol start="2"><li><h2 id="矩阵理论中为了更便捷地研究-玩耍-，引入了零化多项式和最小多项式的概念。"><a href="#矩阵理论中为了更便捷地研究-玩耍-，引入了零化多项式和最小多项式的概念。" class="headerlink" title="矩阵理论中为了更便捷地研究(玩耍)，引入了零化多项式和最小多项式的概念。"></a>矩阵理论中为了更便捷地研究(玩耍)，引入了零化多项式和最小多项式的概念。</h2></li></ol><h2 id="条件"><a href="#条件" class="headerlink" title="条件"></a>条件</h2><ol><li>$n$阶矩阵可对角化的充分必要条件：<ul><li>矩阵有$n$个线性无关的特征向量(原理：这$n$个线性无关的特征向量正好构成了相似变换的可逆矩阵)</li><li>每个特征值的几何重数&#x3D;代数重数(通常几何重数&lt;&#x3D;代数重数)(原理：从Jordan标准型上直观的来解释一下，代数重数是Jordan标准型对角线上$\lambda$总共出现的次数，几何重数(特征子空间的维数)是特征值为$$\lambda$$的Jordan块$$J_k(\lambda)$$的块数，每一块对应一个特征子空间的基向量，块之间阶数不一样对应基向量之间是线性无关的。若代数重数&#x3D;几何重数，则每个Jordan块大小为1，退化为对角矩阵) ？</li></ul></li><li>$n$阶矩阵的最小多项式没有重根<ul><li>零化多项式：特征多项式f()肯定是满足$$f(A)&#x3D;0$$</li><li>$$\lambda$$是$A$的特征值充要条件：最小多项式$m(\lambda)&#x3D;0$</li><li>相似矩阵有相同的最小多项式</li><li>分块对角矩阵的最小多项式是各块最小多项式的最小公倍式(对照分块对角阵的特征多项式&#x3D; 各块最小多项式的乘积)</li></ul></li></ol>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
      <category>矩阵理论</category>
      
      <category>课程专题</category>
      
      <category>矩阵理论</category>
      
    </categories>
    
    
    <tags>
      
      <tag>矩阵对角化</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>课程笔记-矩阵理论(1)——线性代数知识回顾(下)</title>
    <link href="/2021/10/11/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-%E7%9F%A9%E9%98%B5%E7%90%86%E8%AE%BA1.5/"/>
    <url>/2021/10/11/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-%E7%9F%A9%E9%98%B5%E7%90%86%E8%AE%BA1.5/</url>
    
    <content type="html"><![CDATA[<h1 id="课程笔记：矩阵理论-1-——线性代数知识回顾-下"><a href="#课程笔记：矩阵理论-1-——线性代数知识回顾-下" class="headerlink" title="课程笔记：矩阵理论(1)——线性代数知识回顾(下)"></a>课程笔记：矩阵理论(1)——线性代数知识回顾(下)</h1><p>📚书籍：《矩阵理论与应用》张跃辉 Chap 1</p><h2 id="相似矩阵"><a href="#相似矩阵" class="headerlink" title="相似矩阵"></a>相似矩阵</h2><p>$$<br>\mathbf{A}x &#x3D; \lambda x<br>$$</p><ol><li>$$\mathbf{A}$$是方阵(所有有关特征值和特征响亮的讨论是对方阵而言的)</li><li>$$x \ne 0$$</li></ol><p>则称$$x$$是属于特征值$$\lambda$$的$$\mathbf{A}$$上的特征向量</p><h3 id="特征多项式"><a href="#特征多项式" class="headerlink" title="特征多项式"></a>特征多项式</h3><p>$$<br>\begin{align}<br>(\mathbf{A} - \lambda I)x &amp;&#x3D; 0 \<br>\end{align}<br>$$</p><p>由于$$x \ne 0$$,若有满足该等式的$$\lambda$$值，则必为降秩矩阵(奇异矩阵)，故记<br>$$<br>f(\lambda) &#x3D; |\mathbf{A} - \lambda \mathbf{I}| &#x3D; 0<br>$$<br>成为特征多项式。</p><p><strong>谱半径</strong>：数值(复数模长)最大的特征值$$\rho(\mathbf{A}) &#x3D; \max { |\lambda|: \lambda \in \sigma(\mathbf{A})}$$</p><p>从几何上看，所有的特征值都落在以原点为圆心，谱半径$$\rho(\mathbf{A})$$为半径的圆盘内</p><p><strong>矩阵的特征值$$\lambda $$的特征子空间</strong>：给定$$\lambda$$后 $$(\mathbf{A} - \lambda \mathbf{I})x &#x3D; 0$$这个齐次方程的解空间，记为$$V_\lambda $$。该解空间的维度$$\dim(V_\lambda) &#x3D; n - r (\mathbf{A})$$称为特征值$$\lambda$$的<strong>几何重数</strong></p><p><strong>代数重数</strong>：将特征多项式因式分解后， $$f(\lambda) &#x3D;|\mathbf{A}- \lambda \mathbf{I}|&#x3D; \prod_i (\lambda - \lambda_i)^{n_i}&#x3D;0$$，每个特征值的对应的$$n_i$$</p><p><strong><u><em>任何特征值的几何重数不会超过其代数重数</em></u></strong></p><h3 id="特征值的性质"><a href="#特征值的性质" class="headerlink" title="特征值的性质"></a>特征值的性质</h3><ol><li>$$|\mathbf{A}| &#x3D; \prod_i(\lambda_i)^{n_i}$$</li><li>$$\tr(\mathbf{A}) &#x3D; \sum_i n_i \lambda_i $$</li><li>$$\mathbf{A}$$可逆 $$\Leftrightarrow$$ 0不是其特征值</li><li>$$\lambda$$是$$\mathbf{A}$$的特征值，则$$f(\lambda)$$是$$f(\mathbf{A})$$的特征值，特征向量不变</li><li>设$$\mathbf{A}$$可逆，其特征多项式为$$|\mathbf{A}^{-1} - \lambda \mathbf{I}| &#x3D; \prod_{i} (\lambda - \lambda_i^{-1})^{n_i}$$, $$\lambda^{-1}$$也是特征值，而且对应特征向量不变</li><li><strong>相似矩阵具有相同的特征多项式，因此具有相同的特征值</strong></li></ol><h3 id="特征向量的性质"><a href="#特征向量的性质" class="headerlink" title="特征向量的性质"></a>特征向量的性质</h3><ol><li>属于不同特征值的特征向量线性无关</li><li>$$\mathbf{A}$$可以对角化 $$\Leftrightarrow$$ $$\mathbf{A}$$有$$n$$个线性无关的<strong>特征向量</strong> $$\Leftrightarrow$$ $$\mathbb{F}^n$$有一组由$$\mathbf{A}$$特征向量组成的基**  <u><strong>(Que?)</strong></u></li></ol><h3 id="对角化主定理"><a href="#对角化主定理" class="headerlink" title="对角化主定理"></a>对角化主定理</h3><p>一个$$n$$阶矩阵可以对角化 $$\Leftrightarrow$$ 矩阵的每个特征值代数重数与几何重数相等</p><div class="note note-info">            <p>1.特别地，若矩阵有$$n$$个不同的特征值，则可以对角化</p><p>2.实对称矩阵一定可以正交化。而且对于一个实对称矩阵$$\mathbf{A}$$, 存在正交矩阵$$\mathbf{P}$$，使得$$\mathbf{A} &#x3D; P^{-1}DP &#x3D; P^T $$,即通过正交变换变成对角阵，$$\mathbf{A}$$正交相似于对角阵。</p>          </div><p> Note: 此处要填坑，系统复习一下相似矩阵那一章的证明</p><h2 id="矩阵分解"><a href="#矩阵分解" class="headerlink" title="矩阵分解"></a>矩阵分解</h2><h3 id="满秩分解"><a href="#满秩分解" class="headerlink" title="满秩分解"></a>满秩分解</h3><p>任何一个矩阵都可以被分解为列满秩矩阵$$\times$$行满秩矩阵。</p><p>E.g.<br>$$<br>\mathbf{A}<em>{n \times m} &#x3D; \left[\begin{matrix} \mathbf{P}</em>{n \times r(\mathbf{A})} &amp; 0 \end{matrix} \right] \left[\begin{matrix} \mathbf{Q}_{r(\mathbf{A}) \times m} \ 0  \end{matrix}\right]<br>$$<br>一个比较简单的求解方法是，把$$\mathbf{Q}$$搞成一个Hermitte标准形，这样$$\mathbf{P}$$直接可以得出结果：取矩阵$$\mathbf{A}$$的前$$r(A)$$列</p><h3 id="LU分解"><a href="#LU分解" class="headerlink" title="LU分解"></a>LU分解</h3><h3 id="奇异值-SVD-分解"><a href="#奇异值-SVD-分解" class="headerlink" title="奇异值(SVD)分解"></a>奇异值(SVD)分解</h3><h2 id="线性空间"><a href="#线性空间" class="headerlink" title="线性空间"></a>线性空间</h2><ol><li>回顾加群(Abel群)</li></ol><ul><li>封闭性</li><li>结合律</li><li>交换律</li><li>有单位元(0元)</li><li>有逆元(-a)</li></ul><ol start="2"><li><strong>线性空间的直观理解</strong>：$$(V, +)$$是一个加群，定义数域$$\mathbb{F}$$上的数乘运算，则$$（V, + , \cdot）$$是$$\mathbb{F}$$上的一个线性空间。$$\mathbb{F}$$叫做基域。</li></ol><p>$$(\mathbb{R}^3, +, \cdot)$$是$$\mathbb{R}$$的一个线性空间(有限维度的一个线性空间)</p><p>例如，$$f \in c[a,b]$$上的所有连续函数，则$$(c[a, b], + , \cdot)$$ 是一个线性空间(无穷维度)(有泛函的观点了，只要是线性无关的函数就能构成一组基底)。</p><p>例如，矩阵加法和数乘能构成一个线性空间，其一组基为全体基础矩阵$$\mathbb{E}_{ij}$$。特别地，全体$$n$$ 阶方阵组成$$n^2$$维线性空间；全体$$n \times 1$$阶矩阵，即全体$$n$$维向量，构成了$$\mathbb{F}$$上的一个$$n$$维线性空间，其一组基由所有的标准向量组成即$$e_1, e_2, \dots, e_n$$</p><p>3.<strong>线性空间的基向量定义：</strong>若V中存在$$n$$个线性无关的向量，使得V中任意向量都与他们线性相关，则称$$V$$是$$n$$线性空间。</p><p>4.$$V$$中任意向量均能唯一的表为$$\alpha$$的线性组合，$$ \alpha &#x3D; k_1 \alpha_1 + k_2 \alpha_2 + \dots + k_n \alpha_n &#x3D;(\alpha_1, \alpha_2, \dots, \alpha_n) k$$，则称为向量$$\alpha$$关于基的坐标$$(k_1, k_2, \dots, k_n)^T$$。</p><p>线性空间的基一般不唯一，但线性空间的维数是唯一确定的。所以不同基向量包含的向量个数相同。</p><ol start="5"><li><strong>基扩充定理</strong>:</li><li>过渡矩阵：设$$n$$维线性空间的一组基$$(\alpha_1, \alpha_2, \dots, \alpha_n)$$与另一组基$$(\beta_1, \beta_2, \dots, \beta_n)$$如果存在如下关系:</li></ol><p>$$<br>\left[\begin{matrix} \beta_1&amp; \beta_2 &amp; \dots \beta_n \end{matrix} \right] &#x3D; \left[\begin{matrix} \alpha_1&amp; \alpha_2 &amp; \dots \alpha_n \end{matrix} \right] \mathbf{P}<br>$$</p><p>所以可见，每个列向量$$\beta_i$$都是$$\alpha_j$$的线性组合，组合系数由$$P_i$$列向量控制。</p><img src="https://i.loli.net/2021/10/11/UiXGb2vNnJfgFus.jpg" style="zoom: 33%;" /><h2 id="内积空间"><a href="#内积空间" class="headerlink" title="内积空间"></a>内积空间</h2><ol><li><p>背景：内积的引入和“长度“有关。在线性空间中引入内积的概念，衡量两个向量的“远近”。如果内积归一化后，就是通过两个单位向量的夹角来衡量两个向量的距离。</p></li><li><p><strong>内积空间</strong>：设$$V是$$$$\mathbb{F}$$上的线性空间，若对$$V$$中的任意两个向量，都定义了$$\mathbb{F}$$中的一个数$$(\alpha, \beta)$$,使得满足</p><ul><li><p>(共轭对称性) $$(\alpha, \beta) &#x3D; \overline{(\beta, \alpha)}$$</p></li><li><p>(正定性）$$(\alpha, \alpha) \ge 0$$</p></li><li><p>(双线性) $$(a \alpha + b \beta， \gamma) &#x3D; a(\alpha, \gamma) + b(\beta, \gamma)$$ (共轭双线性)$$(\alpha, a \beta + b \gamma) &#x3D; a(\alpha, \beta) + \overline{b}(\alpha, \beta)$$</p><p>  则称其为内积空间。</p></li></ul></li><li><p>内积与范数性质</p><p> 比较重要的且容易忘记的，Cauchy-Schwards不等式， 三角不等式</p></li><li><p>内积的作用：从代数上，两个向量的角度，两个向量在线性空间中的距离都可以由内积来定义。有了角度和长度，从几何上更好解释向量之间的位置关系。比如两个向量正交是垂直(角度90°)，实数域的线性空间上两个向量线性相关当且仅当夹角为0或$$\pi$$。</p></li><li><p>Gram-Schmidt正交化方法: 已知线性无关组$$\alpha_1 , \dots, \alpha_n$$,求标准正交组</p></li></ol><p>$$<br>\beta_k &#x3D; \alpha_k - \sum_i (\alpha_i, \gamma_i)\gamma_i<br>$$</p><p>$$<br>\gamma_k &#x3D; \frac{\beta_k}{||\beta_k||}<br>$$</p><p>几何意义：再求第$$k$$个标准正交基时，投影到$$\gamma_1 \dots \gamma_{k-1}$$个标准正交基构成的”超平面“上的投影等于其在各个$$\gamma_i$$分量上的投影向量之和。所以减去后自然就与$$\gamma_1 \dots \gamma_{k-1}$$正交，最后再单位化。</p><ol start="6"><li>酉矩阵$$\mathbf{A}^H \mathbf{A} &#x3D; \mathbf{I}$$, 在实数域上就是正交矩阵$$\mathbf{A}^T \mathbf{A} &#x3D; \mathbf{I}$$。该矩阵的每个行(列)向量两两正交，且为单位向量。正交矩阵的集合意义几乎就是旋转变换，利用正交矩阵做旋转变换可以去掉二次型中的交叉项，变成标准形式。 <img src="https://i.loli.net/2021/10/11/BWxoCmdMzDG9c12.png" alt="image-20211011195300299" style="zoom: 67%;" /></li></ol><img src="https://i.loli.net/2021/10/11/agG4bAcnJCmlEeW.png" alt="image-20211011195358735" style="zoom: 60%;" /><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>《线性代数》 第二版 居余马 清华大学出版社<a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>《矩阵理论与应用》 张跃辉  上海交通大学出版社<a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
      <category>矩阵理论</category>
      
    </categories>
    
    
    <tags>
      
      <tag>课程笔记</tag>
      
      <tag>矩阵，线性代数</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>课程笔记-矩阵理论2</title>
    <link href="/2021/10/11/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-%E7%9F%A9%E9%98%B5%E7%90%86%E8%AE%BA2/"/>
    <url>/2021/10/11/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-%E7%9F%A9%E9%98%B5%E7%90%86%E8%AE%BA2/</url>
    
    <content type="html"><![CDATA[<h1 id="课程笔记：矩阵理论-2-——矩阵与线性变换"><a href="#课程笔记：矩阵理论-2-——矩阵与线性变换" class="headerlink" title="课程笔记：矩阵理论(2)——矩阵与线性变换"></a>课程笔记：矩阵理论(2)——矩阵与线性变换</h1>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
      <category>矩阵理论</category>
      
    </categories>
    
    
    <tags>
      
      <tag>课程笔记</tag>
      
      <tag>矩阵，线性代数</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>课程笔记-矩阵理论(1)——线性代数知识回顾(上)</title>
    <link href="/2021/10/10/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-%E7%9F%A9%E9%98%B5%E7%90%86%E8%AE%BA1/"/>
    <url>/2021/10/10/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-%E7%9F%A9%E9%98%B5%E7%90%86%E8%AE%BA1/</url>
    
    <content type="html"><![CDATA[<h1 id="课程笔记：矩阵理论-1-——线性代数知识回顾-上"><a href="#课程笔记：矩阵理论-1-——线性代数知识回顾-上" class="headerlink" title="课程笔记：矩阵理论(1)——线性代数知识回顾(上)"></a>课程笔记：矩阵理论(1)——线性代数知识回顾(上)</h1><p>📚书籍：《矩阵理论与应用》张跃辉 Chap 1</p><h2 id="梦开始的地方："><a href="#梦开始的地方：" class="headerlink" title="梦开始的地方："></a>梦开始的地方：</h2><p>约定本课程讨论均在复数域$C$的子域上进行。</p><h2 id="引言：-线性代数是什么"><a href="#引言：-线性代数是什么" class="headerlink" title="引言： 线性代数是什么"></a>引言： 线性代数是什么</h2><blockquote><p>本科阶段的线性代数课程讨论两个相关问题：一个是引入矩阵来阶线性方程组，另一个是利用线性方程组来研究矩阵。</p></blockquote><h4 id="矩阵解线性方程组"><a href="#矩阵解线性方程组" class="headerlink" title="矩阵解线性方程组"></a>矩阵解线性方程组</h4><p>$$<br>\begin{align}<br>\textbf{A}x &amp;&#x3D; b \<br>x_1 \alpha_1 + x_2 \alpha_2 + \dots + x_n \alpha_n &amp;&#x3D;  b<br>\end{align}<br>$$</p><p>可以通过研究齐次方程组<br>$$<br>\textbf{A}x &#x3D; 0<br>$$<br>的解结构来来研究$$\textbf{A}x&#x3D;b$$的解结构，任意解可以表达为基础解系的线性组合。</p><details><summary>展开可详细回顾线性方程组求解</summary><p><b>截图自《线性代数》第二版 居余马著 P47 - P48</b></p><img width="680" src="https://i.loli.net/2021/10/10/725C3QEdf1hiDVA.jpg" alt="图1"><img width="680" src="https://i.loli.net/2021/10/10/XlW96OenEbxMIZv.jpg" alt="图2"></details><h4 id="矩阵提供二次型的简洁表达"><a href="#矩阵提供二次型的简洁表达" class="headerlink" title="矩阵提供二次型的简洁表达"></a>矩阵提供二次型的简洁表达</h4><p>$$<br>\begin{align}<br>f(x) &amp;&#x3D; x^T\textbf{A}x \<br>&amp;&#x3D;  x^T [\alpha_1, \alpha_2, \dots, \alpha_n] x \<br>&amp;&#x3D; [x^T \alpha_1, x^T\alpha_2, \dots, x^T\alpha_n] x \<br>&amp;&#x3D; \sum_j{(x^T\alpha_j)x_j} \<br>&amp;&#x3D; \sum_j(\sum_i(x_i\alpha_{ij})x_j) \<br>&amp;&#x3D; \sum_{ij}{\alpha_{ij} x_i x_j}</p><p>\end{align}<br>$$</p><p>请熟悉这个形式和上面的推导过程~系数矩阵$$\textbf{A}$$中的每一项$$a_{ij}$$对应的$$x_i$$与$$x_j$$的乘积的系数。假如$$\textbf{A}$$是一个对角矩阵的话，那么就是标准型$$\sum_{i} \alpha_{ii}x_i^2 $$，（就好比二维平面上的圆），有交叉项的话好比二维平面的椭圆。$$\textbf{A}$$一定是一个对称矩阵，因为$$x_ix_j$$的系数就是$$x_jx_i$$的系数。</p><h3 id="矩阵乘法与二次型的关系"><a href="#矩阵乘法与二次型的关系" class="headerlink" title="矩阵乘法与二次型的关系"></a>矩阵乘法与二次型的关系</h3><p>如何理解n阶方阵的高次幂$$\textbf{A}^m$$?</p><p>相似矩阵$$\textbf{A} \sim \textbf{B}$$,  $$\textbf{B} &#x3D; \textbf{P}^{-1}A\textbf{P}$$。如果利用特征值与特征向量可将$$\textbf{A} &#x3D; \textbf{P}^{-1}\textbf{D}\textbf{P}$$，那么$$\textbf{A}^m &#x3D; \textbf{P}^{-1}\textbf{D}^m\textbf{P}$$, $$\textbf{A}^{-1} &#x3D; \textbf{P}^{-1}\textbf{D}^{-1}\textbf{P}$$。</p><p>此外实对称矩阵可以正交对角化，即存在正交矩阵$$\textbf{Q}$$，使得<br>$$<br>\textbf{Q}^{-1}\textbf{A}\textbf{Q} &#x3D; \textbf{Q}^T \textbf{A} \textbf{Q} &#x3D; \textbf{D}<br>$$<br>可以利用坐标变换$$x &#x3D; \textbf{Q}y $$ 将实二次型化为标准型<br>$$<br>f &#x3D; \lambda_1y_1^2 + \lambda_2y_2^2 + \lambda_3y_3^2 + \dots + \lambda_ny_n^2<br>$$</p><h2 id="矩阵的基础运算及其性质"><a href="#矩阵的基础运算及其性质" class="headerlink" title="矩阵的基础运算及其性质"></a>矩阵的基础运算及其性质</h2><h3 id="共轭转置"><a href="#共轭转置" class="headerlink" title="共轭转置"></a>共轭转置</h3><ol><li>$\mathbf{A}$的共轭矩阵记作$\bar{\mathbf{A}}$，$\mathbf{A}$的共轭转置矩阵记作$\mathbf{A}^H$, $\mathbf{A}^H&#x3D;\bar{\mathbf{A}}^T$当,$\mathbf{A} \in \mathbb{R}^{m \times n}$时，$\bar{\mathbf{A}}&#x3D;\mathbf{A}^T$</li><li>$$\textbf{A} &#x3D; \textbf{A}^H$$的矩阵被称为Hermitte矩阵，其中当$$\mathbf{A} \in \mathbb{R}^{n \times n}$$是实对称矩阵。</li></ol><h3 id="矩阵乘法"><a href="#矩阵乘法" class="headerlink" title="矩阵乘法"></a>矩阵乘法</h3><p>矩阵乘法可以按照行向量进行, 比如对于$$n \times m$$与$$m \times p$$的两个矩阵相乘<br>$$<br>\begin{align}<br>\mathbf{A}\mathbf{B}<br>&amp;&#x3D;\left[ \begin{matrix}\alpha_1^T \ \alpha_2^T \ \cdots \ \alpha_n^T  \end{matrix}\right]<br>\left[ \begin{matrix}x_1, x_2,\cdots,x_p  \end{matrix}\right] \<br>&amp;&#x3D;\left[ \begin{matrix}\alpha_1^Tx_1 &amp; \alpha_1^Tx_2 &amp; \cdots &amp; \alpha_1^T x_p \ \alpha_2^Tx_1&amp; \alpha_2^Tx_2 &amp; \cdots &amp; \alpha_2^T x_p \<br>\vdots&amp; \vdots &amp; \ddots &amp; \vdots \<br>\alpha_n^Tx_1 &amp; \alpha_n^Tx_2 &amp; \cdots &amp; \alpha_n^T x_p</p><p>\end{matrix}\right]<br>\end{align}<br>$$<br>前一个矩阵的每一行乘以后一个矩阵的每一列，得到新矩阵的每一个元素。</p><p>其中，当$$\mathbf{B}$$ 是一个$$m \times 1$$(列向量)时,<br>$$<br>\begin{align}<br>\mathbf{A}\mathbf{B}<br>&amp;&#x3D;\left[ \begin{matrix}\alpha_1^T \ \alpha_2^T \ \cdots \ \alpha_n^T  \end{matrix}\right]\left[x\right] \<br>&amp;&#x3D;\left[ \begin{matrix} \alpha_1^T x\ \alpha_2^Tx \ \vdots \ \alpha_n^T x \end{matrix}\right] \<br>&amp;&#x3D;\left[ \begin{matrix} a_{11}x_1 + a_{12}x_2 + a_{13}x_3 + \dots + a_{1m} x_m\ a_{21}x_1 + a_{22}x_2 + a_{23}x_3 + \dots + a_{2m} x_m \ \vdots \ a_{n1}x_1 + a_{n2}x_2 + a_{n3}x_3 + \dots + a_{nm} x_m<br>\end{matrix}\right] \<br>&amp;&#x3D;x_1\left[ \begin{matrix} a_{11} \ a_{21} \ \vdots \ a_{n1} \end{matrix}\right] + \dots + x_m\left[ \begin{matrix} a_{1m} \ a_{2m} \ \vdots \ a_{nm} \end{matrix}\right] \<br>&amp;&#x3D; x_1A_1 + x_2 A_2 + \dots + x_m A_m \<br>&amp;&#x3D; \left[\begin{matrix}A_1 &amp; A_2 &amp; \cdots &amp; A_n \end{matrix} \right] \left[x \right]<br>\end{align}<br>$$</p><p>这里，可以把第一个矩阵摆成$$n$$行组成的列向量形式，也可以摆成$$m$$列的行向量的形式。<strong>其中第一种形式比较容易理解，因为这就是高斯消元法中$$n$$个线性方程组联立在一起的形式</strong>。但是第二种形式特别容易出错，搞晕，因为第二种形式的线性组合是数乘向量求和的形式，多做几遍就熟悉了。</p><div class="note note-primary">            <p><strong>第二种形式的意义：矩阵乘一个列向量相当于矩阵所有列的线性组合。</strong></p>          </div><p>同样的，一个行向量乘矩阵相当于矩阵所有行的线性组合，即<br>$$<br>y^T \mathbf{A} &#x3D; y^T \left[\begin{matrix}\alpha_1^T \ \alpha_2^T \ \cdots \ \alpha_n^T  \end{matrix} \right] &#x3D; y_1\alpha_1^T + y_2\alpha_2^T + \dots y_n \alpha_n^T<br>$$<br>因此矩阵乘法$$\mathbf{A}\mathbf{B} &#x3D; \mathbf{C}$$,可以看做$$\mathbf{C}$$的每一列，都是$$\mathbf{A}$$每一列的线性组合，组合系数是$$\mathbf{B}$$的对应列；$$\mathbf{C}$$的每一行可以看做是$$\mathbf{A}$$的每一行的线性组合，组合系数是$$\mathbf{A}$$的对应行。若$$\mathbf{A}\mathbf{B}&#x3D;0$$，那么$$\mathbf{B}$$的每一列向量都是$$\mathbf{A}x &#x3D; 0$$的齐次方程组的解。同理，$$\mathbf{A}$$的每一行也是$$y^T\mathbf{B}&#x3D;0$$的齐次方程组的解。</p><h3 id="方阵的多项式"><a href="#方阵的多项式" class="headerlink" title="方阵的多项式"></a>方阵的多项式</h3><p>$$<br>f(\mathbf{A}) &#x3D; a_0 \mathbf{I} + a_1 \mathbf{A} + a_2\mathbf{A}^2 + \dots + a_m\mathbf{A}^m<br>$$</p><p>称为$$\mathbf{A}$$的多项式。易知，$$f(\mathbf{A})g(\mathbf{A}) &#x3D; g(\mathbf{A})f(\mathbf{A})$$，同一方阵的多项式是可以交换的。</p><h2 id="行列式，迹，伴随矩阵，逆，秩等性质"><a href="#行列式，迹，伴随矩阵，逆，秩等性质" class="headerlink" title="行列式，迹，伴随矩阵，逆，秩等性质"></a>行列式，迹，伴随矩阵，逆，秩等性质</h2><h3 id="行列式"><a href="#行列式" class="headerlink" title="行列式"></a>行列式</h3><ol><li>$$\det{\mathbf{A}} &#x3D; |\mathbf{A}|$$</li><li>$$|\mathbf{AB}| &#x3D; |\mathbf{A}||\mathbf{B}|$$</li><li>当$$|\mathbf{A}| \ne 0$$时， $$\mathbf{AB} &#x3D; 0$$必有$$\mathbf{B}&#x3D;0$$,$$\mathbf{AB} &#x3D; \mathbf{AC}$$必有$$\mathbf{B}&#x3D;\mathbf{C}$$(满秩矩阵齐次方程有唯一解，就是0解)。</li><li>求解二阶行列式对角线法，求解三阶行列式用沙路法。求解$$n$$阶行列式用递推法（如下按照第$$i$$行展开）$$\left|\begin{matrix} a_{11} &amp; a_{12} &amp; \dots &amp; a_{1n} \ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \ a_{n1} &amp; a_{n2}&amp; \dots &amp;a_{nn}  \end{matrix} \right| &#x3D; \sum_j a_{ij}A_{ij}$$ (代数余子式忘了的自己wiki去)。最后完全展开的结果有$$n!$$项，每一项都是不同行不同列元素乘积，然后一半是正号，一半是负号。</li><li>n阶方程组有克莱默法则，理论意义上解释了线性方程组和其系数的关系。</li></ol><h3 id="矩阵的迹"><a href="#矩阵的迹" class="headerlink" title="矩阵的迹"></a>矩阵的迹</h3><ol><li><p>$$\tr{(\alpha\mathbf{A}+ \beta \mathbf{B})} &#x3D; \alpha\tr{\mathbf{A}} + \beta \tr{\mathbf{B}} $$</p></li><li><p>$$\tr{\mathbf{AB}} &#x3D; \tr{\mathbf{BA}}$$（如果分别是$$n \times m$$和$$m \times n$$的矩阵即可）</p></li><li><p>$$\tr{ \mathbf{A} \mathbf{A}^H} &#x3D; \sum_{i,j} |a_{ij}|^2$$（所有元素的平方和，故$$\tr{ \mathbf{A} \mathbf{A}^H} &#x3D; 0 \Longrightarrow \mathbf{A}&#x3D;0 $$）</p></li></ol><h3 id="矩阵的秩"><a href="#矩阵的秩" class="headerlink" title="矩阵的秩"></a>矩阵的秩</h3><ol><li>定义：矩阵所有不为0的子式中最高的阶数为矩阵的秩</li><li>如果一个矩阵的秩为1，那么一定存在列向量$$\alpha, \beta$$, 使得$$A &#x3D; \alpha\beta^T$$(列向量x行向量，得到矩阵的每一行都是$$\alpha_i \beta$$，显然任意两行都线性相关)。那么计算矩阵高次幂时$$\mathbf{A}^m &#x3D; \alpha(\beta^T\alpha)^{m-1}\beta^T $$</li><li>矩阵和和乘积的秩有如下不等式($$n \times p , p \times m$$)：</li></ol><p>$$<br>\begin{align}<br>r(\mathbf{A+B}) &amp;\le r(\mathbf{A}) + r(\mathbf{B}) \<br>r(\mathbf{A}) + r(\mathbf{B}) - p \le r(\mathbf{AB}) &amp; \le \min{r(\mathbf{A}), r(\mathbf{B})}<br>\end{align}<br>$$</p><details><summary>想看证明请展开</summary><p><b>截图自<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="《线性代数》 第二版 居余马 清华大学出版社">[1]</span></a></sup>P6</b></p><img width="680" src="https://i.loli.net/2021/10/11/NOefIEdJkPUxwVt.jpg" alt="图1"></details><h3 id="伴随矩阵，逆矩阵"><a href="#伴随矩阵，逆矩阵" class="headerlink" title="伴随矩阵，逆矩阵"></a>伴随矩阵，逆矩阵</h3><ol><li><p>伴随矩阵(如果是实矩阵通常用$$A^*$$，否则常用$$adj \mathbf{A}$$表示，个人喜好，即便是在复数域上也喜欢用$$A^*$$表示，敬请理解)。$$AA^* &#x3D; A^*A &#x3D; |A|I$$(证明可参考<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="《线性代数》 第二版 居余马 清华大学出版社">[1]</span></a></sup>中2.2节例题6)</p></li><li><p>$$\mathbf{A}^{-1} &#x3D; \frac{A^*}{|A|}$$</p></li><li><p>$$(\mathbf{A}^T)^{-1} &#x3D; (\mathbf{A}^{-1})^T$$</p></li><li><p>$$ |\mathbf{A}^{-1}| &#x3D; |\mathbf{A}|^{-1}$$</p></li><li><p>可逆矩阵与任何矩阵乘积不会改变原矩阵的秩 $$r(\mathbf{A}) &#x3D; r(\mathbf{PA}) &#x3D; r(\mathbf{AQ}) &#x3D; r(\mathbf{PAQ}) $$</p></li><li><p>矩阵满秩，非奇异，可逆三者概念等价</p></li></ol><h3 id="分块矩阵"><a href="#分块矩阵" class="headerlink" title="分块矩阵"></a>分块矩阵</h3><div class="note note-info">            <p>分块矩阵是一种非常好用的技巧。</p>          </div><ol><li><p>分块矩阵的加法和数乘就不说了🐒</p></li><li><p>分块矩阵乘法：对于$$n \times p , p \times m $$的两个矩阵相乘，假设把矩阵$$\mathbf{A}$$横切$$r$$刀，竖着切$$s$$刀(你就想象一下家里切豆腐凉拌),矩阵$$\mathbf{B}$$横着切$$s$$刀，竖着切$$t$$刀，显然分块后也是对应每一行x每一列。最后乘起来得到的是$$r \times t$$块<del>油炸豆腐</del> 矩阵。但由于分块乘法，乘法本身也是矩阵乘法，所以也要满足$$\mathbf{A}$$块的列数&#x3D;$$\mathbf{B}$$块的行数。所以在<del>豆腐下锅</del>矩阵分块乘法前，第一个矩阵竖着切的每一刀的宽度要和第二块豆腐横着切的每一刀宽度相同</p><p> (脑子全是香喷喷的铁锅油炸豆腐，浇上味极鲜酱油撒上葱花后热气腾腾地出锅……)</p></li></ol><p>E.g.</p><details><summary>想看例子请戳我，不想看可忽略</summary><p><b>截图自<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="《线性代数》 第二版 居余马 清华大学出版社">[1]</span></a></sup>P82</b></p><img width="680" src="https://i.loli.net/2021/10/11/ktlFSeGERgNmb4B.png" alt="图1"><img width="680" src="https://i.loli.net/2021/10/11/OGQ1cNxFh4J2Uny.png" alt="图1"></details><ol start="3"><li>分块矩阵的转置(自己想，想不出来wiki去)</li><li>分块矩阵的逆矩阵：</li></ol><p>对角分块阵(假如$$\mathbf{A}<em>i$$都可逆)<br>$$<br>\left[\begin{matrix}\mathbf{A}<em>1 &amp;  \  &amp;\mathbf{A}<em>2&amp; \ &amp; &amp;\ddots&amp; \ &amp;&amp;&amp; \mathbf{A}<em>k \end{matrix} \right]^{-1} &#x3D; \left[\begin{matrix} \mathbf{A}^{-1}<em>1 &amp;  \  &amp;\mathbf{A}^{-1}<em>2&amp; \ &amp; &amp;\ddots&amp; \ &amp;&amp;&amp; \mathbf{A}^{-1}<em>k  \end{matrix}  \right]<br>$$<br>普通矩阵，分成几块后做乘法，比如设:<br>$$<br>\left[\begin{matrix}\mathbf{A}</em>{11} &amp; \mathbf{A}</em>{12}   \ \mathbf{A}</em>{21} &amp; \mathbf{A}</em>{22} \  \end{matrix} \right]^{-1} &#x3D; \left[\begin{matrix}\mathbf{X} &amp; \mathbf{Y}  \ \mathbf{U} &amp; \mathbf{V} \  \end{matrix} \right]<br>$$<br>可得<br>$$<br>\begin{align}<br>\mathbf{A}</em>{11}\mathbf{X} + \mathbf{A}</em>{12}\mathbf{U} &amp;&#x3D; \mathbf{I} \<br>\mathbf{A}</em>{11}\mathbf{Y} + \mathbf{A}<em>{12}\mathbf{V} &amp;&#x3D; 0 \<br>\mathbf{A}</em>{21}\mathbf{X} + \mathbf{A}<em>{22}\mathbf{U} &amp;&#x3D; 0 \<br>\mathbf{A}</em>{21}\mathbf{Y} + \mathbf{A}_{22}\mathbf{V} &amp;&#x3D; \mathbf{I}<br>\end{align}<br>$$<br>这样可以把一个高阶的矩阵求逆问题转化成低阶矩阵求逆问题。结合<strong>分治法</strong>和一些<strong>奇奇怪怪</strong>的trick，可以像矩阵乘法一样从$$O(n^3)$$进行时间复杂度优化。</p><ol start="5"><li>分块矩阵的初等变换(以$2 \times 2$为例， 以后填坑)</li></ol><p>分块对换阵<br>$$<br>\left[\begin{matrix}<br>0 &amp; \mathbf{I} \<br>\mathbf{I} &amp; 0<br>\end{matrix} \right]<br>$$</p><p>分块倍乘阵<br>$$<br>\left[\begin{matrix}<br>\mathbf{C}_1 &amp; 0 \<br>0 &amp; \mathbf{I}<br>\end{matrix} \right]<br>$$<br>分块倍加阵<br>$$<br>\left[\begin{matrix}<br>\mathbf{I} &amp; 0 \<br>\mathbf{C}_1 &amp; \mathbf{I}<br>\end{matrix} \right]<br>$$</p><h2 id="线性方程组"><a href="#线性方程组" class="headerlink" title="线性方程组"></a>线性方程组</h2><div class="note note-warning">            <p>这一块讲的非常简略，因为学过线代的人大多数这一章掌握的比较牢靠(往往相似矩阵，矩阵分解，二次型那一块快到期末，掌握得通常不太好)。所以省下笔墨只零零星星地提几点。</p>          </div><ol><li>（<strong>非常重要</strong>）$${\alpha_1, \alpha_2, \dots, \alpha_m }$$ 线性无关 $$\Leftrightarrow$$ $$k_1\alpha_1 + k_2\alpha_2 + \dots + k_m\alpha_m &#x3D; 0$$仅有0解</li><li>（<strong>非常重要</strong>）<strong>极大线性无关组</strong>的定义。(此外，极大线性无关组可以作为方程组的一个基础解系)</li><li>（<strong>非常重要</strong>）齐次线性方程组基本定理：基础解系的向量数量&#x3D; $$n - r(\mathbf{A})$$(从这里可以看出齐次方程组的解空间是$$\mathbb{F}^{n \times m}$$的子空间， 正因为系数矩阵有了秩(序)，才导致解空间维度比原来的空间小了，变成了基础解系的向量数量)</li><li>线性方程组基本定理：是否有解(判别条件)；通解和特解。</li><li>具体计算：化简为Hermitte标准形(行阶梯矩阵标准型)。$$\left[\begin{matrix} \mathbf{I} &amp; \mathbf{A} \ 0 &amp; 0 \end{matrix} \right]$$</li></ol><div class="note note-success">            <blockquote><p>“有的人天生就是战士。他&#x2F;她不会惧怕一切困难，也不会为一切失败与挫折低头，因为他&#x2F;她的目标是星辰大海”⭐️</p></blockquote>          </div><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>《线性代数》 第二版 居余马 清华大学出版社<a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>《矩阵理论与应用》 张跃辉  上海交通大学出版社<a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
      <category>矩阵理论</category>
      
    </categories>
    
    
    <tags>
      
      <tag>课程笔记</tag>
      
      <tag>矩阵，线性代数</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔-关于高维空间样本分布稀疏性的联想</title>
    <link href="/2021/10/09/%E9%9A%8F%E7%AC%94-%E5%85%B3%E4%BA%8E%E9%AB%98%E7%BB%B4%E7%A9%BA%E9%97%B4%E6%A0%B7%E6%9C%AC%E5%88%86%E5%B8%83%E7%A8%80%E7%96%8F%E6%80%A7%E7%9A%84%E8%81%94%E6%83%B3/"/>
    <url>/2021/10/09/%E9%9A%8F%E7%AC%94-%E5%85%B3%E4%BA%8E%E9%AB%98%E7%BB%B4%E7%A9%BA%E9%97%B4%E6%A0%B7%E6%9C%AC%E5%88%86%E5%B8%83%E7%A8%80%E7%96%8F%E6%80%A7%E7%9A%84%E8%81%94%E6%83%B3/</url>
    
    <content type="html"><![CDATA[<h1 id="随笔：关于高维空间样本分布稀疏性的联想"><a href="#随笔：关于高维空间样本分布稀疏性的联想" class="headerlink" title="随笔：关于高维空间样本分布稀疏性的联想"></a>随笔：关于高维空间样本分布稀疏性的联想</h1><h2 id="背景提要"><a href="#背景提要" class="headerlink" title="背景提要"></a>背景提要</h2><pre><code class="hljs">     在高维的向量空间中，假设有限个数的样本从一个均匀分布随机采样得到的，那么可以证明这些样本大多会倾向于分布在高维球体或立方体的面都附近(Surface)，而且即便是最近的两个样本之间的距离也会非常远，因为高维空间实在太稀疏了。比如在卷积神经网络使用的向量维度通常是1024维或2048维。     我一直在思考有没有一个形象化的比喻来直观地解释，或者类比一下这个现象。先想到了两个例子，但可能原理不同，也不一定贴切。</code></pre><h2 id="宇宙膨胀"><a href="#宇宙膨胀" class="headerlink" title="宇宙膨胀"></a>宇宙膨胀</h2><pre><code class="hljs">     现代的宇宙学最大的一个发现就是，我们所处的宇宙是在不断膨胀的，就像一个正在不断被吹胀的气球一样，我们周围的星系在不断离我们远去。但是据说我们的宇宙的维度可能有11维(包括宏观维度和微观物质的维度)，即便我们的宇宙是高维空间，也不像是1024或2048维这么高的维度；而且宇宙空间会纯粹的数学上的向量空间是两个完全不同的系统，即便在某些性质上可能会有相似的交集。</code></pre><p>​突然想起一句台词，”我们像星星一样近，我们像星星一样远。我们近得摩擦生电，我们远如天际云端。“</p><h2 id="社交距离-amp-人际关系的疏远"><a href="#社交距离-amp-人际关系的疏远" class="headerlink" title="社交距离&amp;人际关系的疏远"></a>社交距离&amp;人际关系的疏远</h2><pre><code class="hljs">     我想到的一个完全不相干，但是又很奇妙的比喻：人际关系的亲疏远近。有个比较普遍的现象：小时候很容易交到朋友和玩伴，建立一段Friendship的障碍没有那么大，小孩子之间的社交距离很近；但是随着我们逐渐长大，经历的事情越来越多，我们的心灵越来越社会化，会越发现建立一段Relationship会变得愈发困难。进入社会工作后，自己身边都是”同事“和陌生人，大家都很忙，彼此不想过度打扰，即便是你感觉再好的朋友距离也会比小时候变远，就好像在一段数轴上均匀分布的5个点可能离得很近，但是如果把这5个点随机撒入几十维的高维球中，那么即便是最近的两个点，其欧氏距离也比一维坐标轴上的距离要更遥远。</code></pre><p>​那么比喻的相似点在哪里呢？年龄小的时候，我们所生活在一个低维的世界中，在这个世界中只有”吃饭“，”睡觉“，”写作业“，”玩“等较少的维度中。维度越低的时候，也是我们的心智越稚嫩简单的时候，对陌生人和这个世界容易充满信任。当我们慢慢长大，上大学，读研，工作，建立家庭，我们的生活维度在不断扩张，直到变成一个高维空间。在这个高维空间里，生活和工作中的遇到的人际关系会更加复杂，再也不是在象牙塔里如此的单纯，仿佛整个世界只由几个简单的维度构成。生活在一个高维空间里是很容易疲惫，就好像训练一个高维的神经网络模型一样困难。自然而然，在这个高维空间里，如果把每个人作为这个社会随机采样下的样本，样本之间的距离也会随之自然变远。</p><pre><code class="hljs">     PS: 居然查到了有《社会统计学》这门课，不过真正的社会统计学肯定不是像我这般的胡思乱想。     &quot;无尽的远方,无数的人们,都与我有关。&quot; 希望这句话能带给我一些安慰。</code></pre>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>高维空间， 维度灾难</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔：那些在CV里用腻了的话术与说辞</title>
    <link href="/2021/10/09/%E9%9A%8F%E7%AC%94-%E9%82%A3%E4%BA%9B%E5%9C%A8CV%E9%87%8C%E7%94%A8%E8%85%BB%E4%BA%86%E7%9A%84%E8%AF%9D%E6%9C%AF%E4%B8%8E%E8%AF%B4%E8%BE%9E/"/>
    <url>/2021/10/09/%E9%9A%8F%E7%AC%94-%E9%82%A3%E4%BA%9B%E5%9C%A8CV%E9%87%8C%E7%94%A8%E8%85%BB%E4%BA%86%E7%9A%84%E8%AF%9D%E6%9C%AF%E4%B8%8E%E8%AF%B4%E8%BE%9E/</url>
    
    <content type="html"><![CDATA[<h1 id="随笔：那些在CV里用腻了的话术与说辞"><a href="#随笔：那些在CV里用腻了的话术与说辞" class="headerlink" title="随笔：那些在CV里用腻了的话术与说辞"></a>随笔：那些在CV里用腻了的话术与说辞</h1><p>究其根源，深度学习是一个黑盒子(Black Box)，缺乏一套能够解释清楚深度神经网络的运作机理的数学理论，比如，能否解释深度网络究竟从海量数据中学到了什么，深度网络的反向传播过程的收敛条件，训练深度模型为什么花费那么长时间，究竟有多少时间是对学习真正有用的。由于缺乏理论支撑，很多搞深度学习应用的(如CV, NLP)大多follow这样的研究路线：通过紧跟研究”热点“，读paper参会看别人提出了哪些模型，然后想到一个”灵感“，提出一套自己魔改的结构，抱着试一试的心态做实验发现work了，再来解释一通，就去投稿发paper。所以很多paper(尤其搞CV&#x2F;NLP + DL)都是从性能结果反向解释自己的神经网络结构，并且很多解释是基于经验和直觉的。</p><p>读多了后，发现有一些话术是比较Common的技巧，就是很多工作都愿意用上这些技巧来提点，写作时用上相似的话术去讲故事，总觉一下就觉得比较有意思。以后有空会继续补充。</p><ol><li>Multi-Resolution&#x2F;Hierarchical&#x2F;Pyramid Sturcture&#x2F;Extract Feature at diferent scales</li><li>不同层次&#x2F;级别的特征如何融合：Feature Fusion</li><li>分类问题&#x2F;多模态问题&#x2F;迁移学习问题: Intra-class variance &amp; Inter-class Variance(最小化类内XXX，最大化类间XXX,以保留判别性信息)</li><li>Global Part + Local Part 的多分支设计</li><li>Local Discriminative Part</li><li>粗粒度+细粒度(Coarse + Fine的设计)</li><li>Grouping操作，把相似的归纳到一起(比如聚类，相似度打分)，分类做loss</li><li>Feature Alignment, Domain Alignment, Pixel Alignment；各种Alignment</li><li>Meta Learning + 各种领域；就近年会议review情况来看，元学习快被用烂了(烂大街的感觉)，基本是所有reviewer都觉得没啥可用的。把他作为主要创新点容易被喷novelty不够。</li></ol><p>(未完待续…)</p>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>CV</tag>
      
      <tag>话术</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>课程笔记：统计学习理论与方法(ELS_Chap2)</title>
    <link href="/2021/09/30/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA%E4%B8%8E%E6%96%B9%E6%B3%95-ELS-Chap2/"/>
    <url>/2021/09/30/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA%E4%B8%8E%E6%96%B9%E6%B3%95-ELS-Chap2/</url>
    
    <content type="html"><![CDATA[<h1 id="课程笔记：统计学习理论与方法-ELS-Chap2"><a href="#课程笔记：统计学习理论与方法-ELS-Chap2" class="headerlink" title="课程笔记：统计学习理论与方法(ELS_Chap2)"></a>课程笔记：统计学习理论与方法(ELS_Chap2)</h1><p>📚书籍：《Elements of Statistical Learning》Chap 2 </p><h2 id="符号约定"><a href="#符号约定" class="headerlink" title="符号约定"></a>符号约定</h2><table><thead><tr><th align="center">输入变量(Input Variable)</th><th align="center">$X$, 通常随机变量取值是$$p$$维随机向量($$X\in \mathbb{R}^p$$),其每个分量可用下角标$$X_i$$表示是该维度特征的变量</th></tr></thead><tbody><tr><td align="center">(随机)变量$X$的第i个观测值</td><td align="center">$$x_i$$, 可以是向量或标量</td></tr><tr><td align="center">模型输出</td><td align="center">Quantative Outputs:$$Y$$, Category: $$G$$； 模型预测值$$\hat{Y}$$</td></tr><tr><td align="center">矩阵</td><td align="center">大写加粗正体$$\textbf{X}$$，如$$N$$个维度为$$p$$的向量$$x_1, x_2, \dots, x_N$$组成$$N \times p$$的矩阵$$\textbf{X}$$</td></tr><tr><td align="center">向量</td><td align="center">约定:普通的$$p$$维向量表示成$$x_j$$，不用加粗; 而维度为N时，若加粗为$$\textbf{x}_i$$，代表的是随机变量第$i$个特征分量的所有观测值(N个观测值)组成的向量。E.g. $$x_j^T$$是矩阵$$\textbf{X}$$的第$$j$$行，$$\textbf{x}_i$$是矩阵的第$$i$$列。所有的向量规定为列向量的形式。</td></tr></tbody></table><h2 id="统计学习的重要原则"><a href="#统计学习的重要原则" class="headerlink" title="统计学习的重要原则"></a>统计学习的重要原则</h2><p>Statistical Learning的目标是为了最好的泛化性能(Best Generalization Property)而不是最小的训练误差(Minimum Training Error)</p><h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><ol><li>设输入是一个<strong>列向量</strong>$X$, 其每个维度分量下标是$X_i$<br> $$<br> \begin{align}<br> \hat{f}(x) &amp;&#x3D; \sum_{i&#x3D;1}^{p} \hat{\beta}_iX_i + \hat{\beta}_0 \<br>                  &amp;&#x3D; \hat{\beta}^TX \<br>                  &amp;&#x3D; X^T\hat{\beta}<br> \end{align}<br> $$<br> 写成矩阵形式，其中$$\textbf{X} \in \mathbb{R}^{N \times p} $$</li></ol><p>$$<br>\hat{Y} &#x3D; \textbf{X} \hat{\beta}<br>$$</p><ol start="2"><li><p>线性回归的误差函数衡量方式——最小二乘<br> $$<br> \begin{align}<br> L(Y, \hat{f}(x)) &amp;&#x3D; (Y-\hat{f}(x))^T(Y- \hat{f}(x)) \<br> &amp;&#x3D; (Y- \textbf{X}\hat{\beta})(Y- \textbf{X}\hat{\beta}) \<br> &amp;&#x3D; L(\hat{\beta})<br> \end{align}<br> $$</p></li><li><p>从最小化误差函数的方式，对$$\hat{\beta}$$求导，得：<br> $$<br> \begin{align}</p><p> \frac{\partial{L}}{\partial \hat{\beta}} &amp;&#x3D; -2\textbf{X}^T(Y- \textbf{X}\hat{\beta}) &#x3D; 0 \<br> \hat{\beta} &amp;&#x3D; (X^TX)^{-1}X^TY<br> \end{align}<br> $$</p></li><li><p>思考一下两种数据分布情况对线性回归模型影响：</p><ol><li>训练数据中的每一类都是服从高斯分布，而且每一类的方差一样但是均值不同。</li><li>某一类的训练数据是由多个高斯分布合成的，可能高斯分布的方差和均值都不太一样，</li></ol><p> 对于第1种情况线性回归是比较optimal的，但是对于第二种效果比较差。</p><p> 而K近邻算法对于第2中情况相对会更suitable.</p><p> (先占坑，后续补上解释)</p><p> 如下图所示，先从$$N((1,0)^T, \textbf{I})$$和$$N((0,1)^T, \textbf{I})$$ 中分别随机采样10个sample(共计20个)，前十个作为类别1的均值，后十个作为类别2的均值。然后每个类中各生成100个sample，每个sample都是从$$N(\mu, \frac{\textbf{I}}{5})$$采样，而每次$$\mu$$是从该类中的10个中等概率($$\Pr &#x3D; \frac{1}{10}$$)抽样得到。</p> <img src="https://i.loli.net/2021/09/27/4akOVlu9Q8ex6Wc.png" alt="image-20210927135914964" style="zoom:50%;" /><p> 圆点连线代表的K近邻的Error曲线，两个方块代表三元线性回归的Error值。可以看到:</p><ul><li>随着$$k$$值变小，K近邻模型自由度$$N&#x2F;k$$在变大，模型复杂度在上升。模型逐渐由欠拟合转变为过拟合。后面也会讲解</li><li>可以看到在这种多高斯分布混合采样得到的数据集中，K近邻最好的Error要比线性回归好。</li></ul></li></ol><h2 id="K近邻"><a href="#K近邻" class="headerlink" title="K近邻"></a>K近邻</h2><ol><li><p>几何意义：</p><blockquote><p>在特征空间中，对每个训练实例点$x_i$,距离该点比其他点更近的所有点组成一个区域，叫做单元(Cell).每个训练实例点都拥有一个单元，所有训练实例点的单元对特征空间构成一个划分。 <sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="《统计学习方法》第二版 李航">[1]</span></a></sup></p><img src="https://i.loli.net/2021/09/27/bQgTdCXlYJNnRzW.png" alt="image-20210927144131480" style="zoom:50%;" /></blockquote></li><li><p>具体内容参见李航《统计学习方法》Chap3 课程笔记(先占坑，后面补上笔记后建立链接🔗)</p></li></ol><h2 id="统计决策理论-Statistical-Decision-Theory"><a href="#统计决策理论-Statistical-Decision-Theory" class="headerlink" title="统计决策理论(Statistical Decision Theory)"></a>统计决策理论(Statistical Decision Theory)</h2><p>根据模型选择(Model Selection)的理论，通过最小化<strong>期望预测误差</strong>(Expected Prediction Error, EPE)来确定模型中的参数选择，从而确定模型。<br>$$<br>EPE(f) &#x3D; E(L(Y, \hat{f}(X)))<br>$$</p><h3 id="对于回归问题的EPE"><a href="#对于回归问题的EPE" class="headerlink" title="对于回归问题的EPE"></a>对于回归问题的EPE</h3><p>对于回归问题(不仅仅是线性回归)，<a href="">大多采用最小均方误差的方法计算误差</a>:<br>$$<br>\begin{align}<br>EPE(f) &amp;&#x3D; E(L(Y, \hat{f}(X)) \<br>             &amp;&#x3D; E_{X,Y}((Y - \hat{f}(X))^2) \<br>             &amp;&#x3D; \int\int (Y-f(X)^2)P(x, y) dxdy \<br>             &amp;&#x3D; \int_X {\int_Y(Y- f(X))^2P(y|x)dy}P(x) dx \<br>             &amp;&#x3D; E_X E_{Y|X}(Y - f(X))^2 \<br>\end{align}<br>$$<br>故，若想使得EPE取最小值，只需让对于每个$$X&#x3D;x$$, 使得$$E_{Y|X}$$最小即可(逐点最小化)，可得<br>$$<br>\begin{align*}<br>E_{Y|X}(Y- \hat{f}^2(X)) &amp;&#x3D; E_{Y|X}(Y^2 - 2\hat{f}(X)Y + \hat{f}^2(X)) \<br>&amp;&#x3D;E(Y^2)- 2\hat{f}(X)E(Y) + \hat{f}^2(X) \<br>\end{align*}<br>$$<br>故<br>$$<br>\hat{f}(X) &#x3D; E_{Y|X}(Y) &#x3D; E(Y | X&#x3D;x)<br>$$<br><strong>含义解释</strong>： 理论上期望预测误差最小的回归函数模型，<u>在最小二乘误差意义下</u>，每个训练数据当$$X&#x3D;x$$时，取$$Y$$的平均值($$E(Y)$$)是能够达到的。</p><p>注意，在这里我们认为所有训练数据的$$X$$与$$Y$$是两个独立的随机变量，分别取样自不同的分布中组成了联合概率分布。这是一个重要的assumption，也是统计学习里的基本思路。</p><p>另外$$X&#x3D;x$$可以拓展为$$X \in \epsilon(x)$$,在某个邻域附近的所有样本的$$Y$$值取平均。</p><div class="note note-info">            <p>关于期望$E(Y|X)$和$E_X(Y| X&#x3D;x)$，条件期望中的角标，随机变量和事件区别，符号大小写的不同意义等细微区别，参见随笔XXX（先占坑），以帮助理解。</p><hr><p>2021.10.08 简要补充</p><p>首先要区分随机变量和事件，比如对于随机变量$X$, $X&#x3D;x$是一个事件，即$X$取一个固定的值<strong>这件事情</strong>发生了，所以是一个事件。而随机变量$X$自身是有可能取任何一个值的，只不过有不同的概率(所以为什么随机变量会有概率分布这个概念)。</p><p>所以我们说条件期望时，就是指$E(Y|X&#x3D;x)$，即在$X&#x3D;x$事件发生的条件下，$Y$取各种值的可能性的期望。 $E(Y|X&#x3D;x) &#x3D; \sum_{y \in Y} p(Y&#x3D;y|X&#x3D;x)y$。</p><p>此外事件的独立和变量的独立不是一回事。n个事件的独立要求n个事件两两独立，任意三个独立，任意四个独立……任意n个独立。但是随机变量的独立只需要 $$P(X_1, X_2, \dots, X_n ) &#x3D; \prod_i X_i $$一个条件即可。因为每个随机变量可以取很多值，包含了很多$X_i &#x3D; x$的事件，所以显然约束要比事件独立 要强得多。</p><p>在信息论中，信息熵和条件熵，互信息与条件互信息中，条件部分是随机变量$X$还是事件$X&#x3D;x$是大有讲究的。比如见此图：</p><img src="https://i.loli.net/2021/10/08/2Hr4F1ec7hJEKoI.png" alt="image-20211008200426280" style="zoom: 33%;" /><p>以互信息$$I(X;Y|Z,V)$$和$I(X;Y|Z&#x3D;z,V)$的关系为例，如下图公式所示<br>$$<br>I(X;Y|Z,V) &#x3D; \sum_Z p(Z&#x3D;z)I(X;y|Z&#x3D;z, V)<br>$$</p><p>对于条件信息熵:<br>$$<br>\begin{align}<br>H(X|Y) &amp;&#x3D; \sum_{y \in Y} p(Y&#x3D;y) H(X|Y&#x3D;y) \<br>&amp;&#x3D; \sum_{y \in Y} p(Y&#x3D;y) \sum_{x \in X} p(X&#x3D;x|Y&#x3D;y) \log\frac{1}{p(X&#x3D;x|Y&#x3D;y)} \<br>&amp;&#x3D; \sum_{X,Y} p(x,y)\log\frac{1}{p(X&#x3D;x|Y&#x3D;y)}</p><p>\end{align}<br>$$</p>          </div><h3 id="对于分类问题的EPE"><a href="#对于分类问题的EPE" class="headerlink" title="对于分类问题的EPE"></a>对于分类问题的EPE</h3><p>先做一些符号约定：</p><p>分类问题的输出是离散型(随机)变量，定义其符号为$$G$$ (Group),属于同一类的值在同一个Group里，以此来表示类别信息。<br>$$<br>\begin{align}<br>EPE &amp;&#x3D; E(L(G, \hat{G}(X))) \<br>&amp;&#x3D; E_X E_{Y|X}(L(G, \hat{G}(X))) \<br>&amp;&#x3D; E_X \sum_k  P(Y&#x3D;G_k| X&#x3D;x)L(G_k, \hat{G}(X)) \<br>\end{align}<br>$$<br>同理，通过逐点最小化，只需要<br>$$<br>\hat{G}(X) &#x3D; \underset{g}{\operatorname{arg\min}}, {L(G_k, g)P(Y&#x3D;G_k|X&#x3D;x)}<br>$$<br>原因比较显然，哪个样本属于类的概率大，并且损失函数对于分类错误的惩罚大小共同决定了EPE。对于二分类+ 0-1 Loss来说(分类正确L为0，否则L为1)，那么可得<br>$$<br>\hat{G}(x) &#x3D; \underset{g \in {0,1}}{\operatorname{arg\max}}, P(Y&#x3D;G_g|X&#x3D;x)<br>$$<br>这个分类方式被称为<code>贝叶斯分类器</code>（Bayes classification）: 我们通过$$P(G_k|X)$$把样本分类到最可能的类别中(classify to the most probable class)。贝叶斯分配器得到的Error Rate(Bayes Rate)是所有分类模型中统计学理论上误差最小的。</p><div class="note note-warning">            <p>经过刚才的讲解，读者应该能较为清楚的感受到，统计学习中把$$X$$,$$Y$$作为两个独立的随机变量研究他们的联合分布，本质上研究的是数据与Label的**<u>相关性</u>**信息。即几乎目前绝大多数有监督学习的方法(机器学习,深度神经网络等)本质上都是学习数据与Label的分布和相关性信息。所以说这类模型是数据驱动(data-driven)的，机器并没有理解数据，只是学习到数据的分布而已。</p>          </div><p>k近邻及其Majority Vote选出类别的机制，本质上也是通过训练数据去近似$$P(Y&#x3D;G_g|X&#x3D;x)$$。而当$$k ,N \rightarrow \infty，\frac{k}{N} \rightarrow 0 $$时，K近邻近似得到$$E(Y | X&#x3D;x)$$。通过这个例子，我们也发现EPE的结果在回归和分类问题上本质上是一致的。</p><h2 id="Bias-Variance-Decomposition"><a href="#Bias-Variance-Decomposition" class="headerlink" title="Bias-Variance Decomposition"></a>Bias-Variance Decomposition</h2><p>先考虑在某个点$$ X&#x3D;x_0 $$上的MSE拆分：<br>$$<br>\begin{align}<br>MSE(x_0) &amp;&#x3D; E_\tau[f(x_0) - \hat{y_0}]^2 \<br>&amp;&#x3D; E_\tau [f^2(x_0) - 2f(x_0)\hat{y}_0 + \hat{y_0}^2] \<br>&amp;&#x3D; E_\tau [\hat{y_0}^2 - 2\hat{y_0}E_\tau(\hat{y_0}) + E_\tau^2(\hat{y_0}) + f^2(x_0) - 2f(x_0)\hat{y_0} + 2\hat{y_0}E_\tau(\hat{y_0}) - E_\tau^2(\hat{y_0})] \<br>&amp;&#x3D; E_\tau[(\hat{y_0} - E_\tau(\hat{y_0}))^2] + f^2(x_0) - 2f(x_0)E_\tau(\hat{y_0}) + E_\tau^2(\hat{y_0}) \<br>&amp;&#x3D; E_\tau[(\hat{y_0} - E_\tau(\hat{y_0}))^2] + [f(x_0) - E_\tau(\hat{y_0})]^2<br>\end{align}<br>$$</p><p>设训练集为$$\tau$$, 函数$$f(x)$$为准确的预测模型(has no bias), 计算**$$X&#x3D;x_0$$这一测试点(test point)**上的最小均方误差。由于$$f(x)$$为精准模型，因此其变化只与输入有关，因此在$$\tau$$的期望上可以看做是常数，就有如上变换。</p><p>其中第一项是预测模型$$\hat{y_0}$$自身的方差， 第二项是预测模型与真实模型之间的Bias。对于方差来说，是由于对计算$$y_0$$时带来的数据方差(通常假设是$$N(0,1)$$的高斯噪声，方差由此带来)。对于Bias而言，一方面由于在$$x_0$$的邻域里采样，另一方面由于预测模型总会有误差($$N(0,1)$$高斯噪声的均值)。</p><div class="note note-warning">            <p>注意这里算MSE拆分时的$$f(x_0)$$与数据集$$\tau$$中的随机变量$$Y$$的取值不等价的。因为数据集中的标签$$Y$$往往充满噪声的，而我们假设理想数据集是满足某种分布$$f(x)$$，然后再该分布上加上噪声$$\epsilon$$才得到了数据集中的真实的$$Y$$。所以算bias时 $$ f(x_0) - E_\tau(\hat{y_0})$$ 是不能写成$$ Y - E_\tau(\hat{y_0}) $$的。</p><p>后面我们算EPE的拆分时，把数据集中的真实$Y$放了进去，因此拆分中多了关于噪声的方差项。详细见下面介绍。</p>          </div><h3 id="对于线性回归模型-Y-x3D-X-hat-beta-x3D-X-beta-epsilon"><a href="#对于线性回归模型-Y-x3D-X-hat-beta-x3D-X-beta-epsilon" class="headerlink" title="对于线性回归模型 $$Y &#x3D; X\hat{\beta} &#x3D; X\beta + \epsilon$$"></a>对于线性回归模型 $$Y &#x3D; X\hat{\beta} &#x3D; X\beta + \epsilon$$</h3><p>假设数据集 $$\tau$$中$$Y$$与$$X$$ 成近似线性分布，假设中间差个高斯噪声$$N(0,I_p)$$</p><p>由于<br>$$<br>\begin{align}<br>\hat{\beta} &amp;&#x3D; (X^TX)^{-1}X^Ty \<br>&amp;&#x3D; (X^TX)^{-1}X^T(X \beta + \epsilon) \<br>&amp;&#x3D; \beta + (X^TX)^{-1}X^T \epsilon<br>\end{align}<br>$$</p><p>可得到<br>$$<br>E(\hat{\beta}) &#x3D; \beta \<br>Var(\hat{\beta}) &#x3D; Var((X^TX)^{-1}X^T \epsilon) &#x3D; A^T Var(\epsilon) A &#x3D; A^TA &#x3D; Var(XX^T)^{-1} , ???对吗<br>$$<br>故知道对于$$\beta$$的估计$$\hat{\beta}$$是无偏估计，因而得到在均方误差中线性回归模型满足$$ E(X\beta)-E(X\hat{\beta})&#x3D;0$$即$$f(x_0)-E_\tau(\hat{f}(x_0)) &#x3D; 0$$, bias 为0。</p><p>并且在$$X&#x3D;x_0$$这一测试点(test point)<br>$$<br>\hat{y_0} &#x3D; \hat{x_0}^T\beta + x_0^T\sum_{i&#x3D;1}^N l_i(x_0)\epsilon_i \<br>where,, l_i(x_0) &#x3D; (x_0^T(X^TX)^{-1}X^T)<em>i<br>$$<br>下面，我们对整个数据集(over training set $$\tau$$)的<strong>EPE</strong>进行bias-variance拆分。先考虑固定$$x_0$$,  让$$y_0$$变<br>$$<br>\begin{align}<br>EPE(x_0) &amp;&#x3D; E</em>{y_0 | x_0} E_\tau(y_0 - \hat{y_0})^2 \<br>&amp;&#x3D; Var(y_0|x_0) + E_\tau ((\hat{y_0} - E_\tau(\hat{y_0})^2)) + (E_\tau \hat{y_0} - x_0^T \beta)^2 \<br>&amp;&#x3D; \sigma^2(\epsilon) + E_\tau x_0^T(X^TX)^{-1}x_0 \sigma^2(\epsilon) + 0^2<br>\end{align}<br>$$</p><h2 id="维度灾难-Curse-of-Dimensionality"><a href="#维度灾难-Curse-of-Dimensionality" class="headerlink" title="维度灾难(Curse of Dimensionality)"></a>维度灾难(Curse of Dimensionality)</h2><p>待填坑</p><h2 id="模型选择与误差-Model-Selection-and-Bias"><a href="#模型选择与误差-Model-Selection-and-Bias" class="headerlink" title="模型选择与误差(Model Selection and Bias)"></a>模型选择与误差(Model Selection and Bias)</h2><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>《统计学习方法》第二版 李航<a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
      <category>统计学习理论与方法</category>
      
      <category>教材笔记</category>
      
      <category>Elements of Statistical Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>课程笔记</tag>
      
      <tag>教材笔记</tag>
      
      <tag>统计学习理论与方法</tag>
      
      <tag>ELS</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>课程笔记：强化学习(2)——Bellman方程的动态规划求解</title>
    <link href="/2021/09/29/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/"/>
    <url>/2021/09/29/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A02/</url>
    
    <content type="html"><![CDATA[<h1 id="课程笔记：强化学习-2-——Bellman方程的动态规划求解"><a href="#课程笔记：强化学习-2-——Bellman方程的动态规划求解" class="headerlink" title="课程笔记：强化学习(2)——Bellman方程的动态规划求解"></a>课程笔记：强化学习(2)——Bellman方程的动态规划求解</h1><h2 id="动态规划"><a href="#动态规划" class="headerlink" title="动态规划"></a>动态规划</h2><div class="note note-info">            <p>本节只是非常粗略的把动态规划的要点总结一下，是课堂笔记纲要，并非详细讲解。先占个坑，以后有空再补</p>          </div><ol><li><p>动态规划需要满足的条件：</p><ol><li><p>最优子结构(Optimal substructure):</p><ul><li><p>原问题的最优解一定也是子问题的最优解。例如，一个输入长度为N的序列的某个Scheduling问题（记为T(1，N) ) 的最优解，拆分后一定也是其子问题T(1,N&#x2F;2-1), T(N&#x2F;2, N)的最优解。</p></li><li><p>子问题的最优解可以通过某种形式组合和计算，得到原问题的最优解</p></li></ul></li><li><p>重叠子问题</p><ul><li>子问题与原问题有同样的format和structure</li></ul></li><li><p>无后效性</p><ul><li>计算原问题最优解时，不会对子问题最优解产生影响。</li></ul></li></ol> <div class="note note-success">            <p>马尔科夫链和马尔科夫决策过程满足上述三个条件的，因为:</p><ul><li>Bellman方程将原问题递归的分解为规模更小的子问题</li><li>子问题拥有同样的format和structure</li><li>Que？ 有疑问，虽然马尔科夫过程强调每个state只与上一个state有关，与其他state无关($$P(x_n|x_1,x_2,\dots,x_{n-1}) &#x3D; P(x_n| x_{n-1})$$)，但这是保证MDP无后效性的原因嘛？因为通过迭代的方式求解贝尔曼方程，value早晚会传播到很多时间段以后的。</li></ul>          </div></li><li><p>Dynamic Planning in MDP:</p><ol><li>For prediction(Policy Evaluation):<br> Input: MDP $$&lt;S,A,P,R,\gamma&gt;$$ 和策略 $ \pi $ or MRP$$&lt;S, P^\pi, R^\pi, \gamma&gt;$$(没有显示的策略表示，是Implicit Policy)<br> Output: value function $$v_\pi$$<br> 对Policy Evaluation的理解：给定一个策略，通过贝尔曼方程迭代求出所有状态的价值 $$v_\pi(s)$$</li><li>For control:<br> Input: MDP $$&lt;S,A,P,R,\gamma&gt;$$<br> Output: 最优价值函数$$v_*$$和最优策略$$\pi_*$$</li></ol></li><li><p>Bellman方程:</p></li></ol><p>  <strong>(State-) Value Function:</strong></p>  <img src="https://i.loli.net/2021/09/29/7yIKAVjkGHTbXeU.png" alt="image-20210929174143894" style="zoom:80%;" />$$  \begin{align}  v_\pi(s) &= \sum_{a}\pi(a|s)q_\pi(s,a)\\  &= \sum_a\,\pi(a|s)(R_s^a+\gamma \sum_{s'}(P_{ss'}^a\,v_\pi(s')))(代入q_\pi(s',a)公式)  \end{align}$$<p>  求Optimal $$v_*(s)$$(把求期望操作$\rightarrow$找$\operatorname {arg,max}$操作)：</p>  <img src="https://i.loli.net/2021/09/29/x9D7TQga6GShrsi.png" alt="image-20210929203919230" style="zoom:80%;" />$$  \begin{align}  v_*(s) &= \underset{a}\max\{R_s^a+\gamma \sum_{s'}P_{ss'}^a\,v_\pi(s')\}  <p>  \end{align}<br>$$</p><p>  <strong>Action Function:</strong></p>  <img src="https://i.loli.net/2021/09/29/VMlc1N2g4DPJo5I.png" alt="image-20210929174301352" style="zoom: 80%;" />$$  \begin{align}  q_*(s, a) &= R_{s}^a + \gamma\sum_{s'}\, P_{ss'}^a v_\pi(s') \\  (or&= \sum_{s'}\, P_{ss'}^a (R_{ss'}^a + v_\pi(s')) ) (考虑奖励R_s^a是否与s'有关)\\  &=R_s^a + \gamma\sum_{s'}\,P_{ss'}^a\sum_{a'}\pi(a'|s')q_\pi(s', a') (代入v_\pi(s')公式)\\  \end{align}$$  求Optimal $$q_*(s, a)$$时(把求期望操作$\rightarrow$找$\operatorname {arg\,max}$操作)：  <img src="https://i.loli.net/2021/09/29/xIVUrmEWBt4Nlzf.png" alt="image-20210929204248905" style="zoom:80%;" />$$  q_*(s, a) = R_s^a + \gamma \sum_{s'}\,P_{ss'}^a{\underset{a'}\max\,}q_\pi(s', a'))$$<h2 id="Policy-Evaluation"><a href="#Policy-Evaluation" class="headerlink" title="Policy Evaluation"></a>Policy Evaluation</h2><p><img src="https://i.loli.net/2021/09/29/ghzERuXiC1MtsQp.png" alt="image-20210929211939079"></p><p>给定一个policy $$ \pi $$，评估&#x2F;衡量在$$ \pi$$下每个状态的value function.</p><h2 id="Policy-Iteration"><a href="#Policy-Iteration" class="headerlink" title="Policy Iteration"></a>Policy Iteration</h2><p><img src="https://i.loli.net/2021/09/29/2THt83175YAOZRJ.png" alt="image-20210929212427612"></p><p>这个是通过贪心的策略来迭代得到最好的policy。基本思想是$$ \pi(s) &#x3D; \underset{a}{\operatorname{arg,max}}, q(s, a)$$，即在state function 固定情况下，通过贪心的选择给自己带来最大受益的action a 来作为自己的策略。可以证明经过多次迭代后，策略能收敛到贪心策略下的最优解上。</p><h2 id="Value-Iteration"><a href="#Value-Iteration" class="headerlink" title="Value Iteration"></a>Value Iteration</h2><p><img src="https://i.loli.net/2021/09/29/NpwbPFi3k52uedH.png" alt="image-20210929213203328"></p><p>这个的特点是没有给定任何显示的(explicit)策略$$ \pi $$, 而是只求解state function。</p><p>与Policy Evaluation 和Policy Iteration 不同， 前两者都出现了$$\pi(a|s)$$这个东西，也就是说每个状态下选择某个动作的概率是被记录的，并且可能随着算法也会迭代更新；而Value Iteration 每次选择受益最大的动作来更新value。</p><p>等迭代结束，value全部求出来后，最后一遍求解$$\pi$$。</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p><img src="https://i.loli.net/2021/09/29/pNR9Q8biXWH2GOq.png" alt="image-20210929213757669"></p>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
      <category>强化学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>课程笔记</tag>
      
      <tag>强化学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>课程笔记：算法分析与设计(0)</title>
    <link href="/2021/09/25/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-%E7%AE%97%E6%B3%95%E5%88%86%E6%9E%90%E4%B8%8E%E8%AE%BE%E8%AE%A1/"/>
    <url>/2021/09/25/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-%E7%AE%97%E6%B3%95%E5%88%86%E6%9E%90%E4%B8%8E%E8%AE%BE%E8%AE%A1/</url>
    
    <content type="html"><![CDATA[<h1 id="课程笔记：算法设计与分析-0"><a href="#课程笔记：算法设计与分析-0" class="headerlink" title="课程笔记：算法设计与分析(0)"></a>课程笔记：算法设计与分析(0)</h1><h2 id="Notation"><a href="#Notation" class="headerlink" title="Notation"></a>Notation</h2><h3 id="O-notation"><a href="#O-notation" class="headerlink" title="O-notation"></a>O-notation</h3><ol><li>$f(n) &#x3D; \Omega(g(n))$: $\exist  c,n_0, when\quad n \ge n_0, f(n) \ge cg(n)$  ($\omega(·)$严格$&gt;$)</li><li>$f(n) &#x3D; O(g(n))$: $\exist  c,n_0, when\quad n \ge n_0, f(n) \le cg(n)$ ($o(·)$严格$&lt;$)</li><li>$f(n) &#x3D; \Theta(g(n))$: $\exist  c1, c2,n_0, when\quad n \ge n_0, c_1g(n) \le f(n) \le c_2g(n)$</li></ol><h2 id="高中数学复习回顾"><a href="#高中数学复习回顾" class="headerlink" title="高中数学复习回顾"></a>高中数学复习回顾</h2><h3 id="指数函数-Exponentials"><a href="#指数函数-Exponentials" class="headerlink" title="指数函数(Exponentials)"></a>指数函数(Exponentials)</h3><ol><li><p>泰勒展开(忘了的话自己再推一遍)<br> $$<br> f(x) &#x3D; f(x_0) + f’(x_0)(x-x_0) + \frac{f’’(x_0)}{2}(x-x_0)^2 + \frac{f^{(3)}(x_0)}{3!}(x-x_0)^3 + \dots+ \frac{f^{(n)}(x_0)}{n!}(x-x_0)^{(n)}<br> $$<br> 常用形式:<br> $$<br> e^x &#x3D; 1 + x +\frac{x^2}{2} + \frac{x^3}{3!} + \dots + \frac{x^{(n)}}{n!}+ \dots<br> $$<br> $\ln(1+x)$ 在0点展开， $x \in (-1, 1)$<br> $$<br> \ln(1+x) &#x3D; x-\frac{x^2}{2} + \frac{x^3}{3} - \frac{x^4}{4} + \frac{x^5}{5} - \dots + (-1)^{n-1}\frac{x^n}{n}<br> $$</p><p> <strong>记忆：</strong>$ \ln(1+x)$和$e^x$对比记忆，都是标准形式，不过对数是±交替并且少了1的常数项</p><p> $\sin(x)$在0点展开， $x \in R$:<br> $$<br> \sin(x) &#x3D; x- \frac{x^3}{3!} + \frac{x^5}{5!} - \dots + (-1)^{n-1}\frac{x^{(2n-1)!}}{(2n-1)!}<br> $$<br> <strong>记忆</strong>： $\sin$是“奇”函数, 所以是第“1,3,5…”项。<br> $$<br> cos(x)&#x3D;1 - \frac{x^2}{2} + \frac{x^4}{4!} - \dots + (-1)^{(n-1)}\frac{x^{2(n-1)}}{(2n-2)!}<br> $$</p></li><li><p>关于指数函数的重要不等式(从Taylor直接得到)<br> $$<br> 1 + x \le e^x \le 1 + x + x^2(x \rightarrow 0)<br> $$</p><p> $$<br> e^x &#x3D; 1 + x + \Theta(x^2)(x \rightarrow 0)<br> $$</p></li><li><p>关于对数函数的重要不等式</p></li></ol><p>$$<br>\frac{x}{1+x} \le \ln(x+1) \le x, x &gt; -1<br>$$</p><p>当且仅当$x &#x3D;0$时成立</p><h3 id="对数函数-Logarithm"><a href="#对数函数-Logarithm" class="headerlink" title="对数函数(Logarithm)"></a>对数函数(Logarithm)</h3><ol><li><p>换底公式 $\log_ba &#x3D; \frac{\log_ca}{\log_cb}$</p></li><li><p>$ a^ {\log_bc} &#x3D; a ^ {\log_ba\log_ac } &#x3D; c^{log_ba}$</p></li></ol><h2 id="阶乘-Factorials"><a href="#阶乘-Factorials" class="headerlink" title="阶乘(Factorials)"></a>阶乘(Factorials)</h2><ol><li>Stirling’s Approximation</li></ol><p>$$<br>n! &#x3D; \sqrt{2\pi n}\left(\frac{n}{e} \right)^n\left(1 + \Theta(\frac{1}{n}) \right)<br>$$</p><p>不等式(tight upper bound):<br>$$<br>2^n &lt; n! \le n^n<br>$$</p><ol start="2"><li><p>$$<br> n! &#x3D; \sqrt{2 \pi n}\left(\frac{n}{e} \right)^n e^{\alpha_n}, \frac{1}{12n +1} &lt; \alpha_n &lt; \frac{1}{12n}<br> $$</p></li><li><p>$$<br> \lg(n!) &#x3D; \Theta(n \lg n)<br> $$</p></li></ol>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
      <category>算法分析与设计</category>
      
    </categories>
    
    
    <tags>
      
      <tag>课程笔记</tag>
      
      <tag>算法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Luna--Linear Unified Nested Attention[阅读笔记]</title>
    <link href="/2021/08/20/Luna%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    <url>/2021/08/20/Luna%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="Neural-IPS在投-2021-Luna–Linear-Unified-Nested-Attention-阅读笔记"><a href="#Neural-IPS在投-2021-Luna–Linear-Unified-Nested-Attention-阅读笔记" class="headerlink" title="(Neural IPS在投,2021) Luna–Linear Unified Nested Attention[阅读笔记]"></a>(Neural IPS在投,2021) Luna–Linear Unified Nested Attention[阅读笔记]</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>传统Transformer中的注意力机制计算量是平方级别的。本文提出了Luna，Linear Unified Nested Attention 的方法，通过增加一个额外的固定长度的序列作为输入和输出，把平方级别的注意力计算拆分成两个线性时间的计算步骤来做近似，并且该固定长度的序列可以存储足够的上下文相关信息(Contexual Infomation)。</p><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><ol><li>想提出一个简单有效减低计算复杂度的方法<ul><li>传统的注意力机制的计算和存储都是$O(n^2)$的($n$表示序列的长度)，但是对于长序列输入，序列间相关性往往是稀疏的(不是完全图)</li><li>已有的改进方法：<br>  1)利用稀疏性约束(Sparse Attention)，如local attention<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, andDustin Tran. Image transformer. In *International Conference on Machine Learning*, pages 4055–4064. PMLR, 2018.">[1]</span></a></sup>, strided<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. *arXiv preprint arXiv:1904.10509*, 2019.">[2]</span></a></sup><sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. *arXiv preprint arXiv:2004.05150*, 2020.">[3]</span></a></sup>, Hierarchical Global Attention<sup id="fnref:4" class="footnote-ref"><a href="#fn:4" rel="footnote"><span class="hint--top hint--rounded" aria-label="Guo, Q., Qiu, X., Liu, P., Shao, Y., Xue, X., & Zhang, Z. (2019). Star-transformer. *arXiv preprint arXiv:1902.09113*.">[4]</span></a></sup>,attention with learnable patterns<sup id="fnref:5" class="footnote-ref"><a href="#fn:5" rel="footnote"><span class="hint--top hint--rounded" aria-label="Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Effificient content-based sparse attention with routing transformers. *Transactions of the Association for Computational Linguistics*,9:53–68, 2021.">[5]</span></a></sup><br>  2)利用注意力矩阵的低秩性，如Linformer<sup id="fnref:6" class="footnote-ref"><a href="#fn:6" rel="footnote"><span class="hint--top hint--rounded" aria-label="Wang, S., Li, B., Khabsa, M., Fang, H., & Ma, H. L. (2020). Self-attention with linear complexity. *arXiv preprint arXiv:2006.04768*.">[6]</span></a></sup><br>  3)kernel 方法，如Linear Transformer<sup id="fnref:7" class="footnote-ref"><a href="#fn:7" rel="footnote"><span class="hint--top hint--rounded" aria-label="Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In *International Conference on Machine Learning*, pages 5156–5165. PMLR, 2020.">[7]</span></a></sup>, Performer<sup id="fnref:8" class="footnote-ref"><a href="#fn:8" rel="footnote"><span class="hint--top hint--rounded" aria-label="Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. *arXiv preprint arXiv:2009.14794*, 2020.">[8]</span></a></sup> , Random Feature Attention<sup id="fnref:9" class="footnote-ref"><a href="#fn:9" rel="footnote"><span class="hint--top hint--rounded" aria-label="Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah Smith, and Lingpeng Kong. Random feature attention. In *International Conference on Learning Representations*, 2021. URL https://openreview.net/forum?id=QtTKTdVrFBB.">[9]</span></a></sup></li><li>本文所提出的引入额外固定长度序列的方法简洁优美，且能有效将计算复杂度降低到线性级别。</li></ul></li></ol><h2 id="Theorem-amp-Model"><a href="#Theorem-amp-Model" class="headerlink" title="Theorem&amp;Model"></a>Theorem&amp;Model</h2><p><img src="https://i.loli.net/2021/08/20/LA4bgRN5s2cFiqG.png"><br><img src="https://i.loli.net/2021/08/20/FfWyYXRlCiumvqZ.png"><br><img src="https://i.loli.net/2021/08/20/jJdZAOwCgLXbWH8.png"><br><img src="https://i.loli.net/2021/08/20/h8w5AaLgQjYMNDo.png"><br><img src="https://i.loli.net/2021/08/20/pgj62btJTOqGYlZ.png"></p><h2 id="Contribution-Innovation"><a href="#Contribution-Innovation" class="headerlink" title="Contribution(Innovation)"></a>Contribution(Innovation)</h2><p>Under review，暂不分析innovation如何。</p><h2 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h2><p>暂略，待后续自己实验后分析。</p><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><p>不过，本篇的idea和几乎同一时间(2021.06)放到Archiv上的External Attention using Two Linear Layers for Visual Tasks<sup id="fnref:10" class="footnote-ref"><a href="#fn:10" rel="footnote"><span class="hint--top hint--rounded" aria-label="M. H., Liu, Z. N., Mu, T. J., & Hu, S. M. (2021). Beyond self-attention: External attention using two linear layers for visual tasks. *arXiv preprint arXiv:2105.02358*.URL https://arxiv.org/abs/2105.02358">[10]</span></a></sup> 有同工异曲之处，详细可参考知乎<a href="https://zhuanlan.zhihu.com/p/382961255">介绍</a>。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, andDustin Tran. Image transformer. In <em>International Conference on Machine Learning</em>, pages 4055–4064. PMLR, 2018.<a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. <em>arXiv preprint arXiv:1904.10509</em>, 2019.<a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:3" class="footnote-text"><span>Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. <em>arXiv preprint arXiv:2004.05150</em>, 2020.<a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:4" class="footnote-text"><span>Guo, Q., Qiu, X., Liu, P., Shao, Y., Xue, X., &amp; Zhang, Z. (2019). Star-transformer. <em>arXiv preprint arXiv:1902.09113</em>.<a href="#fnref:4" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:5" class="footnote-text"><span>Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Effificient content-based sparse attention with routing transformers. <em>Transactions of the Association for Computational Linguistics</em>,9:53–68, 2021.<a href="#fnref:5" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:6" class="footnote-text"><span>Wang, S., Li, B., Khabsa, M., Fang, H., &amp; Ma, H. L. (2020). Self-attention with linear complexity. <em>arXiv preprint arXiv:2006.04768</em>.<a href="#fnref:6" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:7" class="footnote-text"><span>Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In <em>International Conference on Machine Learning</em>, pages 5156–5165. PMLR, 2020.<a href="#fnref:7" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:8" class="footnote-text"><span>Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. <em>arXiv preprint arXiv:2009.14794</em>, 2020.<a href="#fnref:8" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:9" class="footnote-text"><span>Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah Smith, and Lingpeng Kong. Random feature attention. In <em>International Conference on Learning Representations</em>, 2021. URL <a href="https://openreview.net/forum?id=QtTKTdVrFBB">https://openreview.net/forum?id=QtTKTdVrFBB</a>.<a href="#fnref:9" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:10" class="footnote-text"><span>M. H., Liu, Z. N., Mu, T. J., &amp; Hu, S. M. (2021). Beyond self-attention: External attention using two linear layers for visual tasks. <em>arXiv preprint arXiv:2105.02358</em>.URL <a href="https://arxiv.org/abs/2105.02358">https://arxiv.org/abs/2105.02358</a><a href="#fnref:10" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
      <category>Transformer</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Attention</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Partial FC--Training 10 Million Identities on a Single Machine[阅读笔记]</title>
    <link href="/2021/08/14/Notes_on_partial_FC/"/>
    <url>/2021/08/14/Notes_on_partial_FC/</url>
    
    <content type="html"><![CDATA[<h1 id="Partial-FC–Training-10-Million-Identities-on-a-Single-Machine-阅读笔记"><a href="#Partial-FC–Training-10-Million-Identities-on-a-Single-Machine-阅读笔记" class="headerlink" title="Partial FC–Training 10 Million Identities on a Single Machine[阅读笔记]"></a>Partial FC–Training 10 Million Identities on a Single Machine[阅读笔记]</h1><p>[TOC]</p><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><ol><li><p>传统的DataParallel模式，无法解决大数据集(包含几十万至千万ID, 千万至亿个训练样本)训练时，分类器的矩阵参数$W_{NK \times d}$爆显存的问题($N$是Batch数量，$K$是GPU个数，d是Feature Map的维度)</p></li><li><p>ModelParallel模式能够有效解决1中问题：通过将矩阵$W$按$N$的维度拆分成多个子矩阵放在不同GPU上即可。但是该模式无法解决$logits_{N \times C}$当$C$很大时爆显存的问题</p></li><li><p>本文提出的解决方式：保留一个Batch中所有的Positive Class， 随机采样Negative Class，并证明采样率较低情况下，仍能保持performance几乎无损。</p></li><li><p>Related Work(只是听作者介绍，没看):</p></li></ol><ul><li><p>HF-softmax(Goodman, 2001)<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="Goodman, J. 2001. Classes for fast maximum entropy training. In *IEEE International Conference on Acoustics, Speech,* *and Signal Processing. Proceedings (Cat. No.01CH37221)*, volume 1, 561–564.">[1]</span></a></sup> 通过对FeatureMap构建随机hash森林，每次检索最近的class center来获得active class subset<br> 缺点： ①class center 存储在RAM中 ②计算feature retrieval也要耗时</p></li><li><p>Softmax Dissection(He et al.2020)<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="He, L.; Wang, Z.; Li, Y.; and Wang, S. 2020. Softmax Dissection: Towards Understanding Intra- and Inter-class Objective for Embedding Learning. In *AAAI Conference on* *Artifificial Intelligence*, 10957–10964.">[2]</span></a></sup> 将softmax分成了intra-class objective和inter-class objective两部分，并且通过减低intra-class objective的计算冗余</p><p> 缺点：①难以拓展到其他softmax based方法</p></li></ul><h2 id="Theorem-amp-Model"><a href="#Theorem-amp-Model" class="headerlink" title="Theorem&amp;Model"></a>Theorem&amp;Model</h2><ol><li><p>softmax公式<br> $$<br> \sigma (X, i) &#x3D; \frac{e^{w_i^T X}}{\sum_{j&#x3D;1}^C e^{w_j^T X}}<br> $$<br> 分子可以在GPU-i上算，只要batch大小不会导致爆显存。分母的话每个GPU只需要提供一个scalar，代表一个求和。</p></li><li><p>Model Parallel 在第i块GPU上的算法：</p></li></ol><img src="https://i.loli.net/2021/08/15/dgfnUjzxpqQGVke.png" alt="image-20210717092711902" style="zoom:50%;" /><p>注解:</p><ul><li>Line 2, <code>allgather</code>是因为同时要使用DataParallel来训练模型</li><li>Line7, <code>allreduce</code>是因为对参数$W$实行ModelParallel,要从不同GPU上reduce $\sum e^{logtits_i}$</li></ul><ol start="3"><li><p>随机选取Negative Class的方法：</p> <img src="https://i.loli.net/2021/08/15/MYaywVFGzf9IeEW.png" alt="image-20210717093753863" style="zoom:50%;" /> <img src="https://i.loli.net/2021/08/15/1tEZPeDjrLW8Gwi.png" alt="image-20210717093818660" style="zoom:50%;" /></li><li></li></ol><h2 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h2><p>暂略(以后想到了再写)</p><h2 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h2><ol><li><p>Training Dataset, Validation Dataset, Testing Dataset分别是啥？Backbone? 损失函数？优化器？参数设置？<br> Training Dataset CASIA, MS1MV2, Celab-500k<br> Testing Dataset LFW(CPLFW, CFLFW), CFP-FP, AgeDB30,<br> Backbone: Resnet-50, 100<br> Mini-batch size &#x3D; 512, 8 x 2080Ti<br> LR: 0.1起步，后面有衰减(不同数据集不一样)</p><p> 训练次数：CASIA： 32K ； MS！MV2 180K； Glint360K 600K.</p></li><li><p>自身Ablation</p><p> 比较不同的sample rate, 以及all-sample 和只对negative class sample<br> 评价指标：Average Cos Diatnce，因为使用了CosFace和Arc Face的损失函数，度量<code>$x_i$</code> ,<code>$W_&#123;y_i&#125;$</code>间的余弦距离</p> <img src="https://i.loli.net/2021/08/15/43dKpjCLtfSJ16v.png" alt="image-20210717094807189" style="zoom:50%;" /> <img src="https://i.loli.net/2021/08/15/r8xdvhPjUKLzR7O.png" alt="image-20210717094939172" style="zoom:50%;" /> 评估在不同的分类数量情况下的内存使用情况 ![image-20210717095831255](https://i.loli.net/2021/08/15/ywXRMfK24xJtrHc.png)</li><li><p>和其他同类方法比较</p></li></ol><img src="https://i.loli.net/2021/08/15/1GxVTFIYbv9m5s3.png" alt="image-20210717095513744" style="zoom:50%;" /><img src="https://i.loli.net/2021/08/15/nfsvX9QTZkBRDqa.png" alt="image-20210717095601380" style="zoom:50%;" /><ol start="4"><li><p>和其他不同类方法比较(看总榜)</p> <img src="https://i.loli.net/2021/08/15/vXkL96DZQta4wbH.png" alt="image-20210717095254291" style="zoom:50%;" /></li><li><p>该文章的长处和不足<br> 长处：<br> ① 在<strong>ID量</strong>特别大的时候，到达十几万甚至百万时，开一个较大的batch-size也不用担心爆GPU显存<br> ② 扔掉大部分negative class确实很节约训练时间，而且loss损失不大</p><p> 不足:<br> ① 在ID较少的数据集上用处不大(比赛9万ID用不到，sample rate&#x3D;1)<br> ②只是随机的扔掉负样本比较粗糙，能不能有一些Mining Hard Negative？(但是要保证时间成本，保证存储Hard Negative Center 不需要耗费太多内存等)</p></li><li><p>自己感觉可以改进的地方<br> ①其实结合分布式原语操作(primitive)中的ReduceAll，想要得到$logits$只需要统计部分信息，比如Max, Sum等，所以不扔掉负样本也能做。但是可能有些特殊情况还是要用到完整的$logits$</p></li></ol><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><p>搜一搜知乎评论，CSDN博客，查一查OpenReview；这些讨论，质疑对我的思维启发非常大的</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>Goodman, J. 2001. Classes for fast maximum entropy training. In <em>IEEE International Conference on Acoustics, Speech,</em> <em>and Signal Processing. Proceedings (Cat. No.01CH37221)</em>, volume 1, 561–564.<a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>He, L.; Wang, Z.; Li, Y.; and Wang, S. 2020. Softmax Dissection: Towards Understanding Intra- and Inter-class Objective for Embedding Learning. In <em>AAAI Conference on</em> <em>Artifificial Intelligence</em>, 10957–10964.<a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
      <category>分类器</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Model Parallel</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
