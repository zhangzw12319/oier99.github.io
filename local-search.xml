<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>numpy部分函数讲解</title>
    <link href="/posts/5d51c93/"/>
    <url>/posts/5d51c93/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="520" height="86" src="//music.163.com/outchain/player?type=2&amp;id=1845153293&amp;auto=0&amp;height=66"></iframe><h1 id="numpy部分函数讲解">Numpy部分函数讲解</h1><p>学习笔记，记录一些<code>numpy</code>中一些不太熟悉但是非常好用的API用法。</p><h2 id="numpy.lexsort"><code>numpy.lexsort()</code></h2><p>官方doc讲解:https://numpy.org/doc/stable/reference/generated/numpy.lexsort.html</p><p>v1.2.4版本为例</p><blockquote><p>numpy.<strong>lexsort</strong>(<em>keys</em>, <em>axis=-1</em>)</p><p>Perform an indirect stable sort using a sequence of keys.</p><p>Given multiple sorting keys, which can be interpreted as columns in aspreadsheet, lexsort returns an array of integer indices that describesthe sort order by multiple columns. <strong>The last key in the sequenceis used for the primary sort order, the second-to-last key for thesecondary sort order, and so on.</strong> The keys argument must be asequence of objects that can be converted to arrays of the same shape.If a 2D array is provided for the keys argument, its rows areinterpreted as the sorting keys and sorting is according to the lastrow, second last row etc.</p><ul><li>Parameters:</li></ul><p><strong>keys</strong>(k, N) array or tuple containing k (N,)-shapedsequences. The <em>k</em> different “columns” to be sorted. The lastcolumn (or row if <em>keys</em> is a 2D array) is the primary sortkey.</p><p><strong>axis</strong>(int, optional) Axis to be indirectly sorted. Bydefault, sort over the last axis.</p><ul><li>Returns:</li></ul><p><strong>indices</strong>(N,) ndarray of ints.Array of indices thatsort the keys along the specified axis.</p></blockquote><p>打个比方，类似于考试排名时，首先以总成绩排名，当总成绩相同时一次按照语文，数学，英语的分数顺序排名，实现了多个关键字的排序功能。例如</p><table><thead><tr class="header"><th style="text-align: center;"></th><th style="text-align: center;">语文</th><th style="text-align: center;">数学</th><th style="text-align: center;">英语</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">小明</td><td style="text-align: center;">92</td><td style="text-align: center;">96</td><td style="text-align: center;">100</td></tr><tr class="even"><td style="text-align: center;">小红</td><td style="text-align: center;">82</td><td style="text-align: center;">90</td><td style="text-align: center;">85</td></tr><tr class="odd"><td style="text-align: center;">小丽</td><td style="text-align: center;">82</td><td style="text-align: center;">60</td><td style="text-align: center;">85</td></tr><tr class="even"><td style="text-align: center;">小亮</td><td style="text-align: center;">82</td><td style="text-align: center;">90</td><td style="text-align: center;">70</td></tr></tbody></table><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy<br><br>a = np.array([<span class="hljs-number">92</span>, <span class="hljs-number">82</span>, <span class="hljs-number">82</span>, <span class="hljs-number">82</span>]) <span class="hljs-comment"># first column</span><br>b = np.array([<span class="hljs-number">96</span>, <span class="hljs-number">90</span>, <span class="hljs-number">60</span>, <span class="hljs-number">90</span>]) <span class="hljs-comment"># second column</span><br>c = np.array([<span class="hljs-number">100</span>, <span class="hljs-number">85</span>, <span class="hljs-number">85</span>, <span class="hljs-number">70</span>]) <span class="hljs-comment"># third column</span><br><br>np.lexsort((c, b, a)) <span class="hljs-comment"># default, last column is the primary key</span><br><br><span class="hljs-comment"># result is array([2, 3, 1, 0])</span><br><span class="hljs-comment"># 小丽&lt;小亮&lt;小红&lt;小明</span><br><br><span class="hljs-comment"># 等价写法</span><br><span class="hljs-comment"># 2D array 排序，相当于原始表格行列转置了</span><br>datasheet = np.array([c, b, a])<br><span class="hljs-comment"># array([[100,  85,  85,  70],</span><br><span class="hljs-comment">#       [ 96,  90,  60,  90],</span><br><span class="hljs-comment">#       [ 92,  82,  82,  82]])</span><br><br>np.lexsort(datasheet)<br><br></code></pre></td></tr></table></figure><p>有一些需要注意的细节:</p><ul><li>默认从小到大排序</li><li>在输入<code>key</code>这一项的时候，如果输入是多个一维数组(数组的长度必须相同)，那么<code>primary key</code>的优先顺序是最后一列，倒数第二列，……，第一列(所以顺序是<code>(c,b,a)</code>不是<code>(a,b,c)</code>)。如果是2Darray(比如作为一个datasheet输入)，那么顺序是最后一行，倒数第二行，……，第一行。</li></ul><p>此外，程序里一维数组都是行向量，组成2D array时数学写法应该是 <spanclass="math inline">\(\left[ \begin{array}{c} c \\b \\ a \end{array}\right]\)</span>，不过程序里<code>np.array([c, b, a])</code>只能横着写，按照以前的习惯容易误以为是四行三列了，实际是三行四列的。所以<code>numpy</code>官方很体贴的将2Darray直接改为按行比较。</p><h2 id="numpy.insert"><code>numpy.insert()</code></h2><p>官方doc讲解:https://numpy.org/doc/stable/reference/generated/numpy.insert.html</p><p>直接看example</p><blockquote><p>Examples 是否指定axis的区别</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>a = np.array([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">2</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">3</span>, <span class="hljs-number">3</span>]])<br><span class="hljs-meta">&gt;&gt;&gt; </span>a<br>array([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>],<br>       [<span class="hljs-number">2</span>, <span class="hljs-number">2</span>],<br>       [<span class="hljs-number">3</span>, <span class="hljs-number">3</span>]])<br><span class="hljs-meta">&gt;&gt;&gt; </span>np.insert(a, <span class="hljs-number">1</span>, <span class="hljs-number">5</span>)<br>array([<span class="hljs-number">1</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1</span>, ..., <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>])<br><span class="hljs-meta">&gt;&gt;&gt; </span>np.insert(a, <span class="hljs-number">1</span>, <span class="hljs-number">5</span>, axis=<span class="hljs-number">1</span>)<br>array([[<span class="hljs-number">1</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1</span>],<br>       [<span class="hljs-number">2</span>, <span class="hljs-number">5</span>, <span class="hljs-number">2</span>],<br>       [<span class="hljs-number">3</span>, <span class="hljs-number">5</span>, <span class="hljs-number">3</span>]])<br></code></pre></td></tr></table></figure><p>Difference between sequence and scalars:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>np.insert(a, [<span class="hljs-number">1</span>], [[<span class="hljs-number">1</span>],[<span class="hljs-number">2</span>],[<span class="hljs-number">3</span>]], axis=<span class="hljs-number">1</span>)<br>array([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],<br>       [<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>],<br>       [<span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>]])<br><span class="hljs-meta">&gt;&gt;&gt; </span>np.array_equal(np.insert(a, <span class="hljs-number">1</span>, [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>], axis=<span class="hljs-number">1</span>),<br><span class="hljs-meta">... </span>               np.insert(a, [<span class="hljs-number">1</span>], [[<span class="hljs-number">1</span>],[<span class="hljs-number">2</span>],[<span class="hljs-number">3</span>]], axis=<span class="hljs-number">1</span>))<br><span class="hljs-literal">True</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>b = a.flatten()<br><span class="hljs-meta">&gt;&gt;&gt; </span>b<br>array([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>])<br><span class="hljs-meta">&gt;&gt;&gt; </span>np.insert(b, [<span class="hljs-number">2</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">5</span>, <span class="hljs-number">6</span>])<br>array([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">5</span>, ..., <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>])<br><span class="hljs-meta">&gt;&gt;&gt; </span>np.insert(b, <span class="hljs-built_in">slice</span>(<span class="hljs-number">2</span>, <span class="hljs-number">4</span>), [<span class="hljs-number">5</span>, <span class="hljs-number">6</span>])<br>array([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">5</span>, ..., <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>])<br><span class="hljs-meta">&gt;&gt;&gt; </span>np.insert(b, [<span class="hljs-number">2</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">7.13</span>, <span class="hljs-literal">False</span>]) <span class="hljs-comment"># type casting</span><br>array([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">7</span>, ..., <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>])<br><span class="hljs-meta">&gt;&gt;&gt; </span>x = np.arange(<span class="hljs-number">8</span>).reshape(<span class="hljs-number">2</span>, <span class="hljs-number">4</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span>idx = (<span class="hljs-number">1</span>, <span class="hljs-number">3</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span>np.insert(x, idx, <span class="hljs-number">999</span>, axis=<span class="hljs-number">1</span>)<br>array([[  <span class="hljs-number">0</span>, <span class="hljs-number">999</span>,   <span class="hljs-number">1</span>,   <span class="hljs-number">2</span>, <span class="hljs-number">999</span>,   <span class="hljs-number">3</span>],<br>       [  <span class="hljs-number">4</span>, <span class="hljs-number">999</span>,   <span class="hljs-number">5</span>,   <span class="hljs-number">6</span>, <span class="hljs-number">999</span>,   <span class="hljs-number">7</span>]])<br></code></pre></td></tr></table></figure></blockquote><h2 id="numpy.unique"><code>numpy.unique()</code></h2><p>https://numpy.org/doc/stable/reference/generated/numpy.unique.html#numpy-unique</p><p>返回一个数组去重后的结果</p><blockquote><p>numpy.<strong>unique</strong>(<em>ar</em>,<em>return_index=False</em>, <em>return_inverse=False</em>,<em>return_counts=False</em>, <em>axis=None</em>, **<em>,</em>equal_nan=True*)[<ahref="https://github.com/numpy/numpy/blob/v1.24.0/numpy/lib/arraysetops.py#L138-L320">source]</a></p><p>Find the unique elements of an array.</p><p>Returns the sorted unique elements of an array. There are threeoptional outputs in addition to the unique elements:</p><ul><li>the indices of the input array that give the uniquevalues（注：第一次出现unique value的位置）</li><li><strong>the indices of the unique array that reconstruct the inputarray</strong></li><li>the number of times each unique value comes up in the inputarray</li></ul></blockquote><p>看example</p><p>1D 情况: 注意输入是多维array, 不指定axis的话会被拉平成一维。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>np.unique([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>], return_index=<span class="hljs-literal">True</span>, return_inverse=<span class="hljs-literal">True</span>, return_counts=<span class="hljs-literal">True</span>)<br>(array([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]), array([<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">4</span>]), array([<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>]), array([<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>]))<br><span class="hljs-meta">&gt;&gt;&gt; </span>a = np.array([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">2</span>, <span class="hljs-number">3</span>]])<br><span class="hljs-meta">&gt;&gt;&gt; </span>np.unique(a)<br>array([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>])<br></code></pre></td></tr></table></figure><p>2D情况：</p><p>下面这个情况常见，例如3D点云Voxel稀疏化操作中，a数组是<code>(N,3)</code>代表记录着<code>N</code>个点三维坐标空间信息。原始点云坐标经过Voxel划分后，得到每个点所在的Voxel的下标索引。稀疏化操作就是只记录有点云存在的Voxel的下标位置，并只对这些位置进行几何变换的操作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>a = np.array([[<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>]])<br><span class="hljs-meta">&gt;&gt;&gt; </span>np.unique(a, axis=<span class="hljs-number">0</span>, return_index=<span class="hljs-literal">True</span>, return_inverse=<span class="hljs-literal">True</span>, return_counts=<span class="hljs-literal">True</span>)<br>(array([[<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>]]), array([<span class="hljs-number">0</span>, <span class="hljs-number">2</span>]), array([<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>]), array([<span class="hljs-number">2</span>, <span class="hljs-number">1</span>]))<br></code></pre></td></tr></table></figure><p><code>numpy.unique()</code>后重建原来数组</p><p>Reconstruct the input array from the unique values and inverse:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>a = np.array([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">6</span>, <span class="hljs-number">4</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>])<br><span class="hljs-meta">&gt;&gt;&gt; </span>u, indices = np.unique(a, return_inverse=<span class="hljs-literal">True</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span>u<br>array([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">6</span>])<br><span class="hljs-meta">&gt;&gt;&gt; </span>indices<br>array([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">4</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>])<br><span class="hljs-meta">&gt;&gt;&gt; </span>u[indices]<br>array([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">6</span>, <span class="hljs-number">4</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>])<br></code></pre></td></tr></table></figure><p>同理2D:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>a = np.array([[<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>]])<br><span class="hljs-meta">&gt;&gt;&gt; </span>u, indices = np.unique(a, axis=<span class="hljs-number">0</span>, return_inverse=<span class="hljs-literal">True</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span>u<br>array([[<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],<br>       [<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>]])<br><span class="hljs-meta">&gt;&gt;&gt; </span>indices<br>array([<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>])<br><span class="hljs-meta">&gt;&gt;&gt; </span>u[indices]<br>array([[<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],<br>       [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],<br>       [<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>]])<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>编程学习</category>
      
      <category>numpy学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>numpy</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Music Collection</title>
    <link href="/posts/5d4e3cda/"/>
    <url>/posts/5d4e3cda/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script>        <div id="aplayer-BZsrFLWi" class="aplayer aplayer-tag-marker" style="margin-bottom: 20px;"></div>  <script>  var options = {"narrow":false,"autoplay":false,"showlrc":3,"mode":"random","mutex":true,"theme":"#e6d0b2","preload":"metadata","listmaxheight":"513px","music":[{"title":"player 桜ノ雨","author":"halyosy/初音ミク","url":"https://lc-gluttony.s3.amazonaws.com/7BL86tkKW6Da/2Bf8xe5dmDOzEQ0GIu2J0ABRples20kI/%E6%A1%9C%E3%83%8E%E9%9B%A8-halyosy-%E5%88%9D%E9%9F%B3%E3%83%9F%E3%82%AF.mp3","pic":"https://p1.music.126.net/YLL0XkhJwp3q0ZSyzFjOgg==/888405395266415.jpg?param=90y90"},{"title":"ひだまりLiving","author":"藤東知夏","url":"https://lc-gluttony.s3.amazonaws.com/7BL86tkKW6Da/4KCAwNYierrnGh6TToKTWCjmnXRtzbVF/%E3%81%B2%E3%81%A0%E3%81%BE%E3%82%8ALiving-%E8%97%A4%E6%9D%B1%E7%9F%A5%E5%A4%8F.mp3","pic":"https://p2.music.126.net/B_sCLXthAHgoVEMIRNU1hw==/109951163559682176.jpg?param=90y90"},{"title":"ごり押しの言い訳","author":"百石元","url":"https://lc-gluttony.s3.amazonaws.com/7BL86tkKW6Da/D8CMwMFDlrVK4uWV60d9WqbnW9Lcz4Es/%E3%81%94%E3%82%8A%E6%8A%BC%E3%81%97%E3%81%AE%E8%A8%80%E3%81%84%E8%A8%B3-%E7%99%BE%E7%9F%B3%E5%85%83.mp3","pic":"https://p2.music.126.net/B_sCLXthAHgoVEMIRNU1hw==/109951163559682176.jpg?param=90y90"},{"title":"God knows","author":"平野綾","url":"https://lc-gluttony.s3.amazonaws.com/7BL86tkKW6Da/jkPObTbA4F1MQYDv25d9z2Ba1T97V1Cn/Godknows-%E5%B9%B3%E9%87%8E%E7%B6%BE.mp3","pic":"https://p2.music.126.net/LTEcJoeajuhP5C2BtbGhtA==/109951165822358136.jpg?param=90y90"},{"title":"となりのトトロ","author":"久石譲","url":"https://lc-gluttony.s3.amazonaws.com/7BL86tkKW6Da/HpuI9xUvQg21sya05w9VeY7w636tpJNP/%E3%81%A8%E3%81%AA%E3%82%8A%E3%81%AE%E3%83%88%E3%83%88%E3%83%AD-%E4%B9%85%E7%9F%B3%E8%AD%B2.mp3","pic":"https://s2.loli.net/2023/01/03/V3LGNSls97peEHK.png"},{"title":"几许风雨","author":"罗文","url":"https://lc-gluttony.s3.amazonaws.com/7BL86tkKW6Da/5X2Y4agBl7If7qLiunueTg8JUdq4erlr/%E5%87%A0%E8%AE%B8%E9%A3%8E%E9%9B%A8-%E7%BD%97%E6%96%87.mp3","pic":"https://p2.music.126.net/vRo5uQ8gIs2OuNJQqtMD_Q==/109951163197518435.jpg?param=90y90"},{"title":"H.He.Li.Be.B.C.N.O.F.Ne.","author":"金元寿子","url":"https://lc-gluttony.s3.amazonaws.com/7BL86tkKW6Da/GavUNyn8XG0Hq44KAuLbc4a43VBAStCz/H.He.Li.Be.B.C.N.O.F.Ne.-%E9%87%91%E5%85%83%E5%AF%BF%E5%AD%90.mp3","pic":"https://p2.music.126.net/lbUnHPcNshuVgCRnVAjIhg==/18517974836952010.jpg?param=90y90"},{"title":"I Seek the Truth (From Frozen 2/Outtake)","author":"Kristen Anderson-Lopez / Patti Murin","url":"https://lc-gluttony.s3.amazonaws.com/7BL86tkKW6Da/ntxowMgKX7tSsqCOHY9DmtsUNWPDMqrr/I%20Seek%20the%20Truth%20%28From%20-Frozen%202-_Outtake%29.mp3","pic":"https://p1.music.126.net/IUy2ZL9d1eLwAywlFDg7VA==/109951165270313538.jpg?param=130y130"}]};  options.element = document.getElementById("aplayer-BZsrFLWi");  var ap = new APlayer(options);    window.aplayers || (window.aplayers = []);  window.aplayers.push(ap);  </script>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>LaTex 讲解系列：如何优雅地插入图表</title>
    <link href="/posts/b2b25956/"/>
    <url>/posts/b2b25956/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script><h1 id="latex-讲解系列-如何优雅地插入图表">LaTex 讲解系列：如何优雅地插入图表</h1><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="520" height="86" src="//music.163.com/outchain/player?type=2&amp;id=761349&amp;auto=0&amp;height=66"></iframe><p>包括插入子图，多图排列，画模型图等等</p><p>具体代码以会议/期刊所给的模板格式为准</p><h2 id="插入图片">插入图片</h2><p><strong>插入一张图片</strong></p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs latex"><span class="hljs-comment">%\begin&#123;center&#125; % 外部居中</span><br> <span class="hljs-keyword">\begin</span>&#123;figure&#125;[!htbp]<br> <span class="hljs-keyword">\centering</span> <span class="hljs-comment">% 内部居中</span><br> <span class="hljs-keyword">\includegraphics</span>[width=0.4<span class="hljs-keyword">\textwidth</span>]&#123;introduction<span class="hljs-built_in">_</span>3<span class="hljs-built_in">_</span>cases.pdf&#125; <span class="hljs-comment">% Reduce the figure size so that it is slightly narrower than the column.</span><br> <span class="hljs-keyword">\caption</span>&#123;XXXXXX&#125;<br> <span class="hljs-keyword">\label</span>&#123;fig1:three<span class="hljs-built_in">_</span>cases&#125;<br> <span class="hljs-keyword">\end</span>&#123;figure&#125;<br> <span class="hljs-comment">%\end&#123;center&#125;</span><br></code></pre></td></tr></table></figure><p><strong>插入多张图片</strong></p><ol type="1"><li><p>对于插入多张子图的组图，如果数量$$4的，建议在PPT或者<code>draw.io</code>中手动排版一下，调整间距大小截图角度等组合成一个整体，作为单张图片来插入。</p></li><li><p>如果数量很多，比如<span class="math inline">\(5 \times6\)</span>的阵列，可以使用<code>MulimgViewer</code>软件(https://github.com/nachifur/MulimgViewer)进行多图联排展现可视化，软件会自动合成一张大单图。</p></li><li><p>若使用LaTex的<code>subfigure</code>来组图，实现方法如下:</p><p>但是困难是 1)一旦图片变多调试会变得非常麻烦 2)非常容易overfull超过页面大小，除非把每张图片缩小的特别小，但这样就看不清了3) 图片内部之间的白色间隔难以消除。非必要不用这种方式。</p></li></ol><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><code class="hljs latex"><span class="hljs-keyword">\begin</span>&#123;center&#125;<br><span class="hljs-keyword">\begin</span>&#123;figure&#125;[htbp]<span class="hljs-comment">% 一个figure环境套多个subfigure环境(保证Figure序号递增)</span><br>    <span class="hljs-keyword">\centering</span><br>    <span class="hljs-keyword">\subfigure</span>&#123;<span class="hljs-comment">% 每个subfigure负责控制一行图片的排列，下辖多个minipage环境</span><br>        <span class="hljs-keyword">\begin</span>&#123;minipage&#125;[h]&#123;0.4<span class="hljs-keyword">\linewidth</span>&#125;<br>            <span class="hljs-keyword">\centering</span><br>            <span class="hljs-keyword">\includegraphics</span>[scale=0.45]&#123;Fig/beat.png&#125;<span class="hljs-comment">% 不同的子图控制不同的缩放比</span><br>            <span class="hljs-keyword">\caption</span>&#123;Beating&#125;<span class="hljs-keyword">\label</span>&#123;Beating&#125;<span class="hljs-comment">% 不同的子图设置不同的标题</span><br>        <span class="hljs-keyword">\end</span>&#123;minipage&#125;<br>        <br>        <span class="hljs-keyword">\begin</span>&#123;minipage&#125;[h]&#123;0.6<span class="hljs-keyword">\linewidth</span>&#125;<br>            <span class="hljs-keyword">\centering</span><br>            <span class="hljs-keyword">\includegraphics</span>[scale=0.45]&#123;Fig/lean.png&#125;<br>            <span class="hljs-keyword">\caption</span>&#123;leaning&#125;<br>        <span class="hljs-keyword">\end</span>&#123;minipage&#125;<br>    &#125;<br>    <span class="hljs-keyword">\centering</span><br>    <span class="hljs-keyword">\subfigure</span>&#123;<br>        <span class="hljs-keyword">\begin</span>&#123;minipage&#125;[h]&#123;0.4<span class="hljs-keyword">\linewidth</span>&#125;<br>            <span class="hljs-keyword">\centering</span><br>            <span class="hljs-keyword">\includegraphics</span>[scale=0.45]&#123;Fig/catch<span class="hljs-built_in">_</span>up.png&#125;<br>            <span class="hljs-keyword">\caption</span>&#123;Catch up&#125;<br>        <span class="hljs-keyword">\end</span>&#123;minipage&#125;<br>        <br>        <span class="hljs-keyword">\begin</span>&#123;minipage&#125;[h]&#123;0.6<span class="hljs-keyword">\linewidth</span>&#125;<br>            <span class="hljs-keyword">\centering</span><br>            <span class="hljs-keyword">\includegraphics</span>[scale=0.45]&#123;Fig/sneak.png&#125;<br>            <span class="hljs-keyword">\caption</span>&#123;Sneaking&#125;<br>        <span class="hljs-keyword">\end</span>&#123;minipage&#125;<br>    &#125;<br>    <br>    <span class="hljs-keyword">\subfigure</span>&#123;<br>        <span class="hljs-keyword">\begin</span>&#123;minipage&#125;[h]&#123;0.4<span class="hljs-keyword">\linewidth</span>&#125;<br>            <span class="hljs-keyword">\centering</span><br>            <span class="hljs-keyword">\includegraphics</span>[scale=0.3]&#123;Fig/stand.png&#125;<br>            <span class="hljs-keyword">\caption</span>&#123;Stand still&#125;<br>        <span class="hljs-keyword">\end</span>&#123;minipage&#125;<br>        <br>        <span class="hljs-keyword">\begin</span>&#123;minipage&#125;[h]&#123;0.6<span class="hljs-keyword">\linewidth</span>&#125;<br>            <span class="hljs-keyword">\centering</span><br>            <span class="hljs-keyword">\includegraphics</span>[scale=0.35]&#123;Fig/slippery.png&#125;<br>            <span class="hljs-keyword">\caption</span>&#123;Slippery&#125;<br>        <span class="hljs-keyword">\end</span>&#123;minipage&#125;<br>    &#125;<br>    <span class="hljs-keyword">\centering</span><br>    <span class="hljs-keyword">\subfigure</span>&#123;<br>        <span class="hljs-keyword">\begin</span>&#123;minipage&#125;[h]&#123;0.4<span class="hljs-keyword">\linewidth</span>&#125;<br>            <span class="hljs-keyword">\centering</span><br>            <span class="hljs-keyword">\includegraphics</span>[scale=0.3]&#123;Fig/turnaround.png&#125;<br>            <span class="hljs-keyword">\caption</span>&#123;Upside down&#125;<br>        <span class="hljs-keyword">\end</span>&#123;minipage&#125;<br>        <br>        <span class="hljs-keyword">\begin</span>&#123;minipage&#125;[h]&#123;0.6<span class="hljs-keyword">\linewidth</span>&#125;<br>            <span class="hljs-keyword">\centering</span><br>            <span class="hljs-keyword">\includegraphics</span>[scale=0.3]&#123;Fig/struggle.png&#125;<br>            <span class="hljs-keyword">\caption</span>&#123;Struggle to turn around&#125;<span class="hljs-keyword">\label</span>&#123;struggle&#125;<br>        <span class="hljs-keyword">\end</span>&#123;minipage&#125;<br>    &#125;<br><span class="hljs-keyword">\end</span>&#123;figure&#125;<br><span class="hljs-keyword">\end</span>&#123;center&#125;<br></code></pre></td></tr></table></figure><p>效果图:</p><p><img src="https://s2.loli.net/2022/12/29/TJ81KenuBiYUSwW.png" /></p><h2 id="插入表格">插入表格</h2><h3 id="科研三线表">科研三线表</h3><p>效果图：</p><p><img src="https://s2.loli.net/2022/12/29/CkVqZFRuoxO2NXp.png" /></p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs LaTex"><span class="hljs-keyword">\begin</span>&#123;table*&#125;[htbp] <br><span class="hljs-keyword">\centering</span><br><span class="hljs-keyword">\resizebox</span>&#123;1.0<span class="hljs-keyword">\textwidth</span>&#125;&#123;!&#125;&#123;<span class="hljs-comment">% 注意使用resizebox需要\usepackage&#123;graphicx&#125;，\textwidth是设定好的标准的一行的长度数值。如果要被双栏装下，需改成0.5\textwidth,以此类推。</span><br><span class="hljs-keyword">\begin</span>&#123;tabular&#125;&#123;l|cccc|ccc|ccc|c|c&#125; <span class="hljs-comment">% 需要12列,l,c,r分别代表居左中右对齐，|代表竖杠分割线</span><br><span class="hljs-keyword">\toprule</span> <span class="hljs-comment">%添加表格头部粗线</span><br>Method <span class="hljs-built_in">&amp;</span> PPP <span class="hljs-built_in">&amp;</span> PPPP <span class="hljs-built_in">&amp;</span> QQ <span class="hljs-built_in">&amp;</span> QQQ <span class="hljs-built_in">&amp;</span> QQQQ <span class="hljs-built_in">&amp;</span> RRR <span class="hljs-built_in">&amp;</span> RRRR <span class="hljs-built_in">&amp;</span> TTTT <span class="hljs-built_in">&amp;</span> TTTTT <span class="hljs-built_in">&amp;</span> ZZZZZZ <span class="hljs-built_in">&amp;</span> mIoU <span class="hljs-built_in">&amp;</span> FPS <span class="hljs-keyword">\\</span><br> <span class="hljs-comment">%有n个&amp;，就表示该行有n+1列</span><br><span class="hljs-keyword">\hline</span> <span class="hljs-comment">%绘制一条水平横线</span><br> Method-A <span class="hljs-built_in">&amp;</span> 100.0 <span class="hljs-built_in">&amp;</span> 100.0 <span class="hljs-built_in">&amp;</span> 100.0 <span class="hljs-built_in">&amp;</span> 100.0 <span class="hljs-built_in">&amp;</span> 100.0 <span class="hljs-built_in">&amp;</span> 100.0 <span class="hljs-built_in">&amp;</span> 100.0 <span class="hljs-built_in">&amp;</span> 100.0 <span class="hljs-built_in">&amp;</span> 100.0 <span class="hljs-built_in">&amp;</span> 100.0 <span class="hljs-built_in">&amp;</span> 100.0 <span class="hljs-built_in">&amp;</span> - <span class="hljs-keyword">\\</span><br><br> Method-B <span class="hljs-built_in">&amp;</span> 90.0 <span class="hljs-built_in">&amp;</span> 90.0 <span class="hljs-built_in">&amp;</span> 90.0 <span class="hljs-built_in">&amp;</span> 90.0 <span class="hljs-built_in">&amp;</span> 90.0 <span class="hljs-built_in">&amp;</span> 90.0 <span class="hljs-built_in">&amp;</span> 90.0 <span class="hljs-built_in">&amp;</span> 90.0 <span class="hljs-built_in">&amp;</span> 90.0 <span class="hljs-built_in">&amp;</span> 90.0 <span class="hljs-built_in">&amp;</span> 90.0 <span class="hljs-built_in">&amp;</span> - <span class="hljs-keyword">\\</span><br><br> Method-C <span class="hljs-built_in">&amp;</span> 85.0 <span class="hljs-built_in">&amp;</span> 85.0 <span class="hljs-built_in">&amp;</span> 85.0 <span class="hljs-built_in">&amp;</span> 85.0 <span class="hljs-built_in">&amp;</span> 85.0 <span class="hljs-built_in">&amp;</span> 85.0 <span class="hljs-built_in">&amp;</span> 85.0 <span class="hljs-built_in">&amp;</span> 85.0 <span class="hljs-built_in">&amp;</span> 85.0 <span class="hljs-built_in">&amp;</span> 85.0 <span class="hljs-built_in">&amp;</span> 85.0 <span class="hljs-built_in">&amp;</span> - <span class="hljs-keyword">\\</span><br><span class="hljs-keyword">\bottomrule</span> <span class="hljs-comment">%添加表格底部粗线</span><br><span class="hljs-keyword">\end</span>&#123;tabular&#125;<br>&#125;<br><span class="hljs-keyword">\caption</span>&#123;XXXXXX Performance.&#125;<br><span class="hljs-keyword">\label</span>&#123;abc&#125; <span class="hljs-comment">% 添加作为引用的锚点label</span><br><span class="hljs-keyword">\end</span>&#123;table*&#125;<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>LaTex学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LaTex</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LaTex讲解系列：如何插入代码块/伪代码</title>
    <link href="/posts/200e31c0/"/>
    <url>/posts/200e31c0/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script><h1 id="latex-讲解系列-如何插入代码块伪代码">LaTex 讲解系列：如何插入代码块/伪代码</h1><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="520" height="86" src="//music.163.com/outchain/player?type=2&amp;id=815722&amp;auto=0&amp;height=66"></iframe><h2 id="latex-插入代码块的方法">LaTex 插入代码块的方法</h2><p>参考资料：overleaf官方文档(https://www.overleaf.com/learn/latex/Code_listing)</p><p><strong>方法1 使用verbatim环境</strong></p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs latex"><span class="hljs-keyword">\begin</span>&#123;verbatim&#125;<span class="hljs-string"></span><br><span class="hljs-string">% 插入你的代码</span><br><span class="hljs-string"></span><span class="hljs-keyword">\end</span>&#123;verbatim&#125;<br></code></pre></td></tr></table></figure><p>但是这种方式插入的效果是最朴素的，没有代码高亮、关键字边框，字体大小和行间距调整等格式设置。就是朴素的单倍行距显示，比较丑陋。</p><p><strong>方法2 使用listings环境进行优化</strong></p><p>step-1 引入宏包 <code>\usepackage&#123;listings&#125;</code></p><p>step-2 设置格式，下方通过宏定义的方式定义了<code>mystyle</code></p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs latex"><span class="hljs-keyword">\lstdefinestyle</span>&#123;mystyle&#125;&#123;<br>    <span class="hljs-comment">% backgroundcolor=\color&#123;backcolour&#125;,% 背景色   </span><br>    keywordstyle= <span class="hljs-keyword">\color</span>&#123; blue!70&#125;,<span class="hljs-comment">% 关键字/程序语言中的保留字颜色</span><br>    commentstyle= <span class="hljs-keyword">\color</span>&#123;red!50!green!50!blue!50&#125;,<span class="hljs-comment">% 程序中注释的颜色</span><br>    <span class="hljs-comment">% commentstyle= \color[RGB]&#123;40, 400, 255&#125;</span><br>    numberstyle=<span class="hljs-keyword">\tiny</span><span class="hljs-keyword">\color</span>&#123;codegray&#125;,<span class="hljs-comment">% 左侧行号显示的颜色</span><br>    stringstyle=<span class="hljs-keyword">\color</span>&#123;codepurple&#125;,<br>    basicstyle=<span class="hljs-keyword">\ttfamily</span><span class="hljs-keyword">\footnotesize</span>,<br>    breakatwhitespace=false,         <br>    breaklines=true,<span class="hljs-comment">% 对过长的代码自动换行                </span><br>    captionpos=b,                    <br>    keepspaces=true,                 <br>    numbers=left,<span class="hljs-comment">% 在左侧显示行号                 </span><br>    numbersep=5pt,                  <br>    showspaces=false,                <br>    showstringspaces=false,<span class="hljs-comment">% 不显示字符串中的空格</span><br>    showtabs=false,                  <br>    tabsize=2,<br>    <span class="hljs-comment">% frame=none,% 不显示边框</span><br>    frame=shadowbox,<span class="hljs-comment">% 边框阴影</span><br>    <span class="hljs-comment">%   escapebegin=\begin&#123;CJK*&#125;,escapeend=\end&#123;CJK*&#125;,      % 代码中出现中文必须加上，否则报错</span><br>&#125;<br></code></pre></td></tr></table></figure><p><strong>step -3 导入代码</strong></p><p>可以从源文件导入，只需要一行命令如下:</p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs latex"><span class="hljs-keyword">\lstinputlisting</span>[language=Python, style=mystyle ]&#123;ReduceClass.py&#125;<br></code></pre></td></tr></table></figure><p>填写好语言项(<code>languange=Python</code>),选择需要的格式(<code>style=mystyle</code>,假如定义了多个style可以选择一个来用)，最后在花括号里选择引用的代码文件。</p><p>也可以选择创建一个环境，如下：</p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs latex"><span class="hljs-keyword">\begin</span>[languange=Python]&#123;lstlisting&#125;<br><span class="hljs-comment">% 代码块</span><br><span class="hljs-keyword">\end</span>&#123;lstlisting&#125;<br></code></pre></td></tr></table></figure><h2 id="latex中插入伪代码的方法">LaTex中插入伪代码的方法</h2><p>常用的伪代码宏包有<code>algorithm</code>, <code>algorithm2e</code>,``等等。那么如何选择使用呢？首先，如果有论文投稿计划的话，按照相关期刊/会议要求LaTex使用伪代码的规定使用哪些宏包；如果没有明确规定，那么怎么顺手怎么来。</p><p><strong><code>algorithm</code>宏包</strong>，支持简单的伪代码格式。</p><p>基本格式如下:</p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs latex"><span class="hljs-keyword">\begin</span>&#123;algorithm&#125;[H]<br><span class="hljs-keyword">\KwIn</span>&#123;XXX&#125;<span class="hljs-comment">% 算法输入</span><br>    <span class="hljs-keyword">\KwOut</span>&#123;XXX&#125;<span class="hljs-comment">% 算法输出</span><br>    <span class="hljs-keyword">\BlankLine</span><br>    <span class="hljs-keyword">\caption</span>&#123;XXX&#125;<span class="hljs-comment">% 算法标题</span><br>    <br>    <span class="hljs-comment">%写伪代码</span><br><span class="hljs-keyword">\end</span>&#123;algorithm&#125;<br></code></pre></td></tr></table></figure><p><strong><code>algorithm2e</code>宏包</strong></p><p>step-1包裹在<code>algorithm</code>环境中(同时也要有<code>algorithm宏包</code>)</p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs latex"><span class="hljs-keyword">\usepackage</span>[vlined,ruled,linesnumbered]&#123;algorithm2e&#125;<br></code></pre></td></tr></table></figure><p>step-2 For循环怎么写</p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs LaTex"><span class="hljs-keyword">\For</span>&#123;<span class="hljs-built_in">$</span>i <span class="hljs-keyword">\leftarrow</span> 1<span class="hljs-built_in">$</span> to <span class="hljs-built_in">$</span>100<span class="hljs-built_in">$</span>&#125;&#123;<br>    XXX<span class="hljs-comment">% 循环体内容</span><br>&#125;<br></code></pre></td></tr></table></figure><p>step-3 While循环怎么写</p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs latex"><span class="hljs-keyword">\While</span>&#123;a &gt; 0&#125;&#123;<br>XXX<span class="hljs-comment">% 循环体内容</span><br>&#125;<br></code></pre></td></tr></table></figure><p>step-4 If-else 判断语句</p><p>if...then...</p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs latex"><span class="hljs-keyword">\If</span>&#123;<span class="hljs-built_in">$</span>i&gt;1<span class="hljs-built_in">$</span>&#125;&#123;<span class="hljs-built_in">$</span><span class="hljs-keyword">\operatorname</span>&#123;QuickSort&#125;(A[1,<span class="hljs-keyword">\cdots</span>,i-1])<span class="hljs-built_in">$</span>&#125;<br></code></pre></td></tr></table></figure><p>if...then...else...</p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs latex"><span class="hljs-keyword">\eIf</span>&#123;<span class="hljs-built_in">$</span>j<span class="hljs-built_in">$</span> is odd&#125;&#123;<br>if执行以及<br>&#125;&#123;else执行语句&#125;<br></code></pre></td></tr></table></figure><p>step-5 定义子函数</p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs latex"><span class="hljs-keyword">\SetKwFunction</span>&#123;FMain&#125;&#123;你定义的函数名，如gcd&#125;<br><span class="hljs-keyword">\SetKwProg</span>&#123;Fn&#125;&#123;Function&#125;&#123;:&#125;&#123;&#125;<br><span class="hljs-keyword">\Fn</span>&#123;<span class="hljs-keyword">\FMain</span>&#123;函数的参变量,如int a, int b&#125;&#125;&#123;<br>XXX<span class="hljs-comment">% 函数内容</span><br>&#125;<br><span class="hljs-keyword">\textbf</span>&#123;End Function&#125;<br>...<br>c <span class="hljs-keyword">\leftarrow</span> gcd(a, b) <span class="hljs-comment">% 在某个地方调用</span><br></code></pre></td></tr></table></figure><p>e.g.1. 冒泡排序</p><p>效果图：</p><p><img src="https://s2.loli.net/2022/12/29/p3NC4QEujBvq9Fd.png" style="zoom:80%;" /></p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs latex"><span class="hljs-keyword">\begin</span>&#123;algorithm&#125;[H]<br>    <span class="hljs-keyword">\KwIn</span>&#123;An array <span class="hljs-built_in">$</span>A[1,<span class="hljs-keyword">\dots</span>,n]<span class="hljs-built_in">$</span>&#125;<br>    <span class="hljs-keyword">\KwOut</span>&#123;<span class="hljs-built_in">$</span>A<span class="hljs-built_in">$</span> sorted nondecreasingly&#125;<br>    <span class="hljs-keyword">\BlankLine</span><br>    <span class="hljs-keyword">\caption</span>&#123;BubbleSort&#125;<span class="hljs-keyword">\label</span>&#123;Alg-Bubble&#125;<br><br>    <span class="hljs-built_in">$</span>i<span class="hljs-keyword">\leftarrow</span> 1<span class="hljs-built_in">$</span>; <span class="hljs-built_in">$</span>sorted<span class="hljs-keyword">\leftarrow</span> false<span class="hljs-built_in">$</span><span class="hljs-keyword">\;</span><br><br>    <span class="hljs-keyword">\While</span>&#123;<span class="hljs-built_in">$</span>i<span class="hljs-keyword">\leq</span> n-1<span class="hljs-built_in">$</span> <span class="hljs-keyword">\textbf</span>&#123;and not&#125; <span class="hljs-built_in">$</span>sorted<span class="hljs-built_in">$</span>&#125;&#123;<br>        <span class="hljs-built_in">$</span>sorted<span class="hljs-keyword">\leftarrow</span> true<span class="hljs-built_in">$</span><span class="hljs-keyword">\;</span><br>        <span class="hljs-keyword">\For</span>&#123;<span class="hljs-built_in">$</span>j<span class="hljs-keyword">\leftarrow</span> n <span class="hljs-built_in">$</span> <span class="hljs-keyword">\textbf</span>&#123;downto&#125; <span class="hljs-built_in">$</span>i+1<span class="hljs-built_in">$</span>&#125;&#123;<br>            <span class="hljs-keyword">\If</span>&#123;<span class="hljs-built_in">$</span>A[j]&lt;A[j-1]<span class="hljs-built_in">$</span>&#125;&#123;<br>                interchange <span class="hljs-built_in">$</span>A[j]<span class="hljs-built_in">$</span> and <span class="hljs-built_in">$</span>A[j-1]<span class="hljs-built_in">$</span><span class="hljs-keyword">\;</span><br>                <span class="hljs-built_in">$</span>sorted<span class="hljs-keyword">\leftarrow</span> false<span class="hljs-built_in">$</span><span class="hljs-keyword">\;</span><br>            &#125;<br>        &#125;<br>        <span class="hljs-built_in">$</span>i<span class="hljs-keyword">\leftarrow</span> i+1<span class="hljs-built_in">$</span><span class="hljs-keyword">\;</span><br>    &#125;<br><span class="hljs-keyword">\end</span>&#123;algorithm&#125;<br></code></pre></td></tr></table></figure><p>e.g.2. 大数据分析中经典的AMS-F2估计算法(并使用Median Trick, AverageTrick)</p><p>效果图：</p><p><img src="https://s2.loli.net/2022/12/29/oOpJuM1kHL3mbcr.png" style="zoom: 80%;" /></p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><code class="hljs latex"><span class="hljs-keyword">\begin</span>&#123;algorithm&#125;[H]<br>        <span class="hljs-keyword">\KwIn</span>&#123;4-wise independent hash family <span class="hljs-built_in">$</span>h<span class="hljs-built_in">$</span>, stream <span class="hljs-built_in">$</span>a<span class="hljs-built_in">_</span>i <span class="hljs-keyword">\in</span> [n]<span class="hljs-built_in">$</span>, <span class="hljs-built_in">$</span><span class="hljs-keyword">\epsilon</span><span class="hljs-built_in">$</span>, <span class="hljs-built_in">$</span><span class="hljs-keyword">\eta</span><span class="hljs-built_in">$</span>, <span class="hljs-built_in">$</span><span class="hljs-keyword">\delta</span><span class="hljs-built_in">$</span>&#125;<br>        <span class="hljs-keyword">\KwOut</span>&#123;<span class="hljs-built_in">$</span>z<span class="hljs-built_in">^</span>2<span class="hljs-built_in">$</span>&#125;<br>        <span class="hljs-keyword">\BlankLine</span><br>        <span class="hljs-keyword">\caption</span>&#123;AMS Scheme for <span class="hljs-built_in">$</span>F<span class="hljs-built_in">_</span>2<span class="hljs-built_in">$</span>&#125;<br>        <br>        <span class="hljs-keyword">\SetKwFunction</span>&#123;FMain&#125;&#123;AMS-F2&#125;<br>        <span class="hljs-keyword">\SetKwProg</span>&#123;Fn&#125;&#123;Function&#125;&#123;:&#125;&#123;&#125;<br>        <span class="hljs-keyword">\Fn</span>&#123;<span class="hljs-keyword">\FMain</span>&#123;stream <span class="hljs-built_in">$</span>a<span class="hljs-built_in">_</span>i<span class="hljs-built_in">$</span>, hash family <span class="hljs-built_in">$</span>h(<span class="hljs-keyword">\cdot</span>)<span class="hljs-built_in">$</span>&#125;&#125;&#123;<br>        <span class="hljs-built_in">$</span> z <span class="hljs-keyword">\longleftarrow</span> 0 <span class="hljs-built_in">$</span><span class="hljs-keyword">\;</span>  <br>        <br>        <span class="hljs-keyword">\While</span>&#123;stream <span class="hljs-built_in">$</span>a<span class="hljs-built_in">_</span>i<span class="hljs-built_in">$</span> is not empty&#125;&#123;<br>            <span class="hljs-built_in">$</span>a<span class="hljs-built_in">_</span>i<span class="hljs-built_in">$</span> is current item<span class="hljs-keyword">\;</span><br>            <br>            <span class="hljs-built_in">$</span>z <span class="hljs-keyword">\longleftarrow</span> z + h(a<span class="hljs-built_in">_</span>i)<span class="hljs-built_in">$</span><span class="hljs-keyword">\;</span><br>        &#125;<br>        <br>        <span class="hljs-keyword">\textbf</span>&#123;return&#125; <span class="hljs-built_in">$</span> z<span class="hljs-built_in">^</span>2<span class="hljs-built_in">$</span> <br>&#125;<br><span class="hljs-keyword">\textbf</span>&#123;End Function&#125;<br><br>// apply average trick<br><br><span class="hljs-built_in">$</span>q <span class="hljs-keyword">\longleftarrow</span> <span class="hljs-keyword">\lceil</span> <span class="hljs-keyword">\frac</span>&#123;4&#125;&#123;<span class="hljs-keyword">\eta</span> <span class="hljs-keyword">\epsilon</span><span class="hljs-built_in">^</span>2&#125; <span class="hljs-keyword">\rceil</span><span class="hljs-built_in">$</span><span class="hljs-keyword">\;</span><br><br><span class="hljs-built_in">$</span>sum<span class="hljs-built_in">_</span>1 <span class="hljs-keyword">\longleftarrow</span> 0<span class="hljs-built_in">$</span><br><br><span class="hljs-keyword">\For</span>&#123;<span class="hljs-built_in">$</span>j <span class="hljs-keyword">\leftarrow</span> 1<span class="hljs-built_in">$</span> to <span class="hljs-built_in">$</span>q<span class="hljs-built_in">$</span>&#125;&#123;<br>    <span class="hljs-built_in">$</span>sum<span class="hljs-built_in">_</span>1 <span class="hljs-keyword">\longleftarrow</span> sum<span class="hljs-built_in">_</span>1 +<span class="hljs-built_in">$</span> AMS-F2(stream <span class="hljs-built_in">$</span>a<span class="hljs-built_in">_</span>i<span class="hljs-built_in">$</span>, <span class="hljs-built_in">$</span>h(<span class="hljs-keyword">\cdot</span>)<span class="hljs-built_in">$</span>)<br>&#125;<br><br><span class="hljs-built_in">$</span>avg <span class="hljs-keyword">\longleftarrow</span> sum<span class="hljs-built_in">_</span>1 / q<span class="hljs-built_in">$</span><br><br>//apply median trick<br><br><span class="hljs-built_in">$</span>s <span class="hljs-keyword">\longleftarrow</span> <span class="hljs-keyword">\lceil</span> <span class="hljs-keyword">\frac</span>&#123;3<span class="hljs-keyword">\eta</span><span class="hljs-built_in">^</span>2&#125;&#123;(<span class="hljs-keyword">\frac</span>&#123;1&#125;&#123;2&#125; - <span class="hljs-keyword">\eta</span>)<span class="hljs-built_in">^</span>2&#125;<span class="hljs-keyword">\ln</span><span class="hljs-keyword">\frac</span>&#123;2&#125;&#123;<span class="hljs-keyword">\delta</span>&#125; <span class="hljs-keyword">\rceil</span><span class="hljs-built_in">$</span><br><br>create a empty set <span class="hljs-built_in">$</span><span class="hljs-keyword">\Phi</span><span class="hljs-built_in">$</span><br><br><span class="hljs-keyword">\For</span>&#123;<span class="hljs-built_in">$</span>j <span class="hljs-keyword">\leftarrow</span> 1<span class="hljs-built_in">$</span> to <span class="hljs-built_in">$</span>s<span class="hljs-built_in">$</span>&#125;&#123;<br>    Add the result of AMS-F2(stream <span class="hljs-built_in">$</span>a<span class="hljs-built_in">_</span>i<span class="hljs-built_in">$</span>, <span class="hljs-built_in">$</span>h(<span class="hljs-keyword">\cdot</span>)<span class="hljs-built_in">$</span>) into set <span class="hljs-built_in">$</span><span class="hljs-keyword">\Phi</span><span class="hljs-built_in">$</span><br>&#125;<br><br><span class="hljs-built_in">$</span>med <span class="hljs-keyword">\longleftarrow</span><span class="hljs-built_in">$</span> the smallest median of <span class="hljs-built_in">$</span><span class="hljs-keyword">\Phi</span><span class="hljs-built_in">$</span><br><br><span class="hljs-keyword">\Return</span>&#123;<span class="hljs-built_in">$</span>avg<span class="hljs-built_in">$</span>, <span class="hljs-built_in">$</span>med<span class="hljs-built_in">$</span>&#125; <span class="hljs-keyword">\;</span><br><span class="hljs-keyword">\end</span>&#123;algorithm&#125;<br></code></pre></td></tr></table></figure><p><strong><code>algorithmic</code>宏包</strong></p><p>占个坑，以后填</p><p>(感觉这个宏包不太方便定义指令)</p>]]></content>
    
    
    <categories>
      
      <category>LaTex学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LaTex</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>map_reduce</title>
    <link href="/posts/77084934/"/>
    <url>/posts/77084934/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script><h1id="一种简单优雅的map-reduce手工实现方式python">一种简单优雅的Map-Reduce手工实现方式(Python)</h1><p>参考链接：https://blog.csdn.net/weixin_45203607/article/details/127369408Python-多进程(进程,锁,通讯,进程池)https://blog.csdn.net/HaidongDU/article/details/112795797 Python多线程中的join用法</p><h2 id="map-reudce-原理">Map-Reudce 原理</h2><p>Map-Reduce是谷歌提出的，用于处理大数据文本输入进行统计的计算框架。其总体思路简单巧妙，所有的输入文本统一通过<code>map</code>和<code>reduce</code>两段计算过程便可以得到自己想要的统计结果(如词频统计，排序分类等等)。<code>map</code>的计算过程是将输入的流式数据(如文本流，访问流等等)通过某种规则映射到<code>key-value</code>键值对的形式。例如</p><blockquote><p>词频分析：Map计算过程</p><p>输入：“It was the best of times, it was the worst of times. ”</p><p>输出： &lt;it, 2&gt;, &lt;was, 2&gt;, &lt;the, 2&gt;, &lt;of, 2&gt;,&lt;times, 2&gt;</p></blockquote><p>这个过程可以在成千上百个分布式节点上部署，每个节点分别处理海量的文件，然后得到如上键值对形式。经过中间层和网络通信将各个节点的键值对数据shuffle在一起(比如最简单的保存在本地临时存储文件，然后传输这谢谢文件)。</p><p><code>reduce</code>的过程便是将这些中间的键值对结果合并、分类，得到最终的统计结果。这个过程也可以在多个计算节点完成，例如有26个节点负责处理reduce过程，每个节点代表<code>a</code>~<code>z</code>的一个首字母。<code>key</code>所代表的单词首字母传输到符合的<code>reduce</code>节点里，然后将对应的<code>value</code>（例如这里是词频）合并。当然<code>reduce</code>节点的划分要考虑因素比较多，比如考虑负载均衡的问题。因为有些节点可能分到的任务比其他的节点多，(假如<code>b</code>开头的单词会比<code>u</code>开头的单词多，那么上述的划分方式就会导致负载不均衡。</p><blockquote><p>Map节点1输出: &lt;it, 2&gt;, &lt;was, 2&gt;, &lt;the, 2&gt;, &lt;of,2&gt;, &lt;times, 2&gt;</p><p>Map节点2输出：&lt;he, 1&gt;, &lt;was, 1&gt;, &lt;taller,1&gt;,&lt;than, 1&gt;, &lt;Mike, 1&gt;</p><p>Reduce节点1结果(负责<code>a</code>~<code>n</code>)：&lt;he, 1&gt;,&lt;it, 2&gt;, &lt;Mike, 1&gt;</p><p>Reduce 节点2结果(负责<code>o</code>~<code>z</code>)：&lt;of, 2&gt;,&lt;taller, 1&gt;, &lt;than, 1&gt;, &lt;the, 2&gt;, &lt;times, 2&gt;,&lt;was, 3&gt;</p></blockquote><h2 id="本demo的实现原理">本demo的实现原理</h2><p>本demo实现一个成绩平均分计算功能。输入文件格式如下:</p><blockquote><p>File1.txt:</p><p>小红 98.3</p><p>小明 85.7</p><p>...</p><p>小倩 86.4</p></blockquote><p>有多组如上图格式的输入文件代表每个学生在不同科目中取得的成绩。要求输出每个学生的平均分(<spanclass="math inline">\(\frac{\text{总成绩}}{\text{参加的科目数量}}\)</span>)。</p><p>实现结构式<code>multi-mapper one reducer</code>的结构，并利用多进程编程的方式模拟多个<code>mapper</code>处理节点。整个程序由<code>Mapper</code>类，<code>Reducer</code>类和<code>Manager</code>三部分构成。<code>Manager</code>脚本负责实例化多个<code>Mapper</code>和一个<code>Reducer</code>，并开启多个进程，每个进程负责一个计算节点。为方便演示，本demo中只设置了一个<code>Reducer</code>。若设置多个<code>Reducer</code>，需要额外实现对<code>key</code>值的划分功能。此外，中间网络传输部分采用通过字典格式保存到本地路径，再将路径的列表由<code>Mapper -&gt; Manager -&gt; Reducer</code>传递模拟网络传递过程。</p><h3 id="mapper类">Mapper类</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> random<br><span class="hljs-keyword">import</span> pickle<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Mapper</span>():<br>    <span class="hljs-string">&quot;&quot;&quot;Map-Reduce Mapper</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        filepath (str): the path to load single text file for mapper</span><br><span class="hljs-string">        mid_save_path (str): the path to save intermediate result(don&#x27;t include filename)</span><br><span class="hljs-string">        worker_id (optional, int): id</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, filepath:<span class="hljs-built_in">str</span>, mid_save_path:<span class="hljs-built_in">str</span>, worker_id:<span class="hljs-built_in">int</span>=-<span class="hljs-number">1</span></span>) -&gt; <span class="hljs-literal">None</span>:<br>        self.file_path = filepath<br>        self.mid_save_path = mid_save_path<br>        self.score_dict = &#123;&#125; <span class="hljs-comment"># &#123;&quot;Lucy&quot;: &#123;&quot;total_score&quot;: 180, &quot;count&quot;: 2&#125;&#125;</span><br>        <span class="hljs-keyword">if</span> worker_id &gt; <span class="hljs-number">0</span>:<br>            self.worker_id = worker_id<br>        <span class="hljs-keyword">else</span>:<br>            self.worker_id = random.randint(<span class="hljs-number">100000</span>,<span class="hljs-number">10000000</span>) + random.randint(<span class="hljs-number">12</span>,<span class="hljs-number">100</span>)<br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_add_item</span>(<span class="hljs-params">self, name:<span class="hljs-built_in">str</span>, score:<span class="hljs-built_in">int</span></span>):<br>        <span class="hljs-keyword">if</span> name <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> self.score_dict.keys():<br>            entry = &#123;<span class="hljs-string">&quot;total&quot;</span>: score, <span class="hljs-string">&quot;count&quot;</span>: <span class="hljs-number">1</span>&#125;<br>        <span class="hljs-keyword">else</span>:<br>            entry = self.score_dict[name]<br>            entry[<span class="hljs-string">&quot;total&quot;</span>] += score<br>            entry[<span class="hljs-string">&quot;count&quot;</span>] += <span class="hljs-number">1</span><br>        self.score_dict[name] = entry<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">run</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Main Function of Mapper. </span><br><span class="hljs-string">        Load text from source files, split to name and score</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            save_path(str):return path where the key-value result is saved</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(self.file_path) <span class="hljs-keyword">as</span> f:<br>            line = f.readline()<br>            <span class="hljs-keyword">while</span> line != <span class="hljs-string">&quot;&quot;</span>:<br>                line = line.split()<br>                name, score = line[<span class="hljs-number">0</span>], line[<span class="hljs-number">1</span>]<br>                self._add_item(name, score)<br>                line = f.readline()<br>        <br>        filename = <span class="hljs-built_in">str</span>(self.worker_id) + <span class="hljs-string">&quot;_intermediate_results.pkl&quot;</span><br>        save_path = os.path.join(self.mid_save_path, filename)<br>        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(save_path, <span class="hljs-string">&#x27;wb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>            pickle.dump(self.score_dict, f, pickle.HIGHEST_PROTOCOL)<br>        <span class="hljs-keyword">return</span> save_path<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    mapper = Mapper(filepath=<span class="hljs-string">&#x27;file&#123;&#125;.txt&#x27;</span>.<span class="hljs-built_in">format</span>(<span class="hljs-number">1</span>), mid_save_path=<span class="hljs-string">&quot;./&quot;</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Mapper <span class="hljs-subst">&#123;mapper.worker_id&#125;</span> started...&quot;</span>)<br>    filename=mapper.run()<br></code></pre></td></tr></table></figure><h3 id="reducer-类">Reducer 类</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> random<br><span class="hljs-keyword">import</span> pickle<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Reducer</span>():<br>    <span class="hljs-string">&quot;&quot;&quot;Map-Reduce Reducer</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        filepath_lists (List[str]): the path lists to load key-value data</span><br><span class="hljs-string">        save_path (str): the path to save final resuls(don&#x27;t include filename)</span><br><span class="hljs-string">        worker_id (optional, int): id</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, filepath_lists, save_path:<span class="hljs-built_in">str</span>, worker_id:<span class="hljs-built_in">int</span>=-<span class="hljs-number">1</span></span>) -&gt; <span class="hljs-literal">None</span>:<br>        self.filepath_lists = filepath_lists<br>        self.save_path = save_path<br>        self.result_dict = &#123;&#125;<br>        <span class="hljs-keyword">if</span> worker_id &gt; <span class="hljs-number">0</span>:<br>            self.worker_id = worker_id<br>        <span class="hljs-keyword">else</span>:<br>            self.worker_id = random.randint(<span class="hljs-number">100000</span>,<span class="hljs-number">10000000</span>) + random.randint(<span class="hljs-number">12</span>,<span class="hljs-number">100</span>)<br><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_merge</span>(<span class="hljs-params">self, kv</span>):<br>        <span class="hljs-keyword">for</span> k,v <span class="hljs-keyword">in</span> kv.items():<br>            <span class="hljs-keyword">if</span> k <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> self.result_dict.keys():<br>                new_entry = &#123;&#125;<br>                new_entry[<span class="hljs-string">&quot;total&quot;</span>] = <span class="hljs-built_in">float</span>(v[<span class="hljs-string">&quot;total&quot;</span>])<br>                new_entry[<span class="hljs-string">&quot;count&quot;</span>] = <span class="hljs-built_in">int</span>(v[<span class="hljs-string">&quot;count&quot;</span>])<br>                self.result_dict[k] = new_entry<br>            <span class="hljs-keyword">else</span>:<br>                entry1 = self.result_dict[k]<br>                entry1[<span class="hljs-string">&quot;total&quot;</span>] += <span class="hljs-built_in">float</span>(v[<span class="hljs-string">&quot;total&quot;</span>])<br>                entry1[<span class="hljs-string">&quot;count&quot;</span>] += <span class="hljs-built_in">int</span>(v[<span class="hljs-string">&quot;count&quot;</span>])<br><br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_average</span>(<span class="hljs-params">self</span>):<br>        dd = &#123;&#125;<br>        <span class="hljs-keyword">for</span> k,v <span class="hljs-keyword">in</span> self.result_dict.items():<br>            avg = v[<span class="hljs-string">&quot;total&quot;</span>] / v[<span class="hljs-string">&quot;count&quot;</span>]<br>            dd[k] = avg<br>        <br>        self.result_dict = dd<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">run</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Main Function of Reducer</span><br><span class="hljs-string">        Reduce key-value results to final averge score for each student</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        kv_lists = []<br>        <span class="hljs-keyword">for</span> each <span class="hljs-keyword">in</span> self.filepath_lists:<br>            <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(each, <span class="hljs-string">&#x27;rb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>                kv = pickle.load(f)<br>                kv_lists.append(kv)<br>        <br>        <span class="hljs-keyword">for</span> kv <span class="hljs-keyword">in</span> kv_lists:<br>            self._merge(kv)<br>        <br>        self._average()<br>        <br>        save_path = os.path.join(self.save_path, <span class="hljs-string">&quot;results.txt&quot;</span>)<br>        f = <span class="hljs-built_in">open</span>(save_path, <span class="hljs-string">&quot;w&quot;</span>)<br>        <span class="hljs-keyword">for</span> k,v <span class="hljs-keyword">in</span> self.result_dict.items():<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;k&#125;</span>: <span class="hljs-subst">&#123;v&#125;</span>分&quot;</span>, file=f)        <br>        <br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    reducer = Reducer(filepath_lists=[<span class="hljs-string">&#x27;2173983_intermediate_results.pkl&#x27;</span>], save_path=<span class="hljs-string">&quot;./&quot;</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Reducer <span class="hljs-subst">&#123;reducer.worker_id&#125;</span> started...&quot;</span>)<br>    reducer.run()      <br></code></pre></td></tr></table></figure><h3 id="manager-脚本">Manager 脚本</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> multiprocessing <span class="hljs-keyword">as</span> mp<br><span class="hljs-keyword">import</span> time<br><br><span class="hljs-keyword">from</span> multiprocessing <span class="hljs-keyword">import</span> Manager<br><span class="hljs-keyword">from</span> MapClass <span class="hljs-keyword">import</span> Mapper<br><span class="hljs-keyword">from</span> ReduceClass <span class="hljs-keyword">import</span> Reducer<br><br>num_mapper = <span class="hljs-number">3</span><br>num_reducer = <span class="hljs-number">1</span><br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">mapper_task</span>(<span class="hljs-params">filenum:<span class="hljs-built_in">int</span>, d:<span class="hljs-built_in">list</span>, barrier</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Mapper Starter FUnction</span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        filenum (int): e.g. 1 indicates &quot;file1.txt&quot;</span><br><span class="hljs-string">        d (list): parameters for saving intermediate file paths.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    mapper = Mapper(filepath=<span class="hljs-string">&#x27;file&#123;&#125;.txt&#x27;</span>.<span class="hljs-built_in">format</span>(filenum), mid_save_path=<span class="hljs-string">&quot;./&quot;</span>)<br>    <span class="hljs-comment"># mapper = Mapper(filepath=&#x27;file&#123;&#125;.txt&#x27;.format(filenum), mid_save_path=&quot;./&quot;, worker_id=filenum)</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Mapper <span class="hljs-subst">&#123;mapper.worker_id&#125;</span> started...&quot;</span>)<br>    filename=mapper.run()<br>    d.append(filename)<br>    barrier.wait()<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Worker <span class="hljs-subst">&#123;mapper.worker_id&#125;</span> job completed!&quot;</span>)<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">reducer_task</span>(<span class="hljs-params">d:<span class="hljs-built_in">list</span></span>):<br>    reducer = Reducer(filepath_lists=d, save_path=<span class="hljs-string">&quot;./&quot;</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Reducer <span class="hljs-subst">&#123;reducer.worker_id&#125;</span> started...&quot;</span>)<br>    reducer.run()<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    <span class="hljs-comment"># use Manager context to create shared memory</span><br>    <span class="hljs-keyword">with</span> Manager() <span class="hljs-keyword">as</span> manager:<br>        intermediate_l = manager.<span class="hljs-built_in">list</span>()<br>        process_list = []<br>        <br>        barrier = mp.Barrier(parties=num_mapper, <span class="hljs-built_in">open</span>)<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_mapper):<br>            process_list.append(mp.Process(target=mapper_task, args=(i+<span class="hljs-number">1</span>, intermediate_l, barrier)))<br>            process_list[i].start()<br>            process_list[i].join()<br>        <br>        <span class="hljs-comment"># 结束同步屏障</span><br>        <span class="hljs-comment"># barrier.wait()</span><br>        barrier.abort()<br>        <br>        process_list = []<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_reducer):<br>            process_list.append(mp.Process(target=reducer_task, args=(intermediate_l,)))<br>            process_list[i].start()<br>            process_list[i].join()<br></code></pre></td></tr></table></figure><h3 id="to-do">To-do</h3><ul><li>barrier同步屏障这一块设置有bug，会进入死锁。暂时未解决该问题。</li></ul>]]></content>
    
    
    <categories>
      
      <category>编程学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Map-Reduce</tag>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>first_encrypted_blog</title>
    <link href="/posts/fd1c2b6f/"/>
    <url>/posts/fd1c2b6f/</url>
    
    <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <script id="hbeData" type="hbeData" data-hmacdigest="450987296873915f2dbaf4d35bd2c2a3b0e85d649b3e6ffcdb39a2d61aca45f2">56bea287a274feb1df45a8617b2a488fab61fdbc84cc38a50a6176d0a17b2b412a584c2c37c5a145a6526672e3b8bec55b2ea34acf12857cc40268c00555506455d9c0e88201eb7625dc0226dc91bb66c6af83aab4f8211114b36d07ff1d03bf9bfd4b78c1c06c75e16aa6d6e239695094f292177c85b1946f76e3ba4789fcfda23a607764e384ddf07b35fe63ac6d56853773197f56dd1134055633c0c4327ff16a1db1131ba34e7498551bc57deebde6311efdcb361b2b1deada82443dcc5b6c49fbafd5c214b73ccfd6c8bbdce3f98b74a0968e4f836e82932499079e5f1851b2426f318460bb0767e187cb55cbdf0d21cd73351ab69991bbce77d02fdbf1e358d6c4c1e8f2c3f6481dab81202186c74d52c8c318bbf35f97e41acf439f927d81067e20d721cc878fbf96639dca27df7480333eb415f0f28eccdc074125397b4d00c3e94508a99d9695d5db1cf0e43c18feaaaf737cb46ce993111ea905b9f9d98d714b1715ed72ad697813c68dddf9473e53a099be4379baffcdc3f06891268476e4910c640028f82743e1e2264204d87257eac76dd7878d6546027d990cf5939b1bbbf2d03e577054e989ba6c759dbf5cf616dedcec9f49496d34e454bd08de1d8bb5cdca761ff4e3195f7da32ddb260e90817a6383c6349291d580241d8e4334327cc5ad4e9a1eb752ba45def013ce65e3b5d023d023eab512befd8d69</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">You must enter the password to read.</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
    
    
    <categories>
      
      <category>工具</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>charts_example</title>
    <link href="/posts/29446e2a/"/>
    <url>/posts/29446e2a/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script><p>参考链接：https://shen-yu.gitee.io/2020/chartjs/</p><div style="width: 90%;margin: 0 auto">    <canvas id="chart4479" style="height: 300px"></canvas></div><script src="https://cdn.jsdelivr.net/npm/chart.js@2.9.3/dist/Chart.min.js"></script><script type="text/javascript">    var ctx = document.getElementById('chart4479').getContext('2d');    var options =     {    type: 'line',    data: {    labels: ['January', 'February', 'March', 'April', 'May', 'June', 'July'],    datasets: [{        label: 'My First dataset',        backgroundColor: 'rgb(255, 99, 132)',        borderColor: 'rgb(255, 99, 132)',        data: [0, 10, 5, 2, 20, 30, 45]        }]    },    options: {        responsive: true,        title: {        display: true,        text: 'Chart.js Line Chart'        }    }};    new Chart(ctx, options);</script>]]></content>
    
    
    <categories>
      
      <category>工具</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Spconv编程学习</title>
    <link href="/posts/c5ac3feb/"/>
    <url>/posts/c5ac3feb/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script><h1 id="spconv-编程学习">Spconv 编程学习</h1><p>spconv版本：2.x</p><p>参考链接:https://github.com/traveller59/spconv/blob/master/docs/USAGE.md官方repo给的教程</p><p>下文大部分直接粘贴，然后加上自己实验后修改补充的一些中文讲解内容。</p><h2 id="short-api-description">Short API description</h2><figure class="highlight elm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs elm"><span class="hljs-keyword">import</span> spconv.pytorch <span class="hljs-keyword">as</span> spconv<br><span class="hljs-title">from</span> spconv.pytorch <span class="hljs-keyword">import</span> functional <span class="hljs-keyword">as</span> Fsp<br><span class="hljs-title">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-title">from</span> spconv.pytorch.utils <span class="hljs-keyword">import</span> PointToVoxel<br><span class="hljs-title">from</span> spconv.pytorch.hash <span class="hljs-keyword">import</span> HashTable<br></code></pre></td></tr></table></figure><table><thead><tr class="header"><th>Layer APIs</th><th>Common Usage</th><th>Dense Version</th><th>Note</th></tr></thead><tbody><tr class="odd"><td><code>spconv.SparseConv3d</code></td><td>Downsample</td><td><code>nn.Conv3d</code></td><td>Use <code>indice_key</code> to save data for inverse</td></tr><tr class="even"><td><code>spconv.SubMConv3d</code></td><td>Convolution</td><td>N/A</td><td>Use <code>indice_key</code> to save data for reuse</td></tr><tr class="odd"><td><code>spconv.SparseInverseConv3d</code></td><td>Upsample</td><td>N/A</td><td>Use pre-saved <code>indice_key</code> to upsample</td></tr><tr class="even"><td><code>spconv.SparseConvTranspose3d</code></td><td>Upsample (don't use this)</td><td><code>nn.ConvTranspose3d</code></td><td>VERY SLOW and CAN'T RECOVER ORIGIN POINT CLOUD</td></tr><tr class="odd"><td><code>spconv.SparseMaxPool3d</code></td><td>Downsample</td><td><code>nn.MaxPool3d</code></td><td>Use <code>indice_key</code> to save data for inverse</td></tr><tr class="even"><td><code>spconv.SparseSequential</code></td><td>Container</td><td><code>nn.Sequential</code></td><td>support layers above and<code>nn.ReLU, nn.BatchNorm, ...</code></td></tr></tbody></table><table><thead><tr class="header"><th>Functional APIs</th><th>Usage</th></tr></thead><tbody><tr class="odd"><td><code>Fsp.sparse_add</code></td><td>Add sparse tensors with same shape and different indices</td></tr></tbody></table><table><thead><tr class="header"><th>Input APIs</th><th>Usage</th></tr></thead><tbody><tr class="odd"><td><code>PointToVoxel</code></td><td>point cloud to voxels</td></tr></tbody></table><table><thead><tr class="header"><th>Misc APIs</th><th>Usage</th></tr></thead><tbody><tr class="odd"><td><code>HashTable</code></td><td>hash table, one-slot</td></tr></tbody></table><table><thead><tr class="header"><th>Layer APIs</th><th><ahref="https://github.com/mit-han-lab/torchsparse">torchsparse</a></th><th><ahref="https://github.com/NVIDIA/MinkowskiEngine">MinkowskiEngine</a></th></tr></thead><tbody><tr class="odd"><td><code>spconv.SparseConv3d</code></td><td><code>Conv3d(stride!=1, transpose=False)</code></td><td><code>MinkowskiConvolution(stride!=1)</code></td></tr><tr class="even"><td><code>spconv.SubMConv3d</code></td><td><code>Conv3d(stride=1, transpose=False)</code></td><td><code>MinkowskiConvolution(stride=1)</code></td></tr><tr class="odd"><td><code>spconv.SparseInverseConv3d</code></td><td><code>Conv3d(stride!=1, transpose=True)</code></td><td><code>MinkowskiConvolutionTranspose</code></td></tr><tr class="even"><td><code>spconv.SparseConvTranspose3d</code></td><td>N/A</td><td><code>MinkowskiConvolutionTranspose</code></td></tr><tr class="odd"><td><code>spconv.SparseMaxPool3d</code></td><td>N/A</td><td><code>MinkowskiMaxPooling</code></td></tr></tbody></table><h2 id="concept">Concept</h2><ul><li>Sparse Conv Tensor: like hybird <ahref="https://pytorch.org/docs/stable/sparse.html#sparse-coo-docs">torch.sparse_coo_tensor</a>but only have two difference: 1. SparseConvTensor only have one densedim, 2. indice of SparseConvTensor is transposed. see torch doc for moredetails.</li><li>Sparse Convolution: equivalent to perform dense convolution when youconvert SparseConvTensor to dense. Sparse Convolution only runcalculation on valid data.</li><li>Submanifold Convolution (SubMConv): like Sparse Convolution butindices keeps same. imagine that you copy same spatial structure tooutput, then iterate them, get input coordinates by conv rule, finallyapply convolution <strong>ONLY</strong> in these outputcoordinates.</li></ul><h2 id="sparseconvtensor">SparseConvTensor</h2><ul><li>features: <code>[N, num_channels]</code> tensor.</li><li>indices: <code>[N, (batch_idx + x + y + z)]</code> coordinate tensorwith batch axis. note that the coordinates xyz order MUST match spatialshape and conv params such as kernel size</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> spconv.pytorch <span class="hljs-keyword">as</span> spconv<br>features = <span class="hljs-comment"># your features with shape [N, num_channels]</span><br>indices = <span class="hljs-comment"># your indices/coordinates with shape [N, ndim + 1], batch index must be put in indices[:, 0]</span><br>spatial_shape = <span class="hljs-comment"># spatial shape of your sparse tensor, spatial_shape[i] is shape of indices[:, 1 + i].</span><br>batch_size = <span class="hljs-comment"># batch size of your sparse tensor.</span><br>x = spconv.SparseConvTensor(features, indices, spatial_shape, batch_size)<br>x_dense_NCHW = x.dense() <span class="hljs-comment"># convert sparse tensor to dense NCHW tensor.</span><br></code></pre></td></tr></table></figure><h3 id="sparse-convolution">Sparse Convolution</h3><p>注：此处代码有改动</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br>os.environ[<span class="hljs-string">&#x27;SPCONV_DEBUG_SAVE_PATH&#x27;</span>]=<span class="hljs-string">&quot;./spconv.log&quot;</span><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> spconv.pytorch <span class="hljs-keyword">as</span> spconv<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ExampleNet</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, shape</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.net = spconv.SparseSequential(<br>        <span class="hljs-comment"># API Paramemter: in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, </span><br>        <span class="hljs-comment"># groups=1, bias=True, padding_mode=&#x27;zeros&#x27;, device=None, dtype=None</span><br>            spconv.SparseConv3d(<span class="hljs-number">32</span>, <span class="hljs-number">64</span>, <span class="hljs-number">3</span>, padding=(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)), <span class="hljs-comment"># just like nn.Conv3d but don&#x27;t support group</span><br>            nn.BatchNorm1d(<span class="hljs-number">64</span>), <span class="hljs-comment"># non-spatial layers can be used directly in SparseSequential.</span><br>            nn.ReLU(),<br>            <span class="hljs-comment"># Kerner是3的时候，注意设置Padding为1，才能保持尺寸一致</span><br>            spconv.SubMConv3d(<span class="hljs-number">64</span>, <span class="hljs-number">64</span>, <span class="hljs-number">3</span>, padding=(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>),indice_key=<span class="hljs-string">&quot;subm0&quot;</span>),<br>            nn.BatchNorm1d(<span class="hljs-number">64</span>),<br>            nn.ReLU(),<br>        <span class="hljs-comment"># when use submanifold convolutions, their indices can be shared to save indices generation time.</span><br>            spconv.SubMConv3d(<span class="hljs-number">64</span>, <span class="hljs-number">64</span>, <span class="hljs-number">3</span>, indice_key=<span class="hljs-string">&quot;subm0&quot;</span>),<br>            nn.BatchNorm1d(<span class="hljs-number">64</span>),<br>            nn.ReLU(),<br>        )<br>        self.conv1 = spconv.SparseConv3d(<span class="hljs-number">32</span>, <span class="hljs-number">64</span>, <span class="hljs-number">3</span>)<br>        self.bn1 = nn.BatchNorm1d(<span class="hljs-number">64</span>)<br>        self.relu1 = nn.ReLU()<br>        self.shape = shape<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, features, coors, batch_size</span>):<br>        coors = coors.<span class="hljs-built_in">int</span>() <span class="hljs-comment"># unlike torch, this library only accept int coordinates.</span><br>        x = spconv.SparseConvTensor(features, coors, self.shape, batch_size)<br>        <span class="hljs-keyword">return</span> self.net(x)<br>    <br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    points_feature_np = np.array(np.random.random(size=<span class="hljs-number">1000</span>))<br>    points_feature_np = points_feature_np[:, np.newaxis]<br>    <span class="hljs-comment"># 目前Spconv版本要求 变量类型是float32</span><br>    points_feature = torch.from_numpy(points_feature_np).to(torch.float32)<br>    points_feature = points_feature.expand_as(torch.zeros((<span class="hljs-number">1000</span>, <span class="hljs-number">32</span>)))<br>    points_indices_np = np.array([np.zeros(<span class="hljs-number">1000</span>), np.random.randint(<span class="hljs-number">360</span>, size=<span class="hljs-number">1000</span>), np.random.randint(<span class="hljs-number">480</span>, size=<span class="hljs-number">1000</span>), np.random.randint(<span class="hljs-number">32</span>, size=<span class="hljs-number">1000</span>)]).transpose()<br>    points_indices = torch.from_numpy(points_indices_np).to(torch.int32)<br>    <br>    model = ExampleNet(shape=[<span class="hljs-number">360</span>, <span class="hljs-number">480</span>, <span class="hljs-number">32</span>]).cuda()<br>    <span class="hljs-comment"># 注意此处向量需要连续化.contiguous()</span><br>    output_points_feature = model(points_feature.cuda().contiguous(), points_indices.cuda().contiguous(), <span class="hljs-number">1</span>)<br>    <span class="hljs-built_in">print</span>(output_points_feature.features.shape)<br>        <br>        <br></code></pre></td></tr></table></figure><h3 id="inverse-convolution">Inverse Convolution</h3><p>Inverse sparse convolution means "inv" of sparse convolution. theoutput of inverse convolution contains same indices as input of sparseconvolution.</p><p><strong>WARNING</strong> <code>SparseInverseConv</code> isn'tequivalent to <code>SparseConvTranspose</code>. SparseConvTranspose isequivalent to <code>ConvTranspose</code> in pytorch, butSparseInverseConv isn't.</p><p>Inverse convolution usually used in semantic segmentation.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">ExampleNet</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, shape</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.net = spconv.SparseSequential(<br>            spconv.SparseConv3d(<span class="hljs-number">32</span>, <span class="hljs-number">64</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>, indice_key=<span class="hljs-string">&quot;cp0&quot;</span>),<br>            spconv.SparseInverseConv3d(<span class="hljs-number">64</span>, <span class="hljs-number">32</span>, <span class="hljs-number">3</span>, indice_key=<span class="hljs-string">&quot;cp0&quot;</span>), <span class="hljs-comment"># need provide kernel size to create weight</span><br>        )<br>        self.shape = shape<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, features, coors, batch_size</span>):<br>        coors = coors.<span class="hljs-built_in">int</span>()<br>        x = spconv.SparseConvTensor(features, coors, self.shape, batch_size)<br>        <span class="hljs-keyword">return</span> self.net(x)<br></code></pre></td></tr></table></figure><h4 id="common-mistake">Common Mistake</h4><ul><li>issue <ahref="https://github.com/traveller59/spconv/issues/467">#467</a></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">WrongNet</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, shape</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.Encoder = spconv.SparseConv3d(channels, channels, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, indice_key=<span class="hljs-string">&quot;cp1&quot;</span>,algo=algo)<br>        self.Sparse_Conv = spconv.SparseConv3d(channels, channels, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">1</span>,algo=algo)<br>        self.Decoder = spconv.SparseInverseConv3d(channels, channels, kernel_size=<span class="hljs-number">3</span>, indice_key=<span class="hljs-string">&quot;cp1&quot;</span>,algo=algo)   <br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, sparse_tensor</span>):<br>        encoded = self.Encoder(sparse_tensor)<br>        s_conv = self.Sparse_Conv(encoded)<br>        <span class="hljs-keyword">return</span> self.Decoder(s_conv).features<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">CorrectNet</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, shape</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.Encoder = spconv.SparseConv3d(channels, channels, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, indice_key=<span class="hljs-string">&quot;cp1&quot;</span>,algo=algo)<br>        self.Sparse_Conv = spconv.SparseConv3d(channels, channels, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">1</span>, indice_key=<span class="hljs-string">&quot;cp2&quot;</span>,algo=algo)<br>        self.Sparse_Conv_Decoder = spconv.SparseInverseConv3d(channels, channels, kernel_size=<span class="hljs-number">3</span>, indice_key=<span class="hljs-string">&quot;cp2&quot;</span>,algo=algo)   <br>        self.Decoder = spconv.SparseInverseConv3d(channels, channels, kernel_size=<span class="hljs-number">3</span>, indice_key=<span class="hljs-string">&quot;cp1&quot;</span>,algo=algo)   <br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, sparse_tensor</span>):<br>        encoded = self.Encoder(sparse_tensor)<br>        s_conv = self.Sparse_Conv(encoded)<br>        <span class="hljs-keyword">return</span> self.Decoder(self.Sparse_Conv_Decoder(s_conv)).features<br></code></pre></td></tr></table></figure><p>The <code>Sparse_Conv</code> in <code>ExampleNet</code> Changespatial structure of output of <code>Encoder</code>, so we can't inverseback to input of <code>Encoder</code> via <code>Decoder</code>, we needto inverse from <code>Sparse_Conv.output</code> to<code>Encoder.output</code> via <code>Sparse_Conv_Decoder</code>, theninverse from <code>Encoder.output</code> to <code>Encoder.input</code>via <code>Decoder</code>.</p><h3 id="sparse-add">Sparse Add</h3><p>In sematic segmentation network, we may use conv1x3, 3x1 and 3x3 in ablock, but it's impossible to sum result from these layers becauseregular add requires same indices.</p><p>spconv &gt;= 2.1.17 provide a operation to add sparse tensors withdifferent indices (shape must same), but with limits:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> spconv.pytorch <span class="hljs-keyword">import</span> functional <span class="hljs-keyword">as</span> Fsp<br>res_1x3 = conv1x3(x)<br>res_3x1 = conv3x1(x)<br><span class="hljs-comment"># WRONG</span><br><span class="hljs-comment"># because we can&#x27;t &quot;inverse&quot; this operation</span><br>wrong_usage_cant_inverse = Fsp.sparse_add(res_1x3, res_3x1)<br><br><span class="hljs-comment"># CORRECT</span><br><span class="hljs-comment"># res_3x3 already contains all indices of res_1x3 and res_3x1, </span><br><span class="hljs-comment"># so output spatial structure isn&#x27;t changed, we can &quot;inverse&quot; back.</span><br>res_3x3 = conv3x3(x)<br>correct = Fsp.sparse_add(res_1x3, res_3x1, res_3x3)<br></code></pre></td></tr></table></figure><p>If you use a network without <code>SparseInverseConv</code>, limitsabove aren't exists, the only drawback of <code>sparse_add</code> isthat it run slower than simple aligned add.</p><h3 id="fast-mixed-percision-training">Fast Mixed PercisionTraining</h3><p>see example/mnist_sparse. we support <code>torch.cuda.amp</code>.</p><h3 id="utility-functions">Utility functions</h3><ul><li>convert point cloud to voxel</li></ul><p>voxel generator in spconv generate indices in <strong>ZYX</strong>order, the params format are <strong>XYZ</strong>.</p><p>generated indices don't include batch axis, you need to add it byyourself.</p><p>see examples/voxel_gen.py for examples.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> spconv.pytorch.utils <span class="hljs-keyword">import</span> PointToVoxel, gather_features_by_pc_voxel_id<br><span class="hljs-comment"># this generator generate ZYX indices.</span><br>gen = PointToVoxel(<br>    vsize_xyz=[<span class="hljs-number">0.1</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">0.1</span>], <br>    coors_range_xyz=[-<span class="hljs-number">80</span>, -<span class="hljs-number">80</span>, -<span class="hljs-number">2</span>, <span class="hljs-number">80</span>, <span class="hljs-number">80</span>, <span class="hljs-number">6</span>], <br>    num_point_features=<span class="hljs-number">3</span>, <br>    max_num_voxels=<span class="hljs-number">5000</span>, <br>    max_num_points_per_voxel=<span class="hljs-number">5</span>)<br>pc = np.random.uniform(-<span class="hljs-number">10</span>, <span class="hljs-number">10</span>, size=[<span class="hljs-number">1000</span>, <span class="hljs-number">3</span>])<br>pc_th = torch.from_numpy(pc)<br>voxels, coords, num_points_per_voxel = gen(pc_th, empty_mean=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><p>If you want to get label for every point of your pc, you need to useanother function to get pc_voxel_id and gather features from sematicsegmentation result:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">voxels, coords, num_points_per_voxel, pc_voxel_id = gen.generate_voxel_with_id(pc_th, empty_mean=<span class="hljs-literal">True</span>)<br>seg_features = YourSegNet(...)<br><span class="hljs-comment"># if voxel id is invalid (point out of range, or no space left in a voxel)</span><br><span class="hljs-comment"># features will be zero.</span><br>point_features = gather_features_by_pc_voxel_id(seg_features, pc_voxel_id)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>编程学习</category>
      
      <category>点云</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python,PyTorch,Spconv</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文十问:GASN(ECCV2022)</title>
    <link href="/posts/a69f9b72/"/>
    <url>/posts/a69f9b72/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script><h1 id="gasneccv-2022">GASN(ECCV 2022)</h1><blockquote><p>论文标题: Efficient Point Cloud Segmentation with Geometry-awareSparse Networks 论文地址: <ahref="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136990193.pdf">PDF</a>作者单位: HKUST, DeepRoute 代码地址:https://github.com/ItIsFriday/PcdSeg</p></blockquote><h2 id="q1-论文试图解决什么问题">Q1 论文试图解决什么问题？</h2><p>本文提出一个新的3D稀疏网络框架，可用于室外场景大规模自动驾驶数据集，在满足SOTA精度的前提下，有非常快的速度，更少的内存消耗，满足更好的实时性。</p><h2 id="q2-这是否是一个新的问题">Q2 这是否是一个新的问题？</h2><p>不是，很多工作都在尝试做这个</p><h2 id="q3-这篇文章要验证一个什么科学假设">Q3这篇文章要验证一个什么科学假设？</h2><ol type="1"><li>本文认为保证精度的关键在于对点云multi-scale特征的充分利用</li><li>本文认为保证速度并且降低显存的关键，是放弃基于点级别的操作(pooling),网络设计全部基于sparse voxel-basedrepresentation。比如稀疏卷积，池化，多特征融合，MLP均在该稀疏特征层次上完成。</li></ol><h2id="q4-有哪些相关研究如何归类谁是这一课题在领域内值得关注的研究员">Q4有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？</h2><ol type="1"><li>基于Raw Point的网络: PointNet, PointNet++ (maintain and utilize thepointwise geometry).</li><li>通过采样降低点的数量，提高效率: RandLA，KPConv.（但是采样会导致极大的信息损失，这些网络在室外场景精度表现不高）</li><li>划分/投影到预先设定的Grids里(如2D, 3D, Sparse3D)，再进行卷积/稀疏卷积操作：AF2S3Net, Cylinder3D, SECOND,PointPillar…4.Point, Voxel, Range等多种representation融合感知的网络:PVCNN, SPVCNN, DRINet, RPVNet</li><li>基于图的(主要室内用得多，大规模室外计算速度/显存开销偏大)</li><li>基于transformer的</li></ol><h2 id="q5-论文中提到的解决方案之关键是什么">Q5论文中提到的解决方案之关键是什么？</h2><ol type="1"><li>整体网络由Sparse Feature Encoder(SFE)和Sparse Geometry FeatureEnhancement(SGFE)两个模块组成。前者用稀疏卷积提取特征后送入SGFE；SGFE设计了多尺度稀疏特征投影模块(Multi-scale SparseProjection)来增强对几何信息的提取，将多尺度融合后的特征送到下一层的SFE。</li><li>相比于PVCNN，SPVCNN,DRINet等point+voxel的表征方式，作者只保留了sparse-voxelrepresentation送入SFE和SGFE，不需要point-wiserepresentation，大大减小了计算开销与显存开销。</li><li>point-wise representation能完整地提取点云的局部几何信息。为了弥补放弃使用pointrepresentation带来的几何信息感知缺失，作者设计了多尺度稀疏特征投影模块(Multi-scale SparseProjection)来增强对几何信息的提取。认为不同的尺度下提取的特征是对点云结构的先验知识学习，在不同层次上获取并编码点云的几何特征。</li></ol><h2 id="q6-论文中的实验是如何设计的">Q6 论文中的实验是如何设计的？</h2><p>主要做的语义分割实验，比较mIoU, 参数量，显存占用和速度</p><h2 id="q7-用于定量评估的数据集是什么代码有没有开源">Q7用于定量评估的数据集是什么？代码有没有开源？</h2><p>nuScenes, semanticKitti.</p><h2 id="q8-论文中的实验及结果有没有很好地支持需要验证的科学假设">Q8论文中的实验及结果有没有很好地支持需要验证的科学假设？</h2><p>SemanticKitti和nuScenes，在mIoU和Cylinder3D, RPVNet,AF2S3net等SOTA网络基本持平的基础上，速度快2-5倍，显存占用量少2-3倍，参数量少2-10倍，仅一张2080Ti能跑的很好，性能优异</p><h2 id="q9-这篇论文到底有什么贡献">Q9 这篇论文到底有什么贡献？</h2><p>提出了一个性能好，速度快，显存占用少，适用于大规模室外点云数据集的网络</p><h2 id="q10-下一步呢有什么工作可以继续深入">Q10下一步呢？有什么工作可以继续深入？</h2><p>别卷了，卷不动了，有点强</p>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
      <category>点云</category>
      
      <category>通用点云理解与网络设计</category>
      
      <category>2022</category>
      
    </categories>
    
    
    <tags>
      
      <tag>论文十问系列</tag>
      
      <tag>ECCV2022</tag>
      
      <tag>Efficiency</tag>
      
      <tag>Sparse Voxel-Based Rrepresentation</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/posts/4a17b156/"/>
    <url>/posts/4a17b156/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script><p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your veryfirst post. Check <a href="https://hexo.io/docs/">documentation</a> formore info. If you get any problems when using Hexo, you can find theanswer in <ahref="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> oryou can ask me on <ahref="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="quick-start">Quick Start</h2><h3 id="create-a-new-post">Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <ahref="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="run-server">Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="generate-static-files">Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <ahref="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="deploy-to-remote-sites">Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <ahref="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p><p>推荐资料：</p><ol type="1"><li>如何搭配Fluid优雅的写一篇文档：https://hexo.fluid-dev.com/posts/fluid-write/</li><li>使用ECharts插件绘制炫酷图表：https://hexo.fluid-dev.com/posts/hexo-echarts/</li><li>给博客文章迁入PPT演示：https://hexo.fluid-dev.com/posts/hexo-nodeppt/</li></ol><p>优雅，实在是太优雅了！</p><hr /><p>下面我们开始不正经：</p><p>再次感谢Fluid官方和作者大大，为表感谢直接把你们的网站强行搬来套娃了😁</p><p>官方示例:</p><iframe src="https://hexo.fluid-dev.com/" width="100%" height="500" name="topFrame" scrolling="yes" noresize="noresize" frameborder="0" id="topFrame"></iframe><hr /><p>作者：</p><iframe src="https://zkqiang.cn/" width="100%" height="500" name="topFrame" scrolling="yes" noresize="noresize" frameborder="0" id="topFrame"></iframe>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>LaTeX讲解系列：常用数学符号大全</title>
    <link href="/posts/98374ae/"/>
    <url>/posts/98374ae/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script><h1 id="latex-讲解系列常用数学符号大全">LaTex讲解系列：常用数学符号大全</h1><div class="note note-primary">            <p>本文整理过程中着重参考了：</p><p>https://www.cnblogs.com/yalphait/articles/8685586.html（LaTex命令符号大全）</p><p>https://blog.csdn.net/u012684062/article/details/78398191 （LaTex所有常用数学符号整理）</p><p>这两份非常全面，非常感谢！尤其第一份大佬的，还介绍了不同符号在数学，物理计算机等不同学科中用法(比如方程组中用希腊小写字母，向量用粗体小写，矩阵用粗体大写等)，还有很多美观排版的建议与事例，强烈安利阅读一番！</p><p>除此之外整合了其他网站提到的常用的公式命令，一并列在最后<ahref="#参考资料">参考资料</a>中。</p>          </div><h2 id="不同的数学模式">不同的数学模式</h2><h3 id="重音符号">重音符号</h3><table><thead><tr class="header"><th><span class="math inline">\(\hat{a}\)</span><code>\hat&#123;a&#125;</code></th><th><span class="math inline">\(\check{a}\)</span><code>\check&#123;a&#125;</code></th><th><span class="math inline">\(\tilde{a}\)</span><code>\tilde&#123;a&#125;</code></th></tr></thead><tbody><tr class="odd"><td><span class="math inline">\(\grave{a}\)</span><code>\grave&#123;a&#125;</code></td><td><span class="math inline">\(\dot{a}\)</span><code>\dot&#123;a&#125;</code></td><td><span class="math inline">\(\ddot{a}\)</span><code>\ddot&#123;a&#125;</code></td></tr><tr class="even"><td><span class="math inline">\(\bar{a}\)</span><code>\bar&#123;a&#125;</code></td><td><span class="math inline">\(\vec{a}\)</span><code>\vac&#123;a&#125;</code></td><td><span class="math inline">\(\widehat{A}\)</span><code>\widehat&#123;A&#125;</code></td></tr><tr class="odd"><td><span class="math inline">\(\acute{a}\)</span><code>\acute&#123;a&#125;</code></td><td><span class="math inline">\(\breve{a}\)</span><code>\breve&#123;a&#125;</code></td><td><span class="math inline">\(\widetilde{A}\)</span><code>\widetilde&#123;A&#125;</code></td></tr></tbody></table><h2 id="其他">其他</h2><table><thead><tr class="header"><th><span class="math inline">\(\mathbf{E}\)</span><code>\mathbf&#123;E&#125;</code></th><th><span class="math inline">\(\mathbb{R}\)</span><code>\mathbb&#123;R&#125;</code></th><th><span class="math inline">\(\mathit{ABC}\)</span><code>\mathit&#123;ABC&#125;</code></th></tr></thead><tbody><tr class="odd"><td><span class="math inline">\(\mathrm{ABC}\)</span><code>\mathrm&#123;ABC&#125;</code></td><td><span class="math inline">\(\mathfrak{ABC}\)</span><code>\mathfrak&#123;ABC&#125;</code></td><td></td></tr></tbody></table><h2 id="希腊字母">希腊字母</h2><table><thead><tr class="header"><th><span class="math inline">\(\alpha\)</span> <code>\alpha</code></th><th><span class="math inline">\(\theta\)</span> <code>\theta</code></th><th><span class="math inline">\(o\)</span> <code>o</code></th><th><span class="math inline">\(\upsilon\)</span><code>\upsilon</code></th></tr></thead><tbody><tr class="odd"><td><span class="math inline">\(\beta\)</span> <code>\beta</code></td><td><span class="math inline">\(\vartheta\)</span><code>\vartheta</code></td><td><span class="math inline">\(\pi\)</span> <code>\pi</code></td><td><span class="math inline">\(\phi\)</span> <code>\phi</code></td></tr><tr class="even"><td><span class="math inline">\(\gamma\)</span> <code>\gamma</code></td><td><span class="math inline">\(\iota\)</span> <code>\iota</code></td><td><span class="math inline">\(\varpi\)</span> <code>\varpi</code></td><td><span class="math inline">\(\varphi\)</span><code>\varphi</code></td></tr><tr class="odd"><td><span class="math inline">\(\delta\)</span> <code>\delta</code></td><td><span class="math inline">\(\kappa\)</span> <code>\kappa</code></td><td><span class="math inline">\(\rho\)</span> <code>\rho</code></td><td><span class="math inline">\(\chi\)</span> <code>\chi</code></td></tr><tr class="even"><td><span class="math inline">\(\epsilon\)</span><code>\epsilon</code></td><td><span class="math inline">\(\lambda\)</span><code>\lambda</code></td><td><span class="math inline">\(\varrho\)</span><code>\varrho</code></td><td><span class="math inline">\(\psi\)</span> <code>\psi</code></td></tr><tr class="odd"><td><span class="math inline">\(\varepsilon\)</span><code>\varepsilon</code></td><td><span class="math inline">\(\mu\)</span> <code>\mu</code></td><td><span class="math inline">\(\sigma\)</span> <code>\sigma</code></td><td><span class="math inline">\(\omega\)</span> <code>\omega</code></td></tr><tr class="even"><td><span class="math inline">\(\zeta\)</span> <code>\zeta</code></td><td><span class="math inline">\(\nu\)</span> <code>\nu</code></td><td><span class="math inline">\(\varsigma\)</span><code>\varsigma</code></td><td></td></tr><tr class="odd"><td><span class="math inline">\(\eta\)</span> <code>\eta</code></td><td><span class="math inline">\(\xi\)</span> <code>\xi</code></td><td><span class="math inline">\(\tau\)</span> <code>\tau</code></td><td></td></tr><tr class="even"><td><span class="math inline">\(\Gamma\)</span> <code>\Gamma</code></td><td><span class="math inline">\(\Lambda\)</span><code>\Lambda</code></td><td><span class="math inline">\(\Sigma\)</span> <code>\Sigma</code></td><td><span class="math inline">\(\Psi\)</span> <code>\Psi</code></td></tr><tr class="odd"><td><span class="math inline">\(\Theta\)</span> <code>\Theta</code></td><td><span class="math inline">\(\Pi\)</span> <code>\Pi</code></td><td><span class="math inline">\(\Phi\)</span> <code>\Phi</code></td><td></td></tr></tbody></table><h2 id="运算符">运算符</h2><h3 id="关系符号">关系符号</h3><p>可以在下列符号的相应命令前加上<code>\not</code>命令，得到其否定形式，例如<span class="math inline">\(\in\)</span>与<span class="math inline">\(\not \in\)</span> 。</p><table><thead><tr class="header"><th><span class="math inline">\(\leq\)</span> <code>\le</code></th><th><span class="math inline">\(\ge\)</span> <code>\ge</code></th><th><span class="math inline">\(\equiv\)</span> <code>\equiv</code></th><th><span class="math inline">\(\ll\)</span> <code>\ll</code></th></tr></thead><tbody><tr class="odd"><td><span class="math inline">\(\gg\)</span> <code>\gg</code></td><td><span class="math inline">\(\doteq\)</span> <code>\doteq</code></td><td><span class="math inline">\(\prec\)</span> <code>\prec</code></td><td><span class="math inline">\(\succ\)</span> <code>\succ</code></td></tr><tr class="even"><td><span class="math inline">\(\simeq\)</span> <code>\simeq</code></td><td><span class="math inline">\(\subset\)</span><code>\subset</code></td><td><span class="math inline">\(\supset\)</span><code>\supset</code></td><td><span class="math inline">\(\approx\)</span><code>\approx</code></td></tr><tr class="odd"><td><span class="math inline">\(\subseteq\)</span><code>\subseteq</code></td><td><span class="math inline">\(\supseteq\)</span><code>\supseteq</code></td><td><span class="math inline">\(\cong\)</span> <code>\cong</code></td><td><span class="math inline">\(\in\)</span> <code>\in</code></td></tr><tr class="even"><td><span class="math inline">\(\vdash\)</span> <code>\vdash</code></td><td><span class="math inline">\(\dashv\)</span> <code>\dashv</code></td><td></td><td></td></tr></tbody></table><h3 id="运算符-1">运算符</h3><table><thead><tr class="header"><th><span class="math inline">\(\pm\)</span> <code>\pm</code></th><th><span class="math inline">\(\mp\)</span> <code>\mp</code></th><th><span class="math inline">\(\cdot\)</span> <code>\cdot</code></th><th><span class="math inline">\(\div\)</span> <code>\div</code></th></tr></thead><tbody><tr class="odd"><td><span class="math inline">\(\times\)</span> <code>\times</code></td><td><span class="math inline">\(\setminus\)</span><code>\setminus</code></td><td><span class="math inline">\(\star\)</span> <code>\star</code></td><td><span class="math inline">\(\cup\)</span> <code>\cup</code></td></tr><tr class="even"><td><span class="math inline">\(\cap\)</span> <code>\cap</code></td><td><span class="math inline">\(\ast\)</span> <code>\ast</code></td><td><span class="math inline">\(\circ\)</span> <code>\circ</code></td><td><span class="math inline">\(\lor\)</span> <code>\lor</code></td></tr><tr class="odd"><td><span class="math inline">\(\land\)</span> <code>\land</code></td><td><span class="math inline">\(\oplus\)</span> <code>\oplus</code></td><td><span class="math inline">\(\ominus\)</span><code>\ominus</code></td><td><span class="math inline">\(\diamond\)</span><code>\diamond</code></td></tr><tr class="even"><td><span class="math inline">\(\otimes\)</span><code>\otimes</code></td><td><span class="math inline">\(\odot\)</span> <code>\odot</code></td><td><span class="math inline">\(\oslash\)</span><code>\oslash</code></td><td><span class="math inline">\(\amalg\)</span> <code>\amalg</code></td></tr><tr class="odd"><td><span class="math inline">\(\bigtriangleup\)</span><code>\bigtrianglep</code></td><td><span class="math inline">\(\bigtriangledown\)</span><code>\bigtriangledown</code></td><td><span class="math inline">\(\dagger\)</span><code>\dagger</code></td><td><span class="math inline">\(\ddagger\)</span><code>\ddagger</code></td></tr><tr class="even"><td><span class="math inline">\(\wr\)</span> <code>\wr</code></td><td><span class="math inline">\(\lnot\)</span> <code>\not</code></td><td></td><td></td></tr></tbody></table><h3 id="大运算符">”大“运算符</h3><table style="width:100%;"><thead><tr class="header"><th><span class="math inline">\(\sum_i^j\)</span><code>\sum_i^&#123;j&#125;</code></th><th><span class="math inline">\(\prod\)</span> <code>\prod</code></th><th><span class="math inline">\(\bigcup\)</span><code>\bigcup</code></th><th><span class="math inline">\(\bigcap\)</span><code>\bigcap</code></th></tr></thead><tbody><tr class="odd"><td><span class="math inline">\(\bigvee\)</span><code>\bigvee</code></td><td><span class="math inline">\(\bigwedge\)</span><code>\bigwedge</code></td><td><span class="math inline">\(\int_a^b\)</span><code>\int_a^b</code></td><td><span class="math inline">\(\oint\)</span> <code>\oint</code></td></tr><tr class="even"><td><span class="math inline">\(\bigoplus\)</span><code>\bigoplus</code></td><td><span class="math inline">\(\bigotimes\)</span><code>\bigotimes</code></td><td><span class="math inline">\(\bigodot\)</span><code>\bigodot</code></td><td></td></tr></tbody></table><h3 id="箭头">箭头</h3><table><thead><tr class="header"><th><span class="math inline">\(\leftarrow\)</span><code>\leftarrow</code></th><th><span class="math inline">\(\longleftarrow\)</span><code>\longleftarrow</code></th><th><span class="math inline">\(\rightarrow\)</span><code>\rightarrow</code></th><th><span class="math inline">\(\longrightarrow\)</span><code>\longrightarrow</code></th></tr></thead><tbody><tr class="odd"><td><span class="math inline">\(\leftrightarrow\)</span><code>\leftrightarrow</code></td><td><span class="math inline">\(\longleftrightarrow\)</span><code>\longleftrightarrrow</code></td><td><span class="math inline">\(\Leftarrow\)</span><code>\Leftarrow</code></td><td><span class="math inline">\(\Rightarrow\)</span><code>\Rightarrow</code></td></tr><tr class="even"><td><span class="math inline">\(\Leftrightarrow\)</span><code>\Leftrightarrow</code></td><td><span class="math inline">\(\iff\)</span><code>\iff(bigger spaces)</code></td><td><span class="math inline">\(\mapsto\)</span><code>\mapsto</code></td><td><span class="math inline">\(\rightleftharpoons\)</span><code>\rightleftharpoons</code></td></tr><tr class="odd"><td><span class="math inline">\(\leadsto\)</span><code>\leadsto</code></td><td></td><td></td><td></td></tr></tbody></table><h3 id="常用函数符号">常用函数符号</h3><table><thead><tr class="header"><th><span class="math inline">\(\sin \theta\)</span><code>\sin \theta</code></th><th><span class="math inline">\(\cos\theta\)</span><code>\cos\theta</code></th><th><span class="math inline">\(\tan\theta\)</span><code>\tan\theta</code></th><th><span class="math inline">\(\arcsin\)</span><code>\arcsin</code></th></tr></thead><tbody><tr class="odd"><td><span class="math inline">\(\arccos\)</span><code>\arccos</code></td><td><span class="math inline">\(\cosh\)</span> <code>\cosh</code></td><td><span class="math inline">\(\tanh\)</span> <code>\tanh</code></td><td><span class="math inline">\(\limsup\)</span><code>\limsup</code></td></tr><tr class="even"><td><span class="math inline">\(f&#39;(x) = \lim_{\Delta x \rightarrow0} \frac{f(x + \Delta x) - f(x)}{\Delta x}\)</span><code>\lim_&#123;\Delta x \rightarrow 0&#125; \frac&#123;f(x + \Delta x) - f(x)&#125;&#123;\Delta x&#125;</code></td><td><span class="math inline">\(\max\)</span> <code>\max</code></td><td><span class="math inline">\(\min\)</span> <code>\min</code></td><td><span class="math inline">\(\inf\)</span> <code>\inf</code></td></tr><tr class="odd"><td><span class="math inline">\(\log\)</span> <code>\log</code></td><td><span class="math inline">\(\ln\)</span> <code>\ln</code></td><td><span class="math inline">\(\ker\)</span> <code>\ker</code></td><td><span class="math inline">\(\Pr\)</span> <code>\Pr</code></td></tr><tr class="even"><td><span class="math inline">\(\dim\)</span> <code>\dim</code></td><td><span class="math inline">\(\det\)</span> <code>\det</code></td><td><span class="math inline">\(\exp\)</span> <code>\exp</code></td><td></td></tr></tbody></table><h3 id="其他-1">其他</h3><table><thead><tr class="header"><th><span class="math inline">\(\lvert a \rvert\)</span><code>\lvert a \rvert</code></th><th><span class="math inline">\(\lVert a \rVert\)</span><code>\lVert a \rVert</code></th><th><span class="math inline">\(\dots\)</span> <code>\dots</code></th><th><span class="math inline">\(\cdots\)</span> <code>\cdots</code>(formatrix)</th></tr></thead><tbody><tr class="odd"><td><span class="math inline">\(\vdots\)</span> <code>\vdots</code></td><td><span class="math inline">\(\ddots\)</span> <code>\ddots</code></td><td><span class="math inline">\(\forall\)</span><code>\forall</code></td><td><span class="math inline">\(\exists\)</span><code>\exists</code></td></tr><tr class="even"><td><span class="math inline">\(\partial\)</span><code>\partial</code></td><td><span class="math inline">\(\nabla\)</span> <code>\nabla</code></td><td><span class="math inline">\(\bot\)</span> <code>\bot</code></td><td><span class="math inline">\(\top\)</span> <code>\top</code></td></tr><tr class="odd"><td><span class="math inline">\(\angle\)</span> <code>\angle</code></td><td><span class="math inline">\(\surd\)</span> <code>\surd</code></td><td><span class="math inline">\(\emptyset\)</span><code>\emptyset</code></td><td><span class="math inline">\(\ell\)</span> <code>\ell</code></td></tr><tr class="even"><td><span class="math inline">\(\infty\)</span> <code>\infty</code></td><td><span class="math inline">\(\heartsuit\)</span><code>\heartsuit</code></td><td><span class="math inline">\(\clubsuit\)</span><code>\clubsuit</code></td><td><span class="math inline">\(\spadesuit\)</span><code>\spadesuit</code></td></tr><tr class="odd"><td><span class="math inline">\(\therefore\)</span><code>\therefore</code></td><td><span class="math inline">\(\because\)</span><code>\because</code></td><td><span class="math inline">\(\mathop{\arg\min}_{\theta}\)</span><code>\mathop&#123;\arg\min&#125;_&#123;\theta&#125;</code>(套在LaTeXequation环境下，下标<spanclass="math inline">\(\theta\)</span>能自动在字母下面)</td><td></td></tr></tbody></table><h2 id="矩阵表示">矩阵表示</h2><ol type="1"><li></li></ol><p><span class="math display">\[\begin{align}    \left[ \begin{matrix}  a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n} \\  a_{21} &amp; a_{22} &amp; \cdots &amp; a_{25} \\  \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\  a_{n1} &amp; a_{n2} &amp; \cdots &amp; a_{nn}  \end{matrix}\right]\end{align}\]</span></p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs latex"><span class="hljs-keyword">\begin</span>&#123;align&#125; <span class="hljs-comment">% 推荐用align或equation的数学环境</span><br><span class="hljs-keyword">\left</span>[ <span class="hljs-keyword">\begin</span>&#123;matrix&#125; <span class="hljs-comment">% 推荐使用matrix的环境, 如果使用array环境需要指定&#123;ccc&#125;列数</span><br>  a<span class="hljs-built_in">_</span>&#123;11&#125; <span class="hljs-built_in">&amp;</span> a<span class="hljs-built_in">_</span>&#123;12&#125; <span class="hljs-built_in">&amp;</span> <span class="hljs-keyword">\cdots</span> <span class="hljs-built_in">&amp;</span> a<span class="hljs-built_in">_</span>&#123;1n&#125; <span class="hljs-keyword">\\</span><br>  a<span class="hljs-built_in">_</span>&#123;21&#125; <span class="hljs-built_in">&amp;</span> a<span class="hljs-built_in">_</span>&#123;22&#125; <span class="hljs-built_in">&amp;</span> <span class="hljs-keyword">\cdots</span> <span class="hljs-built_in">&amp;</span> a<span class="hljs-built_in">_</span>&#123;25&#125; <span class="hljs-keyword">\\</span><br>  <span class="hljs-keyword">\vdots</span> <span class="hljs-built_in">&amp;</span> <span class="hljs-keyword">\vdots</span> <span class="hljs-built_in">&amp;</span> <span class="hljs-keyword">\ddots</span> <span class="hljs-built_in">&amp;</span> <span class="hljs-keyword">\vdots</span> <span class="hljs-keyword">\\</span><br>  a<span class="hljs-built_in">_</span>&#123;n1&#125; <span class="hljs-built_in">&amp;</span> a<span class="hljs-built_in">_</span>&#123;n2&#125; <span class="hljs-built_in">&amp;</span> <span class="hljs-keyword">\cdots</span> <span class="hljs-built_in">&amp;</span> a<span class="hljs-built_in">_</span>&#123;nn&#125;<br>  <span class="hljs-keyword">\end</span>&#123;matrix&#125;<span class="hljs-keyword">\right</span>] <span class="hljs-comment">% \left[ 与\right]配对，[还可以换成&#123;, (等 </span><br><span class="hljs-keyword">\end</span>&#123;align&#125;<br><br><span class="hljs-comment">% 如使用nicematrix宏包，需要在完整版的LaTex环境中引用该包</span><br><span class="hljs-comment">% 详见官方连接https://ctan.org/pkg/nicematrix</span><br></code></pre></td></tr></table></figure><ol start="2" type="1"><li>上下大括号</li></ol><p><span class="math display">\[\begin{align}&amp;\begin{matrix} 5050 \\ \overbrace{ 1+2+\cdots+100 }\end{matrix} \\\\&amp;\begin{matrix} \underbrace{ a+b+\cdots+z } \\ 26\end{matrix}\end{align}\]</span></p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs latex"><span class="hljs-keyword">\begin</span>&#123;align&#125;<br><span class="hljs-built_in">&amp;</span><span class="hljs-keyword">\begin</span>&#123;matrix&#125; 5050 <span class="hljs-keyword">\\</span> <span class="hljs-keyword">\overbrace</span>&#123; 1+2+<span class="hljs-keyword">\cdots</span>+100 &#125;<span class="hljs-keyword">\end</span>&#123;matrix&#125; <span class="hljs-keyword">\\</span><br><span class="hljs-keyword">\\</span><br><span class="hljs-built_in">&amp;</span><span class="hljs-keyword">\begin</span>&#123;matrix&#125; <span class="hljs-keyword">\underbrace</span>&#123; a+b+<span class="hljs-keyword">\cdots</span>+z &#125; <span class="hljs-keyword">\\</span> 26<span class="hljs-keyword">\end</span>&#123;matrix&#125;<br><br><span class="hljs-keyword">\end</span>&#123;align&#125;<br></code></pre></td></tr></table></figure><ol start="3" type="1"><li>方程组/分段函数</li></ol><p><span class="math display">\[=\begin{cases}3x + 5y +  z, &amp; x+y+z &lt;1\\7x - 2y + 4z, &amp; 1\le x+y+z &lt;5\\-6x + 3y + 2z, &amp; x+y+z &gt; 1\end{cases}\]</span></p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs latex">=<br><span class="hljs-keyword">\begin</span>&#123;cases&#125;<br>3x + 5y +  z, <span class="hljs-built_in">&amp;</span> x+y+z &lt;1<span class="hljs-keyword">\\</span><br>7x - 2y + 4z, <span class="hljs-built_in">&amp;</span> 1<span class="hljs-keyword">\le</span> x+y+z &lt;5<span class="hljs-keyword">\\</span><br>-6x + 3y + 2z, <span class="hljs-built_in">&amp;</span> x+y+z &gt; 1 <br><span class="hljs-keyword">\end</span>&#123;cases&#125;<br></code></pre></td></tr></table></figure><ol start="4" type="1"><li>数组</li></ol><p><span class="math display">\[\begin{array}{|c|c||c|} a &amp; b &amp; S \\\hline0&amp;0&amp;1\\0&amp;1&amp;1\\1&amp;0&amp;1\\1&amp;1&amp;0\\\end{array}\]</span></p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs latex"><span class="hljs-keyword">\begin</span>&#123;array&#125;&#123;|c|c||c|&#125; a <span class="hljs-built_in">&amp;</span> b <span class="hljs-built_in">&amp;</span> S <span class="hljs-keyword">\\</span><br><span class="hljs-keyword">\hline</span><br>0<span class="hljs-built_in">&amp;</span>0<span class="hljs-built_in">&amp;</span>1<span class="hljs-keyword">\\</span><br>0<span class="hljs-built_in">&amp;</span>1<span class="hljs-built_in">&amp;</span>1<span class="hljs-keyword">\\</span><br>1<span class="hljs-built_in">&amp;</span>0<span class="hljs-built_in">&amp;</span>1<span class="hljs-keyword">\\</span><br>1<span class="hljs-built_in">&amp;</span>1<span class="hljs-built_in">&amp;</span>0<span class="hljs-keyword">\\</span><br><span class="hljs-keyword">\end</span>&#123;array&#125;<br></code></pre></td></tr></table></figure><p>(未完待续，动态补充)</p><h2 id="参考资料">参考资料</h2><ol type="1"><li>https://www.cnblogs.com/yalphait/articles/8685586.html</li><li>https://zhuanlan.zhihu.com/p/266267223</li><li>https://blog.csdn.net/u012684062/article/details/78398191</li></ol>]]></content>
    
    
    <categories>
      
      <category>LaTex学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LaTex</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文十问:Map-view Transformer(CVPR2022)</title>
    <link href="/posts/7e4de446/"/>
    <url>/posts/7e4de446/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script><h1id="cross-view-transformers-for-real-time-map-view-semantic-segmentationcvpr-20222">Cross-viewTransformers for real-time Map-view Semantic Segmentation(CVPR2022)[2]</h1><blockquote><p>论文标题：Cross-view Transformers for real-time Map-view SemanticSegmentation(CVPR 2022)<br> 论文地址：<ahref="https://arxiv.org/abs/2205.02833">https://arxiv.org/abs/2205.02833</a><br>作者单位：The Chinese University of Hong Kong<br> 代码地址：<ahref="https://github.com/bradyz/cross_view_transformers">https://github.com/bradyz/cross_view_transformers</a><br>一句话读论文：Our architecture implicitly learns a mapping fromindividual camera views into a canonical map-view representation using acamera-aware cross-view attention mechanism.</p></blockquote><h2 id="q1">Q1</h2><p>论文试图解决什么问题？</p><p>做图像特征与地图特征的融合("model geometry and relationships betweendifferent view and a canonical map representation")</p><h2 id="q2">Q2</h2><p>这是否是一个新的问题？</p><p>利用地图信息作为query，参与语义分割网络的跨视图融合，是一个有新意的做法</p><h2 id="q3">Q3</h2><p>这篇文章要验证一个什么科学假设？</p><h2 id="q4">Q4</h2><p>有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？</p><p>做俯视图语义分割，已有方法大致可归为如下两类(包含其存在的问题)Image-based depth estimation are error-prone. Depth-based projectionsare a fairly inflexible and rigid bottleneck to map between views.</p><p>附一份知乎笔记连接：https://zhuanlan.zhihu.com/p/511477453</p><h2 id="q5">Q5</h2><p>论文中提到的解决方案之关键是什么？</p><p>通过cross-view transoformer来做Camera View到MapView的融合。相比于已有方法基于显式地几何关系地映射，这种融合的方式是一种隐式函数的映射("learnany geometric transformation implicitly and directly fromdata")。此外，transformer需要positionalembedding来区分不同空间位置的特征。本文因此设计了camera-aware和map-view两类positionalembedding。</p><h2 id="q6">Q6</h2><p>论文中的实验是如何设计的？</p><h2 id="q7">Q7</h2><p>用于定量评估的数据集是什么？代码有没有开源？</p><p>nuScenes, 已经开源</p><h2 id="q8">Q8</h2><p>论文中的实验及结果有没有很好地支持需要验证的科学假设？</p><p>在俯视图的语义分割中是SOTA(37.5% mIoU),和基于深度估计与投影等已有方法相比comaprable。但是整个赛道与图像语义分割与3D语义分割结果相比(70-80mIoU)，整体后面还有挖掘空间</p><h2 id="q9">Q9</h2><p>这篇论文到底有什么贡献？</p><p>1）利用了地图信息，这是一个比较有新意的setting.2）注意力系数计算方式比较有新意，可以参考拓展</p><h2 id="q10">Q10</h2><p>下一步呢？有什么工作可以继续深入？</p>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
      <category>点云</category>
      
      <category>多模态融合</category>
      
      <category>2022</category>
      
    </categories>
    
    
    <tags>
      
      <tag>一句话读论文系列</tag>
      
      <tag>CVPR2022</tag>
      
      <tag>点云语义分割</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>一句话读论文:Map-view Transformer(CVPR2022)</title>
    <link href="/posts/17ecbaa3/"/>
    <url>/posts/17ecbaa3/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script><h1id="cross-view-transformers-for-real-time-map-view-semantic-segmentationcvpr-20221">Cross-viewTransformers for real-time Map-view Semantic Segmentation(CVPR2022)[1]</h1><blockquote><p>论文标题：Cross-view Transformers for real-time Map-view SemanticSegmentation(CVPR 2022)<br> 论文地址：<ahref="https://arxiv.org/abs/2205.02833">https://arxiv.org/abs/2205.02833</a><br>作者单位：The Chinese University of Hong Kong<br> 代码地址：<ahref="https://github.com/bradyz/%20cross_view_transformers">https://github.com/bradyz/cross_view_transformers<br></a> 一句话读论文：Our architectureimplicitly learns a mapping from individual camera views into acanonical map-view representation using a camera-aware cross-viewattention mechanism.</p></blockquote><h2 id="网络框架">网络框架：</h2><figure><img src="https://s2.loli.net/2022/05/24/okSpGdcwzAjVeiu.png"alt="image-20220523111833587" /><figcaption aria-hidden="true">image-20220523111833587</figcaption></figure><h2 id="核心内容"><strong>核心内容：</strong></h2><p>​ Motivation: 想做图像特征与地图特征的融合("model geometry andrelationships between different view and a canonical maprepresentation")</p><p>​ 已有方法的问题：</p><ul><li>Image-based depth estimation are error-prone.</li><li>Depth-based projections are a fairly inflexible and rigid bottleneckto map between views.</li></ul><p>​ 本文的思路：</p><p>​ 是通过cross-view transoformer来做Map View 到CameraView的融合。相比于已有方法基于显式地几何关系地映射，这种融合的方式是一种隐式函数的映射("learnany geometric transformation implicitly and directly fromdata")。此外，transformer需要positionalembedding来区分不同空间位置的特征。</p><p>​ 注意力系数的计算方式比较巧妙： <span class="math display">\[x^{(I)} \simeq K_k R_k(x^{(W)} - t_k)\]</span> ​ 上述公式展示了世界坐标系<spanclass="math inline">\(x^{(W)}\)</span>与相机坐标系<spanclass="math inline">\(x^{(I)}\)</span>的映射关系，其中<spanclass="math inline">\(t_k\)</span>是车辆在行驶时的平移，<spanclass="math inline">\(K_k\)</span>与<spanclass="math inline">\(R_k\)</span>分别是相机内参矩阵和外参旋转矩阵(相机位姿相对于LiDAR传感器位姿)。<span class="math display">\[sim_k(x^{(I)}, x^{(W)}) = \frac{(R_K^{-1}K_K^{-1}x^{(I)})\cdot (x^{(W)}- t_k)}{\Vert (R_K^{-1}K_K^{-1}x^{(I)})\Vert \Vert (x^{(W)} - t_k)\Vert}\]</span> ​这样将3D坐标点和2D图像对应的坐标联系起来，计算其相似度系数。由于本文中没有3DLiDAR点云数据参与，所以从地图中只能获得xy坐标的信息，无法获得高度(深度)信息。所以这里用的是经过MLP后提取点特征代替了直接使用坐标。</p><blockquote><p>几何意义：The uprojected image coordinate $d_{k,i} = R_k^{-1} K_k^{-1}x_i^{(I)} $ for each image coordinate <spanclass="math inline">\(x_i^{(I)}\)</span> described a direction vectorfrom the origin <span class="math inline">\(t_k\)</span> of camera <spanclass="math inline">\(k\)</span> to the image plane at depth 1.</p><p>代码实现：1)We encode this direction vector <spanclass="math inline">\(d_{k,i}\)</span> using an MLP(shared across kviews) into a D-dimensional positional embedding <spanclass="math inline">\(\delta_{k,i} \in \mathbb{R}^D\)</span>... Wecombine this positional embedding with image features <spanclass="math inline">\(\phi_{k,i}\)</span> in the keys of our cross-viewattention mechanism.</p><p>2)<span class="math inline">\(x^{W}\)</span>从地图获得，没有高度信息怎么办：We start with a learned positionalencoding <span class="math inline">\(c^{(0)}\in \mathbb{R}^{w \times h\times D}\)</span>. We build the map-view representation up overmultiple iterations in our transformer... Each positional embedding isbetter able to project the map-view coordinates into a proxy of the 3Denvironment.</p></blockquote><p>基于上述几何映射的注意力系数计算方式，论文做如下改动作为实际的计算方法：</p><p><span class="math display">\[sim(\delta_{k,i}, \phi_{k,i}, c_j^{n}, \tau_k) = \frac{(\delta_{k,i}+\phi_{k,i})\cdot(c_j^{(n)} - \tau_k)}{\Vert \delta_{k,i}+ \phi_{k,i}\Vert \Vert c_j^{(n)} - \tau_k \Vert }\]</span></p><p>​ <spanclass="math inline">\(\delta_{k,j}\)</span>表示图像的Camera-viewpositional embedding; <span class="math inline">\(\phi_{k,i} =MLP[d_{k,i}]=MLP[(R_K^{-1}K_K^{-1}x^{(I)})]\)</span>表示经过MLP映射后的<spanclass="math inline">\(D\)</span>维向量。相比于原始的计算方式，这里把positionalembedding和图像feature加在一起参与运算。<spanclass="math inline">\(c_j^{(n)}\)</span>和<spanclass="math inline">\(\tau_k\)</span>意义和原始公式相同，不同之处是他们也是经过Transforemr和MLP映射到<spanclass="math inline">\(D\)</span>维向量。</p><h2 id="实验结果"><strong>实验结果：</strong></h2><figure><img src="https://s2.loli.net/2022/05/23/sk2fGHMORJ8UjWX.png"alt="image-20220523135309538" /><figcaption aria-hidden="true">image-20220523135309538</figcaption></figure><figure><img src="https://s2.loli.net/2022/05/23/wdDPYJWxlhzFv8s.png"alt="image-20220523135440759" /><figcaption aria-hidden="true">image-20220523135440759</figcaption></figure><figure><img src="https://s2.loli.net/2022/05/23/Y84bdTZ75nNxlJ2.png"alt="image-20220523135515717" /><figcaption aria-hidden="true">image-20220523135515717</figcaption></figure><figure><img src="https://s2.loli.net/2022/05/23/z4uAlfWCjPGHJ7v.png"alt="image-20220523140115644" /><figcaption aria-hidden="true">image-20220523140115644</figcaption></figure><h2 id="related-work可选"><strong>Related Work(可选):</strong></h2><ul><li></li></ul><h2id="你认为优点不足可以拓展改进的地方可选"><strong>你认为优点/不足/可以拓展改进的地方(可选):</strong></h2><p>优点：</p><ul><li>利用了地图信息，这是一个比较有新意的setting.</li><li>提出的transformer不是简单的高维特征计算cos复杂度。从理论上来说，本文是巧妙利用三维点与二维相机视角存在的仿射变换的关系构建的计算方法。原始公式中只需要坐标信息，然后在此基础上拓展了图像和地图的positionalembedding以及各自的特征信息(如transformer得到的特征向量，图像颜色，点云的极坐标信息等)，可拓展性强。</li></ul><p>不足：</p><ul><li>本文中没有用到LiDAR点云数据。讲道理从LiDAR数据中可以直接获得准确的高度信息，为什么不用呢(是为了提高模型推理速度，避免使用点云数据?)。这样的话从地图中经过多层transformer来估计特征向量的方式总觉得有瑕疵。</li><li>实验评测在语义分割上做的，但是做的是俯视图语义分割而不是3D语义分割。实际应用中3D语义分割和2D图像语义分割比俯视图语义分割实用很多。俯视图语义分割的重要性有待商榷。此外实验结果虽然和传统方法comparable证明这种利用transformer的新技术路线是有用的，但是30多的mIoU和现在3D语义分割中78的mIoU相比仍然逊色很多。</li></ul><p>可以拓展改进的地方：</p><ul><li>后面利用这套同样的idea，去做一做3D语义分割或2D图像分割。这样能挖掘一个更有意思的课题，我从地图中学习到的信息对CameraView和3D点云分割能起到什么帮助？</li></ul>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
      <category>点云</category>
      
      <category>多模态融合</category>
      
      <category>2022</category>
      
    </categories>
    
    
    <tags>
      
      <tag>一句话读论文系列</tag>
      
      <tag>CVPR2022</tag>
      
      <tag>点云语义分割</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文十问:DeepFusion(CVPR2022)</title>
    <link href="/posts/1cc46c4f/"/>
    <url>/posts/1cc46c4f/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script><h1 id="deepfusioncvpr-2022">DeepFusion(CVPR 2022)</h1><blockquote><p>论文标题：DeepFusion: Lidar-Camera Deep Fusion for Multi-Modal 3DObject Detection<br> 论文地址：<ahref="https://arxiv.org/abs/2203.08195">https://arxiv.org/abs/2203.08195</a><br>作者单位：Johns Hopkins University, Google<br> 代码地址：<ahref="https://github.com/tensorflow/lingvo">https://github.com/tensorflow/lingvo</a><br>一句话读论文：This study points out the key role of transformed featurealignment process that plays in multi-modal fusion module, and thusproposes InverseAug and LearnableAlign module to overcome incosistencybetween data augmentation and multi-modal fusion.</p></blockquote><h2 id="q1">Q1</h2><p>论文试图解决什么问题？</p><p>本工作试图解决RGB图像-LiDAR点云网络，由于图像和点云分别进行数据增强操作(如随机旋转)，而导致模态特征之间的对应关系被破坏，使得多模态学习带来的增益被削弱的问题。</p><h2 id="q2">Q2</h2><p>这是否是一个新的问题？</p><p>这是一个比较有意思的，也很实用的新问题1.因为大规模LiDAR点云数据需要大量的数据增强操作利于刷榜，如果多模态与之有冲突，那么提点效果会被大幅降低。2.2020年前的多模态融合多是RGB图像-点云伪图像(环形投影，BeV,透视投影等)的融合，本质是2D-2D网络间的融合。而2021年开始有更多3D-2D网络的多模态模型，因此在这个背景下考虑数据增强和特征对齐的冲突问题，还是比较有意思的</p><h2 id="q3">Q3</h2><p>这篇文章要验证一个什么科学假设？</p><p>1.特征对齐对多模态发挥效果很重要(参考Table1,9) -&gt; 提出InverseAug\2. 深层网络的特征向量在不同模态间对齐，对模型学习语义信息有利 -&gt;提出基于注意力机制的融合方式</p><h2 id="q4">Q4</h2><p>有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？</p><p>\1. Input-Level Decoration:在原始数据层面,利用图形学中地仿射变换与投影，将LiDAR点与Pixel对应。相关工作有：PointPainting(CVPR 2021), PointAugmentation(CVPR 2021), PMF(ICCV2021)</p><p>\2. Mid-Level Fusion:在网络结构层面，对每一层的特征向量隐式融合。相关工作有： Deep ContinuousFusion(ECCV 2018) EP-Net(CVPR 2018), 4D-Net(ICCV 2021), Cross viewTransformer for real-time Map-view Semantic Segmentation(CVPR 2022)TransFusion(CVPR 2022)等</p><h2 id="q5">Q5</h2><p>论文中提到的解决方案之关键是什么？</p><p>\1. InverseAug:把图像和点云分支的数据增强操作取逆，再用仿射变换投影。</p><p>\2. LearnableAlign: 比较常见的Attention的融合方式</p><h2 id="q6">Q6</h2><p>论文中的实验是如何设计的？</p><p>略………………</p><h2 id="q7">Q7</h2><p>用于定量评估的数据集是什么？代码有没有开源？</p><p>Waymo 。开源。</p><h2 id="q8">Q8</h2><p>论文中的实验及结果有没有很好地支持需要验证的科学假设？</p><p>有，比较好。 1.实验效果比较solid:在各大主流目标检测方法上，提升了6-8%左右(LEVEL 2)。2.可视化结果也很好地验证科学猜想。</p><h2 id="q9">Q9</h2><p>这篇论文到底有什么贡献？</p><p>同时考虑多模态融合与数据增强之间的协同作用，并且从原始数据和网络结构两个层面，提出简单有效的方法，较好地解决冲突并验证猜想。Motivation有新意，实验效果比较solid, 通用价值大。</p><h2 id="q10">Q10</h2><p>下一步呢？有什么工作可以继续深入？</p><p>\1. 对于不可逆的数据增强操作，比如用于图像的RandomCrop,RandomErasing,必然造成图像与点云在几何层面无法对应。原文中只能忽略这些增强操作，或者等对齐模块后单独加入不可逆的数据增强操作。</p><p>\2.如实例分割、全景分割任务中，有一些对于实例的数据增强操作，如从其他场景中随机复制一些实例点到另外的场景，那么图像中是否需要对应生成一些假的物体pixel呢？若存在遮挡关系又如何处理？</p><p>\3. 方法相对简单一些，后续工作或许能继续深入挖掘。</p>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
      <category>点云</category>
      
      <category>多模态融合</category>
      
      <category>2022</category>
      
    </categories>
    
    
    <tags>
      
      <tag>CVPR2022</tag>
      
      <tag>论文十问系列</tag>
      
      <tag>多模态，点云目标检测</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>一句话读论文:Panoptic-PHNet(CVPR2022)</title>
    <link href="/posts/9cb0ed26/"/>
    <url>/posts/9cb0ed26/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script><h1 id="pnfcvpr-2022">PNF(CVPR 2022)</h1><blockquote><p>论文标题：Panoptic Neural Fields: A Semantic Object-Aware NeuralScene Representation(CVPR 2022)<br> 论文地址：<ahref="https://arxiv.org/abs/2205.04334">https://arxiv.org/abs/2205.04334</a><br>作者单位：Google Research, Georgia Tech, Simon Fraser University,Stanford University<br> 代码地址：暂无<br> 一句话读论文："We presentPanoptic Neural Fields(PMF), and object-aware neural scenerepresentation that decomposes a scene into a set of objects(things) andbackground(stuff)."</p></blockquote><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="520" height="86" src="///music.163.com/outchain/player?type=2&amp;id=22707001&amp;auto=0&amp;height=66"></iframe><h2 id="网络框架"><strong>网络框架：</strong></h2><figure><img src="https://s2.loli.net/2022/05/23/AiBE8Xg3cu5ql1w.png"alt="image-20220523191212464" /><figcaption aria-hidden="true">image-20220523191212464</figcaption></figure><center style="color:#C0C0C0;text-decoration:underline">图1 Overview of Panoptic Neural Field</center><figure><img src="https://s2.loli.net/2022/05/23/3rnZVSYfXNpMeib.png"alt="image-20220523191253241" /><figcaption aria-hidden="true">image-20220523191253241</figcaption></figure><center style="color:#C0C0C0;text-decoration:underline">图2 Dynamic Challenging 3D Scenes Description</center><h2 id="核心内容"><strong>核心内容：</strong></h2><p>大佬们请收下我的膝盖！！！</p><p>Motivation:</p><ul><li><p>把多用于室内场景的Nerf首次应用到室外自动驾驶场景中</p></li><li><p>Nerf原先多用于View Synthesis，图形学渲染，重建。本篇工作窥得大佬们的野心：想在室外自动驾驶场景中，把分类、语义分割、目标检测、目标追踪、全景分割、三维重建、深度估计、场景编辑与生成等一系列任务全部做到SOTA指标，从而让Nerf一统2D-3D视觉任务的天下。虽然本篇工作是初步的尝试，但是开辟了一个新的研究领域。</p></li><li><p>相比于之前在Nerf基础上各种incremental类型的工作，本篇工作提出一种室外场景通用类型的Nerf框架。主要分为stuff类别和thing类别，除了分别学习传统Nerf模型所需要的color,pose，density等信息，还加入语义信息，最后将stuff类别与thing类别共同合成panopticradiance field，用于各类下游任务。</p></li><li><p>在已有的语义分支+Nerf,Dynamics+Nerf等各种变体基础上，取消了共享的MLP网络，而是为每一种类别的物体instance设计小的MLP网络；此外在初始化上引入类别的先验信息，设计了category-specificmeta-learned initialization</p><p>本文的方法：在原版Nerf基础上，做如下变化</p></li><li><p>Things类别：</p><ul><li>首先用RGB-only 3D Object Detector&amp; Tracker 得到Bounding boxtrack <spanclass="math inline">\(T_k\)</span>(由一系列仿射变换矩阵组成)和语义类别<spanclass="math inline">\(k\)</span>.</li><li>对每个物体实例，用标准的Nerf网络提取特征，该网络是由time-invariantMLP组成(不是随时间变化而变化的RNN时序网络),得到包括color, pose,density等参数信息</li><li>损失函数共同优化Nerf网络和<spanclass="math inline">\(T_k\)</span></li></ul></li><li><p>Stuff 类别：</p><ul><li>用单一的Nerf网络提取Stuff类别，此外还有网络分支学习每个Stuffpixel的语义类别</li></ul></li><li><p>Panoptic-Radiance Field</p><ul><li><p>对color,densiy等通道采取如下融合方式</p></li><li><p><span class="math display">\[  c(x| \theta) = \mathbb{1}s(x)c_x(x|\theta) + \sum_kc_k(T^{-1}x|\theta)  \]</span></p></li></ul></li><li><p>Render Panoptic-Radiance Fields</p><ul><li><span class="math display">\[  C(r|\theta) \sim \sum_{i=1}^Nw(t_i)f(\mathbf{r}(t_i)|\theta)  \]</span></li></ul></li><li><p>Nerf中权重先验的获取</p><ul><li>Bias initalization(设置stuff MLP的bias为-5，thingMLP的bias为0.1，因为真实室外场景中stuff volume大多数是空的，而thingvolume大多数非空) 和 Meta-learn的方式(<code>FedAvg</code> 算法)</li></ul></li></ul><h2 id="贡献点创新性"><strong>贡献点/创新性：</strong></h2><ul><li>见Motivation的第1-3条</li></ul><h2 id="实验结果"><strong>实验结果：</strong></h2><figure><img src="https://s2.loli.net/2022/05/23/xYOCDHST3kIguh9.png"alt="image-20220523194617216" /><figcaption aria-hidden="true">image-20220523194617216</figcaption></figure><center style="color:#C0C0C0;text-decoration:underline">图5 实验结果1</center><figure><img src="https://s2.loli.net/2022/05/23/sowYT8tfqBILU1b.png"alt="image-20220523194641296" /><figcaption aria-hidden="true">image-20220523194641296</figcaption></figure><center style="color:#C0C0C0;text-decoration:underline">图6 实验结果2</center><figure><img src="https://s2.loli.net/2022/05/23/gkLWepDuo41ZQmO.png"alt="image-20220523194752533" /><figcaption aria-hidden="true">image-20220523194752533</figcaption></figure><center style="color:#C0C0C0;text-decoration:underline">图7 实验结果3</center><figure><img src="https://s2.loli.net/2022/05/23/xG91WXNegAr2Pj7.png"alt="image-20220523194819545" /><figcaption aria-hidden="true">image-20220523194819545</figcaption></figure><center style="color:#C0C0C0;text-decoration:underline">图8 实验结果4</center><figure><img src="https://s2.loli.net/2022/05/23/ZLzw3K8InV62dcv.png"alt="image-20220523194847881" /><figcaption aria-hidden="true">image-20220523194847881</figcaption></figure><center style="color:#C0C0C0;text-decoration:underline">图9 实验结果5</center><h2 id="related-work可选-后续再补充"><strong>Related Work(可选,后续再补充):</strong></h2><ul><li>Nerfs</li><li>Nerfs with Semantics</li><li>Nerfs with dynamics</li><li>Nerfs with object decompositions</li><li>Conditional NeRFs</li><li>MVS</li><li>SLAM</li></ul><h2id="你认为优点不足可以拓展改进的地方可选"><strong>你认为优点/不足/可以拓展改进的地方(可选):</strong></h2><p>优点：</p><ul><li>太多了吐槽不完</li></ul><p>缺点：</p><ul><li>虽然很大一统，但是整个框架挺复杂，目前只能在离线的训练和推理，不太容易直接应用在实时场景下。</li></ul><h2 id="其他笔记"><strong>其他笔记:</strong></h2><ul><li>CVer 计算机视觉：https://zhuanlan.zhihu.com/p/513499887</li></ul>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
      <category>点云</category>
      
      <category>全景分割</category>
      
      <category>2022</category>
      
    </categories>
    
    
    <tags>
      
      <tag>一句话读论文系列</tag>
      
      <tag>CVPR2022</tag>
      
      <tag>点云全景分割</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>课程笔记-矩阵理论4-4</title>
    <link href="/posts/760e44bd/"/>
    <url>/posts/760e44bd/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>课程笔记：矩阵理论3-4——盖尔圆定理与特征值估计</title>
    <link href="/posts/c6e1c86a/"/>
    <url>/posts/c6e1c86a/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script><h1id="课程笔记矩阵理论3-4盖尔圆定理与特征值估计">课程笔记：矩阵理论3-4——盖尔圆定理与特征值估计</h1><h2 id="矩阵的盖尔圆盘">矩阵的盖尔圆盘</h2><p>$ N M $阶矩阵，每一行可以定义一个盖尔圆盘，每一列也可以定义一个盖尔圆盘。这里先讨论行的情况(关于行的盖尔圆盘)。<span class="math display">\[D_i(A) = \{ \mathbf{x \in \mathbb{C}: |x - a_{ii}| \le \sum_{i \nej}|a_{ij}|} \}\]</span>圆的半径是除对角元素外所有元素的<strong>模</strong>的和。圆心是对角线元素(由于是复数，在复平面上有实轴和虚轴坐标)。</p><p>这些圆盘区域的并能够得到盖尔区域。</p><h2 id="盖尔圆盘定理">盖尔圆盘定理</h2><p>盖尔圆盘定理: 设 <span class="math inline">\(A\)</span>是n阶矩阵，矩阵的所有特征值一定会落在某个圆盘里。 <spanclass="math display">\[|\lambda_i - a_{ii}| \le \sum_{i \ne j}^n |a_{ij}|, \exist i \in\{1,2,\dots, n \}\]</span></p><p>Proof: <span class="math display">\[\begin{align*}&amp;\mathbf{A}x = \lambda x \\\Leftrightarrow &amp;(\mathbf{A} - \lambda I)x = 0 \\\Leftrightarrow &amp;\sum_{i \ne j} a_{ij}x_j + (a_{ii} - \lambda)x_i =0 \, , i=1,2,\dots,n\\\Leftrightarrow&amp; |\lambda - a_{ii}||x_i| = |\sum_{i \ne j}a_{ij}x_j|\le \sum_{i \ne j}|a_{ij}||x_j| \\\Leftrightarrow &amp; |\lambda - a_{ii}| \le \sum_{i \ne j}|a_{ij}|\frac{|x_j|}{|x_{max}|} \le \sum_{i \ne j}|a_{ij}| \\\end{align*}\]</span> 从第四行，挑选特征向量最大的分量 <spanclass="math inline">\(x_{max}\)</span> ，并把该分量index作为选取的矩阵的行。也就是说特征值满足的不等式，是在特征向量中分量最大的位置对应的行。</p><Details><summary>例3.4.4</summary><img src="https://i.loli.net/2021/11/19/7Ms9ftkrDhiBCzU.png" alt="image-20211119213910167" style="zoom:80%;" /></details><div class="note note-warning">            <p>注意：有的圆盘可能有多个特征值，有的圆盘可能没有特征值。如果需要精细的描述，使用精细圆盘定理，一个联通区域有几块圆盘，那么该区域必须<strong>恰好</strong>有相同个数的特征值</p>          </div><details><summary>例3.4.5</summary><img src="https://i.loli.net/2021/11/19/3CJzTgvXesfNMFQ.png" alt="image-20211119221047116" style="zoom:50%;" /></details><p>Note :圆盘定理不能保证每个圆盘一定有特征值。但是下面精细圆盘定理更详细的说明了特征值分布特点：K个圆盘的并集联通区域一定恰好有K个特征值。</p><h2 id="精细圆盘定理">精细圆盘定理</h2><p>精细圆盘定理：设 <span class="math inline">\(C\)</span>是盖尔区域的一个由<spanclass="math inline">\(k\)</span>个圆盘组成的连通分量，则<spanclass="math inline">\(C\)</span>恰好有<spanclass="math inline">\(k\)</span>个特征值。</p><p>证明思路: 设矩阵<span class="math display">\[\mathbf{A} = \mathbf{D}+ \epsilon \mathbf{B}\]</span>, 其中 <spanclass="math inline">\(\mathbf{D}\)</span> 是对角阵，<spanclass="math inline">\(\mathbf{B}\)</span> 是对角线为0的剩余部分。<spanclass="math inline">\(\epsilon \in [0,1]\)</span> ,这样矩阵 <spanclass="math inline">\(\mathbf{A}\)</span>从对角阵连续变化到原矩阵。对角阵的圆盘半径为0，圆心就是特征值，因此每个圆盘恰好有1个特征值，圆盘外面有<span class="math inline">\(n-1\)</span> 个特征值。在 <spanclass="math inline">\(\epsilon\)</span>连续变化增长到1的过程中，圆盘半径逐渐变大，并且部分圆开始相交，<spanclass="math inline">\(k\)</span> 个圆盘相交，<spanclass="math inline">\(k\)</span> 个圆盘区域外有 <spanclass="math inline">\(n-k\)</span>个特征值。由于矩阵的特征多项式的根是其系数的连续函数，随意特征值也是从圆心出发连续游走的，不会出现跳跃的情况，且增长过程中一直被圆盘包着，所以保证了联通的k个盖尔圆一定恰好有<span class="math inline">\(k\)</span> 个特征值。</p><h2 id="调节圆盘大小的技巧">调节圆盘大小的技巧</h2><p>如何调整矩阵盖尔圆的半径，从而将特征值分离出来(比喻：就好像化学试剂提纯，分离似的)</p><p>原理如下： <span class="math display">\[\begin{align*}\mathbf{D}^{-1}\mathbf{A}\mathbf{D} &amp;= \text{Diag}(\frac{1}{d_1},\frac{1}{d_2}, \dots, \frac{1}{d_n}) \mathbf{A} \text{Diag}(d_1, d_2,\dots, d_n) \\&amp;= (\frac{d_j}{d_i}a_{ij})\end{align*}\]</span></p><p>首先看出对角线元素是不受影响的，非对角先元素会被放缩，放缩系数由该元素的行与列(对应的是其他行)的系数决定</p><p>某一行的放缩系数 <span class="math inline">\(d_i\)</span>和其余行系数 <span class="math inline">\(d_k\)</span> 的关系如下：</p><p>若 <span class="math inline">\(d_i &gt; d_k\)</span>，为了简便起见，我们设除了第 <span class="math inline">\(i\)</span>行的其他行均为1。第 <span class="math inline">\(i\)</span>行的非对角元素放缩系数分母比分子大，故圆盘被缩小；其他行的第 <spanclass="math inline">\(i\)</span> 列元素分子比分母大，会被放大，除第<span class="math inline">\(i\)</span>列外的其他非对角元素不变，总体圆盘会被放大。</p><p>若 <span class="math inline">\(d_i &lt;d_k\)</span>，类似的，设置除了第 <span class="math inline">\(i\)</span>行的其他行均为1。只有第 <span class="math inline">\(i\)</span>行的圆盘会被放大，其余圆盘均缩小。</p><h2 id="利用圆盘定理估计谱半径">利用圆盘定理估计谱半径</h2><p>矩阵的谱半径 <span class="math inline">\(\le \underset{i}{\max}\sum_{j} a_{ij}\)</span> , 并记录 <span class="math inline">\(v =\underset{i}{\max} \sum_{j} a_{ij}\)</span></p><p>证明思路:因为根据圆盘定理，矩阵的每个特征值都一定落在某个圆盘(比如<spanclass="math display">\[a_{ii}\]</span>)里。 <spanclass="math display">\[|\lambda| = |\lambda - a_{ii} + a_{ii}| \le \sum_{j} a_{ij}\]</span> 同理，矩阵的谱半径小于某个“最大”的列 <spanclass="math inline">\(v’ = \underset{j}{\max} \sum_{i}a_{ij}\)</span></p><h2 id="其他估计">其他估计</h2><ol type="1"><li><p>Ostrwoski 圆盘定理</p></li><li><p>Brauer定理(Cassini卵形)</p></li></ol>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
      <category>矩阵理论</category>
      
    </categories>
    
    
    <tags>
      
      <tag>课程笔记</tag>
      
      <tag>特征值</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>课程笔记-统计学习理论与方法(ELS_Chap7)</title>
    <link href="/posts/3c8c02ec/"/>
    <url>/posts/3c8c02ec/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script><h1id="课程笔记统计学习理论与方法els_chap7">课程笔记：统计学习理论与方法（ELS_Chap7）</h1><p>📚书籍：《Elements of Statistical Learning》Chap 7</p><h2 id="两个概念">两个概念</h2><ol type="1"><li>模型选择(Model Selection):模型由参数控制的，模型选择既包括模型类型选择，也包括参数的控制。后面讲贝叶斯信息准则时(BIC)会讲到。</li><li>模型评估(Model Assessment): 给定一个模型，评估其训练误差(In-sampleError)，测试误差(Extra-sample Error)，泛化误差(GeneralizationError)。评估训练误差与测试误差之间的差距Bound.</li></ol><p>两者都是侧重对模型的泛化误差进行评估，而非模型的训练误差。泛化误差是整个统计学习中最核心的关注问题。</p><h2 id="训练误差与泛化误差">训练误差与泛化误差</h2><p>训练误差： <span class="math display">\[\overline{\text{err}} = \frac{1}{N} \sum_{i=1}^N L(y_i, \hat{f}(x_i))\]</span> 泛化误差： <span class="math display">\[\text{Err}(y_i, \hat{f}(x_i)) = E_{(X,Y) \sim \tau}(L(y_i, \hat{f}(x_i))|\tau)\]</span> 其中损失函数通常是:</p><ul><li>均方误差$L = (y_i - (x_i))^2 $</li><li>对数极大似然函数【用于多分类】(严格来说这不算损失函数，但是极大似然最大等价于加符号最小，所以可以写成相同形式):$L = -2<spanclass="math inline">\(.其中\)</span>I(G=k)$相当于真实标签(GroundTruth).</li><li>对数极大似然的特殊情况【0-1分类】： <span class="math inline">\(L =I(G=1)\log(\hat{\Pr}) + (1-I(G=1))\log(1-\hat{\Pr})\)</span></li></ul><div class="note note-primary">            <p>📢注意，我们算误差时的Ground Truth(GT) <spanclass="math inline">\(y\)</span>并不一定是真实的标签，因为获取数据的过程中无法避免有噪声存在(就比如用精密物理仪器测量的结果总会有无法避免的系统误差)。我们的损失函数是让模型的估计值<spanclass="math inline">\(\hat{f}\)</span>和GT算误差，不是和<spanclass="math inline">\(f(x)\)</span>算误差。</p>          </div><h2 id="bias-variance-分解">Bias-variance 分解</h2><p>假设：<span class="math inline">\(\{(x_i,y_i)\}\)</span>是从某个数据分布中采样得到的某个数据集<spanclass="math inline">\(\tau\)</span>。数据的真实分布是<spanclass="math inline">\(y = f(x) +\epsilon\)</span>，数据的真实标签是<spanclass="math inline">\(f(x)\)</span>，但是我们获取数据集时总会有噪声<spanclass="math inline">\(\epsilon\)</span>干扰。通常假设噪声服从均值为0的某个高斯分布。<spanclass="math inline">\(\epsilon \sim N(0,\sigma_{\epsilon}^2)\)</span></p><p>考察在<spanclass="math inline">\(X=x_0\)</span>一点上的单点泛化误差，其中求期望操作<spanclass="math inline">\(E\)</span>是对所有的数据集<spanclass="math inline">\(\tau\)</span>求的期望(Expectation over all thedataset <spanclass="math inline">\(\tau\)</span>).<a name="t1">也就是说在这里，<spanclass="math inline">\(\hat{f}(x_0)\)</span>也是随机变量，它的随机性来源于可能选取不同的数据集<spanclass="math inline">\(\tau\)</span> ；而<spanclass="math inline">\(f(x_0)\)</span>的随机性只来源于<spanclass="math inline">\(\epsilon\)</span>，二者独立无关联</a>。 <spanclass="math display">\[\begin{align*}E(L(Y, \hat{f}(X))|X=x_0, Y=y_0 )&amp;= (y_0 - f(x_0))\,\,\,\text{(这里采用均方误差分析，其他同理)}\\&amp;= E(f(x_0) - \hat{f}(x_0) + \epsilon)^2 \\&amp;= E(f(x_0) - \hat{f}(x_0))^2 + E(\epsilon)^2 + 2E(f(x_0) -\hat{f}(x_0))E(\epsilon)\,\,\,\text{($\epsilon$ 与 $f(x_0) -\hat{f(x_0)}$ 独立)} \\&amp;=  E(f(x_0) - \hat{f}(x_0))^2 + \sigma_{\epsilon}^2 + 0 \\&amp;= E(f(x_0) - E\hat{f}(x_0) + E\hat{f}(x_0) - \hat{f}(x_0))^2 +\sigma_{\epsilon}^2 \\&amp;= E(f(x_0) - E\hat{f}(x_0))^2 + E(\hat{f}(x_0) - \hat{f}(x_0))^2 +2E(f(x_0) - E\hat{f}(x_0))\cdot 0 + \sigma_{\epsilon}^2\\&amp;= E(f(x_0) - E\hat{f}(x_0))^2 + E(\hat{f}(x_0) - \hat{f}(x_0))^2  +\sigma_{\epsilon}^2 \\&amp;= \text{bias}^2 + \text{variance} + \sigma_{\epsilon}^2\end{align*}\]</span></p><div class="note note-info">            <p>关于期望与方差的常用性质，请参见这篇<ahref="https://www.oier99.cn/2021/11/10/随笔-期望与方差的性质/">博客</a>.</p>          </div><h3 id="knn的例子">KNN的例子</h3><p>KNN的模型是<span class="math inline">\(\hat{f}(x_0) =\frac{1}{K}\sum_{i=1}^K y_i,\, x_i \in n_K(x_0)\)</span>. <spanclass="math display">\[\begin{align*}\hat{f}(x_0) &amp;= \frac{1}{K}\sum_{i=1}^K y_i \\&amp;= \frac{1}{K} \sum_{i=1}^K(f(x_i) + \epsilon_i ) \\&amp;\approx \frac{1}{K} \sum_{i=1}^K(f(x_0) + \epsilon_i ) \\&amp;= f(x_0) + \frac{1}{K} \sum_{i=1}^K \epsilon_i\end{align*}\]</span> 为了计算方差方便，在理论推导这里做了一个重要近似：因为KNN取的是K个近邻点来插值估计，所以假设认为他们的“本源”<spanclass="math inline">\(f(x_i) \approxf(x_0)\)</span>.这样，KNN的方差可以如下计算: <spanclass="math display">\[\begin{align*}\text{Var}(\hat{f}(x_0)) &amp;= \text{Var} \frac{1}{K} \sum_{i=1}^K\epsilon_i \\&amp;= \frac{1}{K^2} \text{Var}\sum_{i=1}^K \epsilon_i \\&amp;= \frac{1}{K^2} \sum_{i=1}^K \text{Var}(\epsilon_i) \\&amp;= \frac{1}{K^2} \sum_{i=1}^K \sigma_{\epsilon}^2 \\&amp;= \frac{1}{K} \sigma_{\epsilon}^2\end{align*}\]</span> 故泛化误差拆分为: <span class="math display">\[Err(y_0, \hat{f}(x_0)) \approx \sigma_{\epsilon}^2 + E(f(x_0) -\frac{1}{K}\sum_{i=1}^K y_i )^2 + \frac{1}{K} \sigma_{\epsilon}^2\]</span> 可以看出<spanclass="math inline">\(K\)</span>越小，模型复杂度越大，方差越大，bias应该会越小。</p><h3 id="线性回归的例子">线性回归的例子</h3><p>对于线性回归函数 $(x) = ^T x $ ,解最小二乘误差下的最佳近似参数是<span class="math inline">\(\hat{\beta}= (X^TX)^{-1}X^Ty\)</span></p><p>故<span class="math inline">\(\hat{f}(x) = y^T X(X^TX)^{-1}x = x^TX(X^TX)^-1X^Ty = h(x)y\)</span>.可以看出<spanclass="math inline">\(\hat{f}(x)\)</span>的随机性来源于y，即来源于<spanclass="math inline">\(\epsilon\)</span> .（这与<ahref="#t1">刚才</a>说法不矛盾。因为之前对<spanclass="math inline">\(\hat{f}\)</span>的分析是抽象的符号，而这里是对线性回归具体公式分析）<span class="math display">\[\text{var}(\hat{f}(x_0)) = ||h(x_0)||^2 \sigma_{\epsilon}^2\]</span> 故 <span class="math display">\[Err(y_0, \hat{f}(x_0)) = \sigma_{\epsilon}^2 + E(f(x_0) -\hat{\beta}^Tx_0 )^2 + ||h(x_0)||^2 \sigma_{\epsilon}^2\]</span></p><p>训练误差: <span class="math display">\[\frac{1}{N}\sum_{i=1}^N Err(y_0, \hat{f}(x_0)) = \sigma_{\epsilon}^2 +\sum_{i=1}^NE(f(x_0) - \hat{\beta}^Tx_0 )^2 +\frac{p}{N}\sigma_{\epsilon}^2\]</span> 可以看出模型复杂度由数据量<spanclass="math inline">\(N\)</span>和参数量<spanclass="math inline">\(p\)</span>共同控制。</p><div class="note note-primary">            <p>（这部分的推导需要用到矩阵的迹的性质）</p><p><span class="math inline">\(\text{tr}(ABC) = \text{tr}(BCA) =\text{tr}(CAB)\)</span></p><p><span class="math inline">\(\sum_{i}(x_i y_i) = x^Ty = \text{tr}(xy^T)\)</span></p>          </div><h2 id="乐观度">乐观度</h2><p>乐观度(Optimisim)的概念主要是为了比较模型训练误差与泛化误差之间的差距，或者如何用训练误差去估计泛化误差。</p><p>直接计算训练误差和泛化误差的差有困难，因为模型输入<spanclass="math inline">\(x\)</span>都不固定。所以可以采用重采样的技术，原先训练集<spanclass="math inline">\((x,y)\)</span>,是训练时采样得到的标签，记为<spanclass="math inline">\(y\)</span>；重采样是对相同的<spanclass="math inline">\(x\)</span>, 再次采样<spanclass="math inline">\(y^{\text{New}}\)</span>。 <spanclass="math display">\[\text{Err}_{in}(y_i, \hat{f}(x_i)) = \frac{1}{N}\sum_{i=1}^NE_{Y^{\text{new}}}[L(Y_i^{\text{New}}, \hat{f}(x_i))]\]</span></p><p><span class="math display">\[\overline{err} = \frac{1}{N} \sum_{i=1}^N L(y, \hat{f}(x_i))\]</span></p><p>结论 <span class="math display">\[\begin{align*}    w &amp;= E_y(op) \\    &amp;= \frac{2}{N}\text{Cov}(y, \hat{y})\end{align*}\]</span> 证明过程后续再补。</p><p>若<span class="math inline">\(\hat{y}\)</span>是由参数量为<spanclass="math inline">\(d\)</span>线性回归得到的估计，那么 <spanclass="math display">\[\text{Cov}(y, \hat{y}) = d \sigma_{\epsilon}^2\]</span> 可得到 <span class="math display">\[E_y(\text{Err}_{in}) = E_y(\overline{\text{err}}) + 2\cdot\frac{d}{N}\sigma_{\epsilon}^2\]</span></p><h2 id="有效参数量">有效参数量</h2><p><span class="math display">\[\hat{y} = Sy\]</span></p><p>有效参数数量(Effective Number of Parameters): <spanclass="math display">\[\text{df}(S) = \text{tr}(S)\]</span> 对于线性回归，有如下关系： <span class="math display">\[\sum_{i=1} ^N \text{Cov}(y_i, \hat{y}_i) = \text{df}(\hat{y})\cdot  \sigma_{\epsilon}^2\]</span></p><h2 id="贝叶斯信息量bic">贝叶斯信息量（BIC）</h2><p>题外话：前面的部分主要讲述的模型评价(ModelAssesment)部分，贝叶斯信息量主要讲述的是模型选择的部分</p><p>BIC for 极大似然回归 <span class="math display">\[BIC = -2 \cdot loglik + (\log N) \cdot d\]</span> BIC under Gaussian Model <span class="math display">\[BIC = \frac{N}{\sigma_{\epsilon}^2}[\overline{\text{err}}+ (\log N)\cdot\frac{d}{N} \sigma_{\epsilon}^2]\]</span>他的Motivation是源自贝叶斯派的思想(下面手抄草稿，由于过程有点难，整理后还有很多错误比如符号上下标对不上)</p><figure><img src="https://i.loli.net/2021/11/10/9qVUZlvD2MgstSo.jpg" alt="1" /><figcaption aria-hidden="true">1</figcaption></figure><figure><img src="https://i.loli.net/2021/11/10/yYAO547q1Nf39cM.jpg" alt="2" /><figcaption aria-hidden="true">2</figcaption></figure><figure><img src="https://i.loli.net/2021/11/10/uskaSvyOilJT8mU.jpg" alt="3" /><figcaption aria-hidden="true">3</figcaption></figure><h2 id="交叉验证以后有空再补">交叉验证(以后有空再补)</h2>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
      <category>统计学习理论与方法</category>
      
      <category>教材笔记</category>
      
      <category>Elements of Statistical Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>课程笔记</tag>
      
      <tag>教材笔记</tag>
      
      <tag>统计学习理论与方法</tag>
      
      <tag>ELS</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔-期望与方差的性质</title>
    <link href="/posts/f0d58712/"/>
    <url>/posts/f0d58712/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script><h1 id="随笔期望与方差的常用性质">随笔：期望与方差的常用性质</h1><h2 id="数学期望的性质">数学期望的性质</h2><h2 id="方差的性质">方差的性质</h2><ul><li><p>常数的方差为0：</p><p><span class="math inline">\(\text{Var}(C) = 0\)</span></p><p>逆命题<spanclass="math inline">\(\text{Var}(X)=0\)</span>的充要条件是<spanclass="math inline">\(X\)</span>以概率1取常数<spanclass="math inline">\(\Pr{X=C} = 1\)</span></p></li><li><p><span class="math inline">\(\text{Var}(CX) = C^2\text{Var}(X)\)</span></p></li><li><p>若随机变量<spanclass="math inline">\(X,Y\)</span><strong>相互独立</strong>，而且<spanclass="math inline">\(\text{Var}(X),\text{Var}(Y)\)</span>存在, <spanclass="math inline">\(C_1， C_2\)</span>是常数，，则有 <spanclass="math inline">\(\text{Var}(C_1X + C_2Y) = C_1^2 \text{Var}(X) +C_2^2 \text{Var}(Y)\)</span></p></li></ul><h2 id="期望的性质">期望的性质</h2><ul><li>常数的期望是本身： $ E(C) = 0$</li><li><strong>任意</strong>两个随机变量<spanclass="math inline">\(X,Y\)</span>和常数<span class="math inline">\(C_1,C_2\)</span>有 $ E(C_1X + C_2Y) = C_1E(X) + C_2E(Y)$</li><li>若随机变量<spanclass="math inline">\(X,Y\)</span><strong>相互独立</strong>，则有 <spanclass="math inline">\(E(XY) = E(X)E(Y)\)</span></li><li>Note: <span class="math inline">\(E(X^2)\)</span>和<spanclass="math inline">\(E^2(X)\)</span>不是一回事</li></ul>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
      <category>其他</category>
      
    </categories>
    
    
    <tags>
      
      <tag>概率统计</tag>
      
      <tag>期望方差</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔-每日bug系列（2021-11-05）</title>
    <link href="/posts/c0437afa/"/>
    <url>/posts/c0437afa/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script><h1 id="随笔-每日bug系列_2021-11-05">随笔-每日bug系列_2021-11-05</h1><h2 id="bug-1">Bug 1</h2><p>计算矩阵特征值，求行列式<span class="math inline">\(|\lambda I_n -A|\)</span>时，一定要写成 $$ $$的形式，否则展开求解时极容易出错(今天计算行列式看着原矩阵脑补写特征多项式一连写错五次的痛，对自己的算术能力表示深刻怀疑)</p><h2 id="bug-2">Bug 2</h2><p>记住 <span class="math inline">\(A + 2= A +2I_n\)</span>，是加在对角线元素上的，不是每个位置都加2。这个惯性错误犯了好多次了！！</p><h2 id="bug-3">Bug 3</h2><p>相似矩阵有相同特征值，但逆命题不成立(即有相同特征值，包括每个特征值的代数重数也相同的两个矩阵不一定相似)。因为可能有不同的Jordan标准型形式。比如一个矩阵可以对角化，相似于某个对角阵；而另一个矩阵无法对角化，只能相似于某个Jordan标准型，相同特征值个数的Jordan标准型又有很多种，所以导致这两个矩阵无法相似。</p>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
      <category>每日bug系列</category>
      
    </categories>
    
    
    <tags>
      
      <tag>特征值</tag>
      
      <tag>矩阵理论</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>课程专题-矩阵理论之矩阵对角化的条件汇总</title>
    <link href="/posts/7e08a53a/"/>
    <url>/posts/7e08a53a/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script><h1id="课程专题-矩阵理论之矩阵对角化的条件汇总">课程专题-矩阵理论之矩阵对角化的条件汇总</h1><p>题记：整个初等矩阵理论中要考虑最重要的事情，就是如何尽可能将矩阵化简、压缩。不论是化简为最普世的Jordan标准型，还是更为特殊的矩阵能化简为对角矩阵，以至到后续的各种矩阵分解，都是在研究这一问题。</p><h2 id="引入">引入</h2><ol type="1"><li>定义了特征值，特征向量，特征多项式，特征子空间，相似变换等概念(自我回顾一下)。一些比较重要的性质（常识）</li></ol><ul><li><span class="math inline">\(\tr(A) = \sum\lambda_i\)</span>（包括多重次数的特征值），<spanclass="math inline">\(|A|=\Pi \lambda_i\)</span></li><li>相似矩阵的特征值相同；但特征值相同不一定相似，因为两个矩阵特征值对应的特征子空间不一定一样。</li><li>属于不同特征值的特征向量必不可能相同;进一步，属于不同特征值的特征向量必线性无关。</li><li><span class="math inline">\(kA\)</span> , <spanclass="math inline">\(A^m\)</span>, <spanclass="math inline">\(A^{-1}\)</span> (若<spanclass="math inline">\(A\)</span>可逆)特征值分别是 <spanclass="math inline">\(k\lambda\)</span> , <spanclass="math inline">\(\lambda^m\)</span> , <spanclass="math inline">\(\lambda^{-1}\)</span></li><li>实矩阵的特征值/特征向量有可能是复数，因此特征子空间的数域一般定义在复数上</li><li>特征子空间：特征子空间本质上是 <span class="math inline">\((\lambdaI- A)x=0\)</span> 的解空间(除0以外)，若 <spanclass="math inline">\(x_1\)</span> 和 <spanclass="math inline">\(x_2\)</span> 都是 <spanclass="math inline">\(A\)</span> 的属于 <spanclass="math inline">\(\lambda\)</span> 的特征向量，则 <spanclass="math inline">\(k_1x_1+ k_2x_2\)</span> (<spanclass="math inline">\(\ne 0\)</span>)也是属于 <spanclass="math inline">\(\lambda\)</span> 的特征向量。</li></ul><ol start="2" type="1"><li>矩阵理论中为了更便捷地研究(玩耍)，引入了零化多项式和最小多项式的概念。<ul><li></li></ul></li></ol><h2 id="条件">条件</h2><ol type="1"><li><span class="math inline">\(n\)</span>阶矩阵可对角化的充分必要条件：<ul><li>矩阵有<spanclass="math inline">\(n\)</span>个线性无关的特征向量(原理：这<spanclass="math inline">\(n\)</span>个线性无关的特征向量正好构成了相似变换的可逆矩阵)</li><li>每个特征值的几何重数=代数重数(通常几何重数&lt;=代数重数)(原理：从Jordan标准型上直观的来解释一下，代数重数是Jordan标准型对角线上<spanclass="math inline">\(\lambda\)</span>总共出现的次数，几何重数(特征子空间的维数)是特征值为<span class="math inline">\(\lambda\)</span> 的Jordan块 <spanclass="math inline">\(J_k(\lambda)\)</span>的块数，每一块对应一个特征子空间的基向量，块之间阶数不一样对应基向量之间是线性无关的。若代数重数=几何重数，则每个Jordan块大小为1，退化为对角矩阵)？</li></ul></li><li><span class="math inline">\(n\)</span>阶矩阵的最小多项式没有重根<ul><li>零化多项式：特征多项式f()肯定是满足 <spanclass="math inline">\(f(A)=0\)</span></li><li><span class="math inline">\(\lambda\)</span> 是<spanclass="math inline">\(A\)</span>的特征值充要条件：最小多项式 <spanclass="math inline">\(m(\lambda)=0\)</span></li><li>相似矩阵有相同的最小多项式</li><li>分块对角矩阵的最小多项式是各块最小多项式的最小公倍式(对照分块对角阵的特征多项式=各块最小多项式的乘积)</li></ul></li></ol>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
      <category>矩阵理论</category>
      
      <category>课程专题</category>
      
      <category>矩阵理论</category>
      
    </categories>
    
    
    <tags>
      
      <tag>矩阵对角化</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>课程笔记-矩阵理论(1)——线性代数知识回顾(下)</title>
    <link href="/posts/7736990a/"/>
    <url>/posts/7736990a/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script><h1id="课程笔记矩阵理论1线性代数知识回顾下">课程笔记：矩阵理论(1)——线性代数知识回顾(下)</h1><p>📚书籍：《矩阵理论与应用》张跃辉 Chap 1</p><h2 id="相似矩阵">相似矩阵</h2><p><span class="math display">\[\mathbf{A}x = \lambda x\]</span></p><ol type="1"><li><span class="math inline">\(\mathbf{A}\)</span>是方阵(所有有关特征值和特征响亮的讨论是对方阵而言的)</li><li><span class="math inline">\(x \ne 0\)</span></li></ol><p>则称<span class="math inline">\(x\)</span>是属于特征值 <spanclass="math inline">\(\lambda\)</span> 的 <spanclass="math inline">\(\mathbf{A}\)</span> 上的特征向量</p><h3 id="特征多项式">特征多项式</h3><p><span class="math display">\[\begin{align}(\mathbf{A} - \lambda I)x &amp;= 0 \\\end{align}\]</span></p><p>由于<span class="math inline">\(x \ne 0\)</span>,若有满足该等式的<span class="math inline">\(\lambda\)</span>值，则必为降秩矩阵(奇异矩阵)，故记 <span class="math display">\[f(\lambda) = |\mathbf{A} - \lambda \mathbf{I}| = 0\]</span> 成为特征多项式。</p><p><strong>谱半径</strong>：数值(复数模长)最大的特征值 <spanclass="math inline">\(\rho(\mathbf{A}) = \max \{ |\lambda|: \lambda \in\sigma(\mathbf{A})\}\)</span></p><p>从几何上看，所有的特征值都落在以原点为圆心，谱半径 <spanclass="math inline">\(\rho(\mathbf{A})\)</span> 为半径的圆盘内</p><p><strong>矩阵的特征值 $$的特征子空间</strong>：给定 <spanclass="math inline">\(\lambda\)</span>后 <spanclass="math inline">\((\mathbf{A} - \lambda \mathbf{I})x = 0\)</span>这个齐次方程的解空间，记为 <spanclass="math inline">\(V_\lambda\)</span> 。该解空间的维度<spanclass="math inline">\(\dim(V_\lambda) = n - r (\mathbf{A})\)</span>称为特征值 <span class="math inline">\(\lambda\)</span>的<strong>几何重数</strong></p><p><strong>代数重数</strong>：将特征多项式因式分解后， <spanclass="math inline">\(f(\lambda) =|\mathbf{A}- \lambda \mathbf{I}|=\prod_i (\lambda - \lambda_i)^{n_i}=0\)</span>，每个特征值的对应的<spanclass="math inline">\(n_i\)</span></p><p><strong><u><em>任何特征值的几何重数不会超过其代数重数</em></u></strong></p><h3 id="特征值的性质">特征值的性质</h3><ol type="1"><li><span class="math inline">\(|\mathbf{A}| =\prod_i(\lambda_i)^{n_i}\)</span></li><li>$() = _i n_i _i $</li><li><span class="math inline">\(\mathbf{A}\)</span>可逆 <spanclass="math inline">\(\Leftrightarrow\)</span> 0不是其特征值</li><li><span class="math inline">\(\lambda\)</span> 是 <spanclass="math inline">\(\mathbf{A}\)</span> 的特征值，则 <spanclass="math inline">\(f(\lambda)\)</span> 是 <spanclass="math inline">\(f(\mathbf{A})\)</span> 的特征值，特征向量不变</li><li>设 <span class="math inline">\(\mathbf{A}\)</span>可逆，其特征多项式为 <span class="math inline">\(|\mathbf{A}^{-1} -\lambda \mathbf{I}| = \prod_{i} (\lambda -\lambda_i^{-1})^{n_i}\)</span> , <spanclass="math inline">\(\lambda^{-1}\)</span>也是特征值，而且对应特征向量不变</li><li><strong>相似矩阵具有相同的特征多项式，因此具有相同的特征值</strong></li></ol><h3 id="特征向量的性质">特征向量的性质</h3><ol type="1"><li>属于不同特征值的特征向量线性无关</li><li><span class="math inline">\(\mathbf{A}\)</span> 可以对角化 <spanclass="math inline">\(\Leftrightarrow\)</span> <spanclass="math inline">\(\mathbf{A}\)</span> 有 <spanclass="math inline">\(n\)</span> 个线性无关的<strong>特征向量</strong><span class="math inline">\(\Leftrightarrow\)</span> <spanclass="math inline">\(\mathbb{F}^n\)</span> 有一组由 <spanclass="math inline">\(\mathbf{A}\)</span> 特征向量组成的基**<u><strong>(Que?)</strong></u></li></ol><h3 id="对角化主定理">对角化主定理</h3><p>一个 <span class="math inline">\(n\)</span> 阶矩阵可以对角化 <spanclass="math inline">\(\Leftrightarrow\)</span>矩阵的每个特征值代数重数与几何重数相等</p><div class="note note-info">            <p>1.特别地，若矩阵有 <span class="math inline">\(n\)</span>个不同的特征值，则可以对角化</p><p>2.实对称矩阵一定可以正交化。而且对于一个实对称矩阵 <spanclass="math inline">\(\mathbf{A}\)</span> , 存在正交矩阵 <spanclass="math inline">\(\mathbf{P}\)</span> ，使得 <spanclass="math inline">\(\mathbf{A} = P^{-1}DP = P^T\)</span>,即通过正交变换变成对角阵， <spanclass="math inline">\(\mathbf{A}\)</span> 正交相似于对角阵。</p>          </div><p>Note: 此处要填坑，系统复习一下相似矩阵那一章的证明</p><h2 id="矩阵分解">矩阵分解</h2><h3 id="满秩分解">满秩分解</h3><p>任何一个矩阵都可以被分解为列满秩矩阵 <spanclass="math inline">\(\times\)</span> 行满秩矩阵。</p><p>E.g. <span class="math display">\[\mathbf{A}_{n \times m} = \left[\begin{matrix} \mathbf{P}_{n \timesr(\mathbf{A})} &amp; 0 \end{matrix} \right] \left[\begin{matrix}\mathbf{Q}_{r(\mathbf{A}) \times m} \\ 0  \end{matrix}\right]\]</span> 一个比较简单的求解方法是，把 <spanclass="math inline">\(\mathbf{Q}\)</span> 搞成一个Hermitte标准形，这样<span class="math inline">\(\mathbf{P}\)</span> 直接可以得出结果：取矩阵<span class="math inline">\(\mathbf{A}\)</span> 的前<spanclass="math inline">\(r(A)\)</span> 列</p><h3 id="lu分解">LU分解</h3><h3 id="奇异值svd分解">奇异值(SVD)分解</h3><h2 id="线性空间">线性空间</h2><ol type="1"><li>回顾加群(Abel群)</li></ol><ul><li>封闭性</li><li>结合律</li><li>交换律</li><li>有单位元(0元)</li><li>有逆元(-a)</li></ul><ol start="2" type="1"><li><strong>线性空间的直观理解</strong>：<span class="math inline">\((V,+)\)</span> 是一个加群，定义数域 <spanclass="math inline">\(\mathbb{F}\)</span> 上的数乘运算，则 <spanclass="math inline">\((V, + , \cdot)\)</span> 是 <spanclass="math inline">\(\mathbb{F}\)</span> 上的一个线性空间。 <spanclass="math inline">\(\mathbb{F}\)</span> 叫做基域。</li></ol><p><span class="math inline">\((\mathbb{R}^3, +, \cdot)\)</span> 是<span class="math inline">\(\mathbb{R}\)</span>的一个线性空间(有限维度的一个线性空间)</p><p>例如，<span class="math inline">\(f \inc[a,b]\)</span>上的所有连续函数，则 <span class="math inline">\((c[a,b], + , \cdot)\)</span>是一个线性空间(无穷维度)(有泛函的观点了，只要是线性无关的函数就能构成一组基底)。</p><p>例如，矩阵加法和数乘能构成一个线性空间，其一组基为全体基础矩阵<spanclass="math inline">\(\mathbb{E}_{ij}\)</span>。特别地，全体<spanclass="math inline">\(n\)</span> 阶方阵组成<spanclass="math inline">\(n^2\)</span>维线性空间；全体 <spanclass="math inline">\(n \times 1\)</span> 阶矩阵，即全体<spanclass="math inline">\(n\)</span>维向量，构成了<spanclass="math inline">\(\mathbb{F}\)</span>上的一个<spanclass="math inline">\(n\)</span>维线性空间，其一组基由所有的标准向量组成即<span class="math inline">\(e_1, e_2, \dots, e_n\)</span></p><p>3.<strong>线性空间的基向量定义：</strong>若 <spanclass="math inline">\(V\)</span> 中存在<spanclass="math inline">\(n\)</span>个线性无关的向量，使得 <spanclass="math inline">\(V\)</span> 中任意向量都与他们线性相关，则称<spanclass="math inline">\(V\)</span>是<spanclass="math inline">\(n\)</span>线性空间。</p><p>4.<span class="math inline">\(V\)</span>中任意向量均能唯一的表为<spanclass="math inline">\(\alpha\)</span>的线性组合，$ = k_1 _1 + k_2 _2 + +k_n _n =(_1, _2, , _n) k<spanclass="math inline">\(，则称为向量\)</span><spanclass="math inline">\(关于基的坐标\)</span>(k_1, k_2, , k_n)^T$。</p><p>线性空间的基一般不唯一，但线性空间的维数是唯一确定的。所以不同基向量包含的向量个数相同。</p><ol start="5" type="1"><li><strong>基扩充定理</strong>:</li><li>过渡矩阵：设<spanclass="math inline">\(n\)</span>维线性空间的一组基<spanclass="math inline">\((\alpha_1, \alpha_2, \dots,\alpha_n)\)</span>与另一组基<span class="math inline">\((\beta_1,\beta_2, \dots, \beta_n)\)</span>如果存在如下关系:</li></ol><p><span class="math display">\[\left[\begin{matrix} \beta_1&amp; \beta_2 &amp; \dots \beta_n\end{matrix} \right] = \left[\begin{matrix} \alpha_1&amp; \alpha_2 &amp;\dots \alpha_n \end{matrix} \right] \mathbf{P}\]</span></p><p>所以可见，每个列向量 <span class="math inline">\(\beta_i\)</span>都是 <span class="math inline">\(\alpha_j\)</span>的线性组合，组合系数由<spanclass="math inline">\(P_i\)</span>列向量控制。</p><p><img src="https://i.loli.net/2021/10/11/UiXGb2vNnJfgFus.jpg" style="zoom: 33%;" /></p><h2 id="内积空间">内积空间</h2><ol type="1"><li><p>背景：内积的引入和“长度“有关。在线性空间中引入内积的概念，衡量两个向量的“远近”。如果内积归一化后，就是通过两个单位向量的夹角来衡量两个向量的距离。</p></li><li><p><strong>内积空间</strong>：设<spanclass="math inline">\(V是\)</span><spanclass="math inline">\(\mathbb{F}\)</span>上的线性空间，若对<spanclass="math inline">\(V\)</span>中的任意两个向量，都定义了<spanclass="math inline">\(\mathbb{F}\)</span>中的一个数<spanclass="math inline">\((\alpha, \beta)\)</span>,使得满足</p><ul><li><p>(共轭对称性) <span class="math inline">\((\alpha, \beta) =\overline{(\beta, \alpha)}\)</span></p></li><li><p>(正定性）<span class="math inline">\((\alpha, \alpha) \ge0\)</span></p></li><li><p>(双线性) <span class="math inline">\((a \alpha + b \beta，\gamma) = a(\alpha, \gamma) + b(\beta, \gamma)\)</span>(共轭双线性)<span class="math inline">\((\alpha, a \beta + b \gamma) =a(\alpha, \beta) + \overline{b}(\alpha, \beta)\)</span></p><p>则称其为内积空间。</p></li></ul></li><li><p>内积与范数性质</p><p>比较重要的且容易忘记的，Cauchy-Schwards不等式， 三角不等式</p></li><li><p>内积的作用：从代数上，两个向量的角度，两个向量在线性空间中的距离都可以由内积来定义。有了角度和长度，从几何上更好解释向量之间的位置关系。比如两个向量正交是垂直(角度90°)，实数域的线性空间上两个向量线性相关当且仅当夹角为0或<spanclass="math inline">\(\pi\)</span>。</p></li><li><p>Gram-Schmidt正交化方法: 已知线性无关组<spanclass="math inline">\(\alpha_1 , \dots,\alpha_n\)</span>,求标准正交组</p></li></ol><p><span class="math display">\[\beta_k = \alpha_k - \sum_i (\alpha_i, \gamma_i)\gamma_i\]</span></p><p><span class="math display">\[\gamma_k = \frac{\beta_k}{||\beta_k||}\]</span></p><p>几何意义：再求第<spanclass="math inline">\(k\)</span>个标准正交基时，投影到<spanclass="math inline">\(\gamma_1 \dots\gamma_{k-1}\)</span>个标准正交基构成的”超平面“上的投影等于其在各个<spanclass="math inline">\(\gamma_i\)</span>分量上的投影向量之和。所以减去后自然就与<spanclass="math inline">\(\gamma_1 \dots\gamma_{k-1}\)</span>正交，最后再单位化。</p><ol start="6" type="1"><li>酉矩阵<span class="math inline">\(\mathbf{A}^H \mathbf{A} =\mathbf{I}\)</span>, 在实数域上就是正交矩阵<spanclass="math inline">\(\mathbf{A}^T \mathbf{A} =\mathbf{I}\)</span>。该矩阵的每个行(列)向量两两正交，且为单位向量。正交矩阵的集合意义几乎就是旋转变换，利用正交矩阵做旋转变换可以去掉二次型中的交叉项，变成标准形式。<img src="https://i.loli.net/2021/10/11/BWxoCmdMzDG9c12.png" alt="image-20211011195300299" style="zoom: 67%;" /></li></ol><p><img src="https://i.loli.net/2021/10/11/agG4bAcnJCmlEeW.png" alt="image-20211011195358735" style="zoom: 60%;" /></p><h2 id="参考文献">参考文献</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>《线性代数》 第二版 居余马清华大学出版社<a href="#fnref:1" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>《矩阵理论与应用》 张跃辉上海交通大学出版社<a href="#fnref:2" rev="footnote" class="footnote-backref">↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
      <category>矩阵理论</category>
      
    </categories>
    
    
    <tags>
      
      <tag>课程笔记</tag>
      
      <tag>矩阵，线性代数</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>课程笔记-矩阵理论2</title>
    <link href="/posts/84284e51/"/>
    <url>/posts/84284e51/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script><h1id="课程笔记矩阵理论2矩阵与线性变换">课程笔记：矩阵理论(2)——矩阵与线性变换</h1>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
      <category>矩阵理论</category>
      
    </categories>
    
    
    <tags>
      
      <tag>课程笔记</tag>
      
      <tag>矩阵，线性代数</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>课程笔记-矩阵理论(1)——线性代数知识回顾(上)</title>
    <link href="/posts/6e2da84b/"/>
    <url>/posts/6e2da84b/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script><h1id="课程笔记矩阵理论1线性代数知识回顾上">课程笔记：矩阵理论(1)——线性代数知识回顾(上)</h1><p>📚书籍：《矩阵理论与应用》张跃辉 Chap 1</p><h2 id="梦开始的地方">梦开始的地方：</h2><p>约定本课程讨论均在复数域<spanclass="math inline">\(C\)</span>的子域上进行。</p><h2 id="引言-线性代数是什么">引言： 线性代数是什么</h2><blockquote><p>本科阶段的线性代数课程讨论两个相关问题：一个是引入矩阵来解线性方程组，另一个是利用线性方程组来研究矩阵。</p></blockquote><h4 id="矩阵解线性方程组">矩阵解线性方程组</h4><p><span class="math display">\[\begin{align}\textbf{A}x &amp;= b \\x_1 \alpha_1 + x_2 \alpha_2 + \dots + x_n \alpha_n &amp;=  b\end{align}\]</span></p><p>可以通过研究齐次方程组 <span class="math display">\[\textbf{A}x = 0\]</span> 的解结构来来研究 <spanclass="math inline">\(\textbf{A}x=b\)</span>的解结构，任意解可以表达为基础解系的线性组合。</p><details><summary>展开可详细回顾线性方程组求解</summary><p><b>截图自《线性代数》第二版 居余马著 P47 - P48</b></p><img width="680" src="https://i.loli.net/2021/10/10/725C3QEdf1hiDVA.jpg" alt="图1"><img width="680" src="https://i.loli.net/2021/10/10/XlW96OenEbxMIZv.jpg" alt="图2"></details><h4 id="矩阵提供二次型的简洁表达">矩阵提供二次型的简洁表达</h4><p>$$ <span class="math display">\[\begin{align}f(x) &amp;= x^T\textbf{A}x \\&amp;=  x^T [\alpha_1, \alpha_2, \dots, \alpha_n] x \\&amp;= [x^T \alpha_1, x^T\alpha_2, \dots, x^T\alpha_n] x \\&amp;= \sum_j{(x^T\alpha_j)x_j} \\&amp;= \sum_j(\sum_i(x_i\alpha_{ij})x_j) \\&amp;= \sum_{ij}{\alpha_{ij} x_i x_j}\end{align}\]</span> $$</p><p>请熟悉这个形式和上面的推导过程~系数矩阵<spanclass="math inline">\(\textbf{A}\)</span>中的每一项<spanclass="math inline">\(a_{ij}\)</span>对应的<spanclass="math inline">\(x_i\)</span>与<spanclass="math inline">\(x_j\)</span>的乘积的系数。假如<spanclass="math inline">\(\textbf{A}\)</span>是一个对角矩阵的话，那么就是标准型$<em>{i}</em>{ii}x_i^2 <spanclass="math inline">\(，（就好比二维平面上的圆），有交叉项的话好比二维平面的椭圆。\)</span><spanclass="math inline">\(一定是一个对称矩阵，因为\)</span>x_ix_j<spanclass="math inline">\(的系数就是\)</span>x_jx_i$的系数。</p><h3 id="矩阵乘法与二次型的关系">矩阵乘法与二次型的关系</h3><p>如何理解<span class="math inline">\(n\)</span>阶方阵的高次幂<spanclass="math inline">\(\textbf{A}^m\)</span>?</p><p>相似矩阵<span class="math inline">\(\textbf{A} \sim\textbf{B}\)</span>, <span class="math inline">\(\textbf{B} =\textbf{P}^{-1}A\textbf{P}\)</span>。如果利用特征值与特征向量可将<spanclass="math inline">\(\textbf{A} =\textbf{P}^{-1}\textbf{D}\textbf{P}\)</span>，那么<spanclass="math inline">\(\textbf{A}^m =\textbf{P}^{-1}\textbf{D}^m\textbf{P}\)</span>, <spanclass="math inline">\(\textbf{A}^{-1} =\textbf{P}^{-1}\textbf{D}^{-1}\textbf{P}\)</span>。</p><p>此外实对称矩阵可以正交对角化，即存在正交矩阵<spanclass="math inline">\(\textbf{Q}\)</span>，使得 <spanclass="math display">\[\textbf{Q}^{-1}\textbf{A}\textbf{Q} = \textbf{Q}^T \textbf{A} \textbf{Q}= \textbf{D}\]</span> 可以利用坐标变换$x = y $ 将实二次型化为标准型 <spanclass="math display">\[f = \lambda_1y_1^2 + \lambda_2y_2^2 + \lambda_3y_3^2 + \dots +\lambda_ny_n^2\]</span></p><h2 id="矩阵的基础运算及其性质">矩阵的基础运算及其性质</h2><h3 id="共轭转置">共轭转置</h3><ol type="1"><li><span class="math inline">\(\mathbf{A}\)</span>的共轭矩阵记作<spanclass="math inline">\(\bar{\mathbf{A}}\)</span>，<spanclass="math inline">\(\mathbf{A}\)</span>的共轭转置矩阵记作<spanclass="math inline">\(\mathbf{A}^H\)</span>, <spanclass="math inline">\(\mathbf{A}^H=\bar{\mathbf{A}}^T\)</span>当,<spanclass="math inline">\(\mathbf{A} \in \mathbb{R}^{m \timesn}\)</span>时，<spanclass="math inline">\(\bar{\mathbf{A}}=\mathbf{A}^T\)</span></li><li><span class="math inline">\(\textbf{A} =\textbf{A}^H\)</span>的矩阵被称为Hermite矩阵，其中当<spanclass="math inline">\(\mathbf{A} \in \mathbb{R}^{n \timesn}\)</span>是实对称矩阵。</li></ol><h3 id="矩阵乘法">矩阵乘法</h3><p>矩阵乘法可以按照行向量进行, 比如对于<span class="math inline">\(n\times m\)</span>与<span class="math inline">\(m \timesp\)</span>的两个矩阵相乘 $$ <span class="math display">\[\begin{align}\mathbf{A}\mathbf{B}&amp;=\left[ \begin{matrix}\alpha_1^T \\ \alpha_2^T \\ \cdots \\\alpha_n^T  \end{matrix}\right]   \left[ \begin{matrix}x_1, x_2,\cdots,x_p  \end{matrix}\right] \\&amp;=\left[ \begin{matrix}\alpha_1^Tx_1 &amp; \alpha_1^Tx_2 &amp;\cdots &amp; \alpha_1^T x_p \\ \alpha_2^Tx_1&amp; \alpha_2^Tx_2 &amp;\cdots &amp; \alpha_2^T x_p \\\vdots&amp; \vdots &amp; \ddots &amp; \vdots \\\alpha_n^Tx_1 &amp; \alpha_n^Tx_2 &amp; \cdots &amp; \alpha_n^T x_p\end{matrix}\right]\end{align}\]</span> $$前一个矩阵的每一行乘以后一个矩阵的每一列，得到新矩阵的每一个元素。</p><p>其中，当<span class="math inline">\(\mathbf{B}\)</span> 是一个<spanclass="math inline">\(m \times 1\)</span>(列向量)时, <spanclass="math display">\[\begin{align}\mathbf{A}\mathbf{B}&amp;=\left[ \begin{matrix}\alpha_1^T \\ \alpha_2^T \\ \cdots \\\alpha_n^T  \end{matrix}\right]\left[x\right] \\&amp;=\left[ \begin{matrix} \alpha_1^T x\\ \alpha_2^Tx \\ \vdots \\\alpha_n^T x \end{matrix}\right] \\&amp;=\left[ \begin{matrix} a_{11}x_1 + a_{12}x_2 + a_{13}x_3 + \dots +a_{1m} x_m\\ a_{21}x_1 + a_{22}x_2 + a_{23}x_3 + \dots + a_{2m} x_m \\\vdots \\ a_{n1}x_1 + a_{n2}x_2 + a_{n3}x_3 + \dots + a_{nm} x_m\end{matrix}\right] \\&amp;=x_1\left[ \begin{matrix} a_{11} \\ a_{21} \\ \vdots \\ a_{n1}\end{matrix}\right] + \dots + x_m\left[ \begin{matrix} a_{1m} \\ a_{2m}\\ \vdots \\ a_{nm} \end{matrix}\right] \\&amp;= x_1A_1 + x_2 A_2 + \dots + x_m A_m \\&amp;= \left[\begin{matrix}A_1 &amp; A_2 &amp; \cdots &amp; A_n\end{matrix} \right] \left[x \right]\end{align}\]</span></p><p>这里，可以把第一个矩阵摆成<spanclass="math inline">\(n\)</span>行组成的列向量形式，也可以摆成<spanclass="math inline">\(m\)</span>列的行向量的形式。<strong>其中第一种形式比较容易理解，因为这就是高斯消元法中<spanclass="math inline">\(n\)</span>个线性方程组联立在一起的形式</strong>。但是第二种形式特别容易出错，搞晕，因为第二种形式的线性组合是数乘向量求和的形式，多做几遍就熟悉了。</p><div class="note note-primary">            <p><strong>第二种形式的意义：矩阵乘一个列向量相当于矩阵所有列的线性组合。</strong></p>          </div><p>同样的，一个行向量乘矩阵相当于矩阵所有行的线性组合，即 <spanclass="math display">\[y^T \mathbf{A} = y^T \left[\begin{matrix}\alpha_1^T \\ \alpha_2^T \\\cdots \\ \alpha_n^T  \end{matrix} \right] = y_1\alpha_1^T +y_2\alpha_2^T + \dots y_n \alpha_n^T\]</span> 因此矩阵乘法<span class="math inline">\(\mathbf{A}\mathbf{B} =\mathbf{C}\)</span>,可以看做<spanclass="math inline">\(\mathbf{C}\)</span>的每一列，都是<spanclass="math inline">\(\mathbf{A}\)</span>每一列的线性组合，组合系数是<spanclass="math inline">\(\mathbf{B}\)</span>的对应列；<spanclass="math inline">\(\mathbf{C}\)</span>的每一行可以看做是<spanclass="math inline">\(\mathbf{A}\)</span>的每一行的线性组合，组合系数是<spanclass="math inline">\(\mathbf{A}\)</span>的对应行。若<spanclass="math inline">\(\mathbf{A}\mathbf{B}=0\)</span>，那么<spanclass="math inline">\(\mathbf{B}\)</span>的每一列向量都是<spanclass="math inline">\(\mathbf{A}x =0\)</span>的齐次方程组的解。同理，<spanclass="math inline">\(\mathbf{A}\)</span>的每一行也是<spanclass="math inline">\(y^T\mathbf{B}=0\)</span>的齐次方程组的解。</p><h3 id="方阵的多项式">方阵的多项式</h3><p><span class="math display">\[f(\mathbf{A}) = a_0 \mathbf{I} + a_1 \mathbf{A} + a_2\mathbf{A}^2 +\dots + a_m\mathbf{A}^m  \]</span></p><p>称为<spanclass="math inline">\(\mathbf{A}\)</span>的多项式。易知，<spanclass="math inline">\(f(\mathbf{A})g(\mathbf{A}) =g(\mathbf{A})f(\mathbf{A})\)</span>，同一方阵的多项式是可以交换的。</p><h2id="行列式迹伴随矩阵逆秩等性质">行列式，迹，伴随矩阵，逆，秩等性质</h2><h3 id="行列式">行列式</h3><ol type="1"><li><span class="math inline">\(\det{\mathbf{A}} =|\mathbf{A}|\)</span></li><li><span class="math inline">\(|\mathbf{AB}| =|\mathbf{A}||\mathbf{B}|\)</span></li><li>当<span class="math inline">\(|\mathbf{A}| \ne 0\)</span>时， <spanclass="math inline">\(\mathbf{AB} = 0\)</span>必有<spanclass="math inline">\(\mathbf{B}=0\)</span>,<spanclass="math inline">\(\mathbf{AB} = \mathbf{AC}\)</span>必有<spanclass="math inline">\(\mathbf{B}=\mathbf{C}\)</span>(满秩矩阵齐次方程有唯一解，就是0解)。</li><li>求解二阶行列式对角线法，求解三阶行列式用沙路法。求解<spanclass="math inline">\(n\)</span>阶行列式用递推法（如下按照第<spanclass="math inline">\(i\)</span>行展开）<spanclass="math inline">\(\left|\begin{matrix} a_{11} &amp; a_{12} &amp;\dots &amp; a_{1n} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\a_{n1} &amp; a_{n2}&amp; \dots &amp;a_{nn} \end{matrix} \right| = \sum_ja_{ij}A_{ij}\)</span>(代数余子式忘了的自己wiki去)。最后完全展开的结果有<spanclass="math inline">\(n!\)</span>项，每一项都是不同行不同列元素乘积，然后一半是正号，一半是负号。</li><li>n阶方程组有克莱默法则，理论意义上解释了线性方程组和其系数的关系。</li></ol><h3 id="矩阵的迹">矩阵的迹</h3><ol type="1"><li><p>$ = + $</p></li><li><p><span class="math inline">\(\tr{\mathbf{AB}} =\tr{\mathbf{BA}}\)</span>（如果分别是<span class="math inline">\(n\times m\)</span>和<span class="math inline">\(m \timesn\)</span>的矩阵即可）</p></li><li><p><span class="math inline">\(\tr{ \mathbf{A} \mathbf{A}^H} =\sum_{i,j} |a_{ij}|^2\)</span>（所有元素的平方和，故$ = 0 =0$）</p></li></ol><h3 id="矩阵的秩">矩阵的秩</h3><ol type="1"><li>定义：矩阵所有不为0的子式中最高的阶数为矩阵的秩</li><li>如果一个矩阵的秩为1，那么一定存在列向量<spanclass="math inline">\(\alpha, \beta\)</span>, 使得<spanclass="math inline">\(A =\alpha\beta^T\)</span>(列向量x行向量，得到矩阵的每一行都是<spanclass="math inline">\(\alpha_i\beta\)</span>，显然任意两行都线性相关)。那么计算矩阵高次幂时$^m =(<sup>T)</sup>{m-1}^T $</li><li>矩阵和和乘积的秩有如下不等式(<span class="math inline">\(n \times p, p \times m\)</span>)：</li></ol><p><span class="math display">\[\begin{align}r(\mathbf{A+B}) &amp;\le r(\mathbf{A}) + r(\mathbf{B}) \\r(\mathbf{A}) + r(\mathbf{B}) - p \le r(\mathbf{AB}) &amp; \le\min\{r(\mathbf{A}), r(\mathbf{B})\}\end{align}\]</span></p><details><summary>想看证明请展开</summary><p><b>截图自<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="《线性代数》 第二版 居余马 清华大学出版社">[1]</span></a></sup>P6</b></p><img width="680" src="https://i.loli.net/2021/10/11/NOefIEdJkPUxwVt.jpg" alt="图1"></details><h3 id="伴随矩阵逆矩阵">伴随矩阵，逆矩阵</h3><ol type="1"><li><p>伴随矩阵(如果是实矩阵通常用<spanclass="math inline">\(A^*\)</span>，否则常用<spanclass="math inline">\(adj\mathbf{A}\)</span>表示，个人喜好，即便是在复数域上也喜欢用<spanclass="math inline">\(A^*\)</span>表示，敬请理解)。<spanclass="math inline">\(AA^* = A^*A =|A|I\)</span>(证明可参考<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="《线性代数》 第二版 居余马 清华大学出版社">[1]</span></a></sup>中2.2节例题6)</p></li><li><p><span class="math inline">\(\mathbf{A}^{-1} =\frac{A^*}{|A|}\)</span></p></li><li><p><span class="math inline">\((\mathbf{A}^T)^{-1} =(\mathbf{A}^{-1})^T\)</span></p></li><li><p>$ |^{-1}| = ||^{-1}$</p></li><li><p>可逆矩阵与任何矩阵乘积不会改变原矩阵的秩 $r() = r() = r() = r()$</p></li><li><p>矩阵满秩，非奇异，可逆三者概念等价</p></li></ol><h3 id="分块矩阵">分块矩阵</h3><div class="note note-info">            <p>分块矩阵是一种非常好用的技巧。</p>          </div><ol type="1"><li><p>分块矩阵的加法和数乘就不说了🐒</p></li><li><p>分块矩阵乘法：对于$n p , p m <spanclass="math inline">\(的两个矩阵相乘，假设把矩阵\)</span><spanclass="math inline">\(横切\)</span>r<spanclass="math inline">\(刀，竖着切\)</span>s<spanclass="math inline">\(刀(你就想象一下家里切豆腐凉拌),矩阵\)</span><spanclass="math inline">\(横着切\)</span>s<spanclass="math inline">\(刀，竖着切\)</span>t<spanclass="math inline">\(刀，显然分块后也是对应每一行x每一列。最后乘起来得到的是\)</span>rt<span class="math inline">\(块~~油炸豆腐~~矩阵。但由于分块乘法，乘法本身也是矩阵乘法，所以也要满足\)</span><spanclass="math inline">\(块的列数=\)</span>$块的行数。所以在<del>豆腐下锅</del>矩阵分块乘法前，第一个矩阵竖着切的每一刀的宽度要和第二块豆腐横着切的每一刀宽度相同</p><p>(脑子全是香喷喷的铁锅油炸豆腐，浇上味极鲜酱油撒上葱花后热气腾腾地出锅……)</p></li></ol><p>E.g.</p><details><summary>想看例子请戳我，不想看可忽略</summary><p><b>截图自<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="《线性代数》 第二版 居余马 清华大学出版社">[1]</span></a></sup>P82</b></p><img width="680" src="https://i.loli.net/2021/10/11/ktlFSeGERgNmb4B.png" alt="图1"><img width="680" src="https://i.loli.net/2021/10/11/OGQ1cNxFh4J2Uny.png" alt="图1"></details><ol start="3" type="1"><li>分块矩阵的转置(自己想，想不出来wiki去)</li><li>分块矩阵的逆矩阵：</li></ol><p>对角分块阵(假如<spanclass="math inline">\(\mathbf{A}_i\)</span>都可逆) <spanclass="math display">\[\left[\begin{matrix}\mathbf{A}_1 &amp;  \\  &amp;\mathbf{A}_2&amp; \\&amp; &amp;\ddots&amp; \\ &amp;&amp;&amp; \mathbf{A}_k \end{matrix}\right]^{-1} = \left[\begin{matrix} \mathbf{A}^{-1}_1&amp;  \\  &amp;\mathbf{A}^{-1}_2&amp; \\ &amp; &amp;\ddots&amp; \\&amp;&amp;&amp; \mathbf{A}^{-1}_k  \end{matrix}  \right]\]</span> 普通矩阵，分成几块后做乘法，比如设: <spanclass="math display">\[\left[\begin{matrix}\mathbf{A}_{11} &amp; \mathbf{A}_{12}   \\\mathbf{A}_{21} &amp; \mathbf{A}_{22} \\  \end{matrix} \right]^{-1} =\left[\begin{matrix}\mathbf{X} &amp; \mathbf{Y}  \\ \mathbf{U} &amp;\mathbf{V} \\  \end{matrix} \right]\]</span> 可得 <span class="math display">\[\begin{align}\mathbf{A}_{11}\mathbf{X} + \mathbf{A}_{12}\mathbf{U} &amp;= \mathbf{I}\\\mathbf{A}_{11}\mathbf{Y} + \mathbf{A}_{12}\mathbf{V} &amp;= 0 \\\mathbf{A}_{21}\mathbf{X} + \mathbf{A}_{22}\mathbf{U} &amp;= 0 \\\mathbf{A}_{21}\mathbf{Y} + \mathbf{A}_{22}\mathbf{V} &amp;= \mathbf{I}\end{align}\]</span>这样可以把一个高阶的矩阵求逆问题转化成低阶矩阵求逆问题。结合<strong>分治法</strong>和一些<strong>奇奇怪怪</strong>的trick，可以像矩阵乘法一样从<spanclass="math inline">\(O(n^3)\)</span>进行时间复杂度优化。</p><ol start="5" type="1"><li>分块矩阵的初等变换(以<span class="math inline">\(2 \times2\)</span>为例， 以后填坑)</li></ol><p>分块对换阵 <span class="math display">\[\left[\begin{matrix}0 &amp; \mathbf{I} \\\mathbf{I} &amp; 0\end{matrix} \right]\]</span></p><p>分块倍乘阵 <span class="math display">\[\left[\begin{matrix}\mathbf{C}_1 &amp; 0 \\0 &amp; \mathbf{I}\end{matrix} \right]\]</span> 分块倍加阵 <span class="math display">\[\left[\begin{matrix}\mathbf{I} &amp; 0 \\\mathbf{C}_1 &amp; \mathbf{I}\end{matrix} \right]\]</span></p><h2 id="线性方程组">线性方程组</h2><div class="note note-warning">            <p>这一块讲的非常简略，因为学过线代的人大多数这一章掌握的比较牢靠(往往相似矩阵，矩阵分解，二次型那一块快到期末，掌握得通常不太好)。所以省下笔墨只零零星星地提几点。</p>          </div><ol type="1"><li>（<strong>非常重要</strong>）<span class="math inline">\(\{\alpha_1,\alpha_2, \dots, \alpha_m \}\)</span> 线性无关 <spanclass="math inline">\(\Leftrightarrow\)</span> <spanclass="math inline">\(k_1\alpha_1 + k_2\alpha_2 + \dots + k_m\alpha_m =0\)</span>仅有0解</li><li>（<strong>非常重要</strong>）<strong>极大线性无关组</strong>的定义。(此外，极大线性无关组可以作为方程组的一个基础解系)</li><li>（<strong>非常重要</strong>）齐次线性方程组基本定理：基础解系的向量数量=<span class="math inline">\(n -r(\mathbf{A})\)</span>(从这里可以看出齐次方程组的解空间是<spanclass="math inline">\(\mathbb{F}^{n \times m}\)</span>的子空间，正因为系数矩阵有了秩(序)，才导致解空间维度比原来的空间小了，变成了基础解系的向量数量)</li><li>线性方程组基本定理：是否有解(判别条件)；通解和特解。</li><li>具体计算：化简为Hermitte标准形(行阶梯矩阵标准型)。<spanclass="math inline">\(\left[\begin{matrix} \mathbf{I} &amp; \mathbf{A}\\ 0 &amp; 0 \end{matrix} \right]\)</span></li></ol><div class="note note-success">            <blockquote><p>“有的人天生就是战士。他/她不会惧怕一切困难，也不会为一切失败与挫折低头，因为他/她的目标是星辰大海”⭐️</p></blockquote>          </div><h2 id="参考文献">参考文献</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>《线性代数》 第二版 居余马清华大学出版社<a href="#fnref:1" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>《矩阵理论与应用》 张跃辉上海交通大学出版社<a href="#fnref:2" rev="footnote" class="footnote-backref">↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
      <category>矩阵理论</category>
      
    </categories>
    
    
    <tags>
      
      <tag>课程笔记</tag>
      
      <tag>矩阵，线性代数</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔-关于高维空间样本分布稀疏性的联想</title>
    <link href="/posts/d05437cf/"/>
    <url>/posts/d05437cf/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script><h1id="随笔关于高维空间样本分布稀疏性的联想">随笔：关于高维空间样本分布稀疏性的联想</h1><h2 id="背景提要">背景提要</h2><p>在高维的向量空间中，假设有限个数的样本从一个均匀分布随机采样得到的，那么可以证明这些样本大多会倾向于分布在高维球体或立方体的面都附近(Surface)，而且即便是最近的两个样本之间的距离也会非常远，因为高维空间实在太稀疏了。比如在卷积神经网络使用的向量维度通常是1024维或2048维。</p><p>我一直在思考有没有一个形象化的比喻来直观地解释，或者类比一下这个现象。先想到了两个例子，但可能原理不同，也不一定贴切。</p><h2 id="宇宙膨胀">宇宙膨胀</h2><p>现代的宇宙学最大的一个发现就是，我们所处的宇宙是在不断膨胀的，就像一个正在不断被吹胀的气球一样，我们周围的星系在不断离我们远去。但是据说我们的宇宙的维度可能有11维(包括宏观维度和微观物质的维度)，即便我们的宇宙是高维空间，也不像是1024或2048维这么高的维度；而且宇宙空间会纯粹的数学上的向量空间是两个完全不同的系统，即便在某些性质上可能会有相似的交集。</p><p>突然想起一句台词，”我们像星星一样近，我们像星星一样远。我们近得摩擦生电，我们远如天际云端。“</p><h2 id="社交距离人际关系的疏远">社交距离&amp;人际关系的疏远</h2><p>我想到的一个完全不相干，但是又很奇妙的比喻：人际关系的亲疏远近。有个比较普遍的现象：小时候很容易交到朋友和玩伴，建立一段Friendship的障碍没有那么大，小孩子之间的社交距离很近；但是随着我们逐渐长大，经历的事情越来越多，我们的心灵越来越社会化，会越发现建立一段Relationship会变得愈发困难。进入社会工作后，自己身边都是”同事“和陌生人，大家都很忙，彼此不想过度打扰，即便是你感觉再好的朋友距离也会比小时候变远，就好像在一段数轴上均匀分布的5个点可能离得很近，但是如果把这5个点随机撒入几十维的高维球中，那么即便是最近的两个点，其欧氏距离也比一维坐标轴上的距离要更遥远。</p><p>那么比喻的相似点在哪里呢？年龄小的时候，我们所生活在一个低维的世界中，在这个世界中只有”吃饭“，”睡觉“，”写作业“，”玩“等较少的维度中。维度越低的时候，也是我们的心智越稚嫩简单的时候，对陌生人和这个世界容易充满信任。当我们慢慢长大，上大学，读研，工作，建立家庭，我们的生活维度在不断扩张，直到变成一个高维空间。在这个高维空间里，生活和工作中的遇到的人际关系会更加复杂，再也不是在象牙塔里如此的单纯，仿佛整个世界只由几个简单的维度构成。生活在一个高维空间里是很容易疲惫，就好像训练一个高维的神经网络模型一样困难。自然而然，在这个高维空间里，如果把每个人作为这个社会随机采样下的样本，样本之间的距离也会随之自然变远。</p><p>PS:居然查到了有《社会统计学》这门课，不过真正的社会统计学肯定不是像我这般的胡思乱想。</p><p>"无尽的远方,无数的人们,都与我有关。" 希望这句话能带给我一些安慰。</p>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>高维空间， 维度灾难</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随笔：那些在CV里用腻了的话术与说辞</title>
    <link href="/posts/29b99bf4/"/>
    <url>/posts/29b99bf4/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script><h1id="随笔那些在cv里用腻了的话术与说辞">随笔：那些在CV里用腻了的话术与说辞</h1><p>究其根源，深度学习是一个黑盒子(BlackBox)，缺乏一套能够解释清楚深度神经网络的运作机理的数学理论，比如，能否解释深度网络究竟从海量数据中学到了什么，深度网络的反向传播过程的收敛条件，训练深度模型为什么花费那么长时间，究竟有多少时间是对学习真正有用的。由于缺乏理论支撑，很多搞深度学习应用的(如CV,NLP)大多follow这样的研究路线：通过紧跟研究”热点“，读paper参会看别人提出了哪些模型，然后想到一个”灵感“，提出一套自己魔改的结构，抱着试一试的心态做实验发现work了，再来解释一通，就去投稿发paper。所以很多paper(尤其搞CV/NLP+DL)都是从性能结果反向解释自己的神经网络结构，并且很多解释是基于经验和直觉的。</p><p>读多了后，发现有一些话术是比较Common的技巧，就是很多工作都愿意用上这些技巧来提点，写作时用上相似的话术去讲故事，总觉一下就觉得比较有意思。以后有空会继续补充。</p><ol type="1"><li>Multi-Resolution/Hierarchical/Pyramid Sturcture/Extract Feature atdiferent scales</li><li>不同层次/级别的特征如何融合：Feature Fusion</li><li>分类问题/多模态问题/迁移学习问题: Intra-class variance &amp;Inter-class Variance(最小化类内XXX，最大化类间XXX,以保留判别性信息)</li><li>Global Part + Local Part 的多分支设计</li><li>Local Discriminative Part</li><li>粗粒度+细粒度(Coarse + Fine的设计)</li><li>Grouping操作，把相似的归纳到一起(比如聚类，相似度打分)，分类做loss</li><li>Feature Alignment, Domain Alignment, PixelAlignment；各种Alignment</li><li>Meta Learning +各种领域；就近年会议review情况来看，元学习快被用烂了(烂大街的感觉)，基本是所有reviewer都觉得没啥可用的。把他作为主要创新点容易被喷novelty不够。</li></ol><p>(未完待续…)</p>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>CV</tag>
      
      <tag>话术</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>课程笔记：统计学习理论与方法(ELS_Chap2)</title>
    <link href="/posts/3c10486b/"/>
    <url>/posts/3c10486b/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script><h1id="课程笔记统计学习理论与方法els_chap2">课程笔记：统计学习理论与方法(ELS_Chap2)</h1><p>📚书籍：《Elements of Statistical Learning》Chap 2</p><h2 id="符号约定">符号约定</h2><table><thead><tr class="header"><th style="text-align: center;">输入变量(Input Variable)</th><th style="text-align: center;"><span class="math inline">\(X\)</span>,通常随机变量取值是<span class="math inline">\(p\)</span>维随机向量(<spanclass="math inline">\(X\in\mathbb{R}^p\)</span>),其每个分量可用下角标<spanclass="math inline">\(X_i\)</span>表示是该维度特征的变量</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">(随机)变量<spanclass="math inline">\(X\)</span>的第i个观测值</td><td style="text-align: center;"><spanclass="math inline">\(x_i\)</span>, 可以是向量或标量</td></tr><tr class="even"><td style="text-align: center;">模型输出</td><td style="text-align: center;">Quantative Outputs:<spanclass="math inline">\(Y\)</span>, Category: <spanclass="math inline">\(G\)</span>； 模型预测值<spanclass="math inline">\(\hat{Y}\)</span></td></tr><tr class="odd"><td style="text-align: center;">矩阵</td><td style="text-align: center;">大写加粗正体<spanclass="math inline">\(\textbf{X}\)</span>，如<spanclass="math inline">\(N\)</span>个维度为<spanclass="math inline">\(p\)</span>的向量<span class="math inline">\(x_1,x_2, \dots, x_N\)</span>组成<span class="math inline">\(N \timesp\)</span>的矩阵<span class="math inline">\(\textbf{X}\)</span></td></tr><tr class="even"><td style="text-align: center;">向量</td><td style="text-align: center;">约定:普通的<spanclass="math inline">\(p\)</span>维向量表示成<spanclass="math inline">\(x_j\)</span>，不用加粗; 而维度为N时，若加粗为<spanclass="math inline">\(\textbf{x}_i\)</span>，代表的是随机变量第<spanclass="math inline">\(i\)</span>个特征分量的所有观测值(N个观测值)组成的向量。E.g.<span class="math inline">\(x_j^T\)</span>是矩阵<spanclass="math inline">\(\textbf{X}\)</span>的第<spanclass="math inline">\(j\)</span>行，<spanclass="math inline">\(\textbf{x}_i\)</span>是矩阵的第<spanclass="math inline">\(i\)</span>列。所有的向量规定为列向量的形式。</td></tr></tbody></table><h2 id="统计学习的重要原则">统计学习的重要原则</h2><p>Statistical Learning的目标是为了最好的泛化性能(Best GeneralizationProperty)而不是最小的训练误差(Minimum Training Error)</p><h2 id="线性回归">线性回归</h2><ol type="1"><li>设输入是一个<strong>列向量</strong><spanclass="math inline">\(X\)</span>, 其每个维度分量下标是<spanclass="math inline">\(X_i\)</span> <span class="math display">\[\begin{align}\hat{f}(x) &amp;= \sum_{i=1}^{p} \hat{\beta}_iX_i + \hat{\beta}_0 \\                      &amp;= \hat{\beta}^TX \\                      &amp;= X^T\hat{\beta}\end{align}\]</span> 写成矩阵形式，其中$ ^{N p} $</li></ol><p><span class="math display">\[\hat{Y} = \textbf{X} \hat{\beta}\]</span></p><ol start="2" type="1"><li><p>线性回归的误差函数衡量方式——最小二乘 <spanclass="math display">\[\begin{align}L(Y, \hat{f}(x)) &amp;= (Y-\hat{f}(x))^T(Y- \hat{f}(x)) \\&amp;= (Y- \textbf{X}\hat{\beta})(Y- \textbf{X}\hat{\beta}) \\&amp;= L(\hat{\beta})\end{align}\]</span></p></li><li><p>从最小化误差函数的方式，对<spanclass="math inline">\(\hat{\beta}\)</span>求导，得： $$ <spanclass="math display">\[\begin{align}\frac{\partial{L}}{\partial \hat{\beta}} &amp;= -2\textbf{X}^T(Y-\textbf{X}\hat{\beta}) = 0 \\\hat{\beta} &amp;= (X^TX)^{-1}X^TY\end{align}\]</span> $$</p></li><li><p>思考一下两种数据分布情况对线性回归模型影响：</p><ol type="1"><li>训练数据中的每一类都是服从高斯分布，而且每一类的方差一样但是均值不同。</li><li>某一类的训练数据是由多个高斯分布合成的，可能高斯分布的方差和均值都不太一样，</li></ol><p>对于第1种情况线性回归是比较optimal的，但是对于第二种效果比较差。</p><p>而K近邻算法对于第2中情况相对会更suitable.</p><p>(先占坑，后续补上解释)</p><p>如下图所示，先从<span class="math inline">\(N((1,0)^T,\textbf{I})\)</span>和<span class="math inline">\(N((0,1)^T,\textbf{I})\)</span>中分别随机采样10个sample(共计20个)，前十个作为类别1的均值，后十个作为类别2的均值。然后每个类中各生成100个sample，每个sample都是从<spanclass="math inline">\(N(\mu,\frac{\textbf{I}}{5})\)</span>采样，而每次<spanclass="math inline">\(\mu\)</span>是从该类中的10个中等概率(<spanclass="math inline">\(\Pr = \frac{1}{10}\)</span>)抽样得到。</p><p><img src="https://i.loli.net/2021/09/27/4akOVlu9Q8ex6Wc.png" alt="image-20210927135914964" style="zoom:50%;" /></p><p>圆点连线代表的K近邻的Error曲线，两个方块代表三元线性回归的Error值。可以看到:</p><ul><li>随着<spanclass="math inline">\(k\)</span>值变小，K近邻模型自由度<spanclass="math inline">\(N/k\)</span>在变大，模型复杂度在上升。模型逐渐由欠拟合转变为过拟合。后面也会讲解</li><li>可以看到在这种多高斯分布混合采样得到的数据集中，K近邻最好的Error要比线性回归好。</li></ul></li></ol><h2 id="k近邻">K近邻</h2><ol type="1"><li><p>几何意义：</p><blockquote><p>在特征空间中，对每个训练实例点<spanclass="math inline">\(x_i\)</span>,距离该点比其他点更近的所有点组成一个区域，叫做单元(Cell).每个训练实例点都拥有一个单元，所有训练实例点的单元对特征空间构成一个划分。<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="《统计学习方法》第二版 李航">[1]</span></a></sup><img src="https://i.loli.net/2021/09/27/bQgTdCXlYJNnRzW.png" alt="image-20210927144131480" style="zoom:50%;" /></p></blockquote></li><li><p>具体内容参见李航《统计学习方法》Chap3课程笔记(先占坑，后面补上笔记后建立链接🔗)</p></li></ol><h2id="统计决策理论statistical-decision-theory">统计决策理论(StatisticalDecision Theory)</h2><p>根据模型选择(ModelSelection)的理论，通过最小化<strong>期望预测误差</strong>(ExpectedPrediction Error, EPE)来确定模型中的参数选择，从而确定模型。 <spanclass="math display">\[EPE(f) = E(L(Y, \hat{f}(X)))\]</span></p><h3 id="对于回归问题的epe">对于回归问题的EPE</h3><p>对于回归问题(不仅仅是线性回归)，<ahref="">大多采用最小均方误差的方法计算误差</a>: <spanclass="math display">\[\begin{align}EPE(f) &amp;= E(L(Y, \hat{f}(X)) \\             &amp;= E_{X,Y}((Y - \hat{f}(X))^2) \\             &amp;= \int\int (Y-f(X)^2)P(x, y) dxdy \\             &amp;= \int_X \{\int_Y(Y- f(X))^2P(y|x)dy\}P(x) dx \\             &amp;= E_X E_{Y|X}(Y - f(X))^2 \\\end{align}\]</span> 故，若想使得EPE取最小值，只需让对于每个<spanclass="math inline">\(X=x\)</span>, 使得<spanclass="math inline">\(E_{Y|X}\)</span>最小即可(逐点最小化)，可得 <spanclass="math display">\[\begin{align*}E_{Y|X}(Y- \hat{f}^2(X)) &amp;= E_{Y|X}(Y^2 - 2\hat{f}(X)Y +\hat{f}^2(X)) \\&amp;=E(Y^2)- 2\hat{f}(X)E(Y) + \hat{f}^2(X) \\\end{align*}\]</span> 故 <span class="math display">\[\hat{f}(X) = E_{Y|X}(Y) = E(Y | X=x)\]</span> <strong>含义解释</strong>：理论上期望预测误差最小的回归函数模型，<u>在最小二乘误差意义下</u>，每个训练数据当<spanclass="math inline">\(X=x\)</span>时，取<spanclass="math inline">\(Y\)</span>的平均值(<spanclass="math inline">\(E(Y)\)</span>)是能够达到的。</p><p>注意，在这里我们认为所有训练数据的<spanclass="math inline">\(X\)</span>与<spanclass="math inline">\(Y\)</span>是两个独立的随机变量，分别取样自不同的分布中组成了联合概率分布。这是一个重要的assumption，也是统计学习里的基本思路。</p><p>另外<span class="math inline">\(X=x\)</span>可以拓展为<spanclass="math inline">\(X \in\epsilon(x)\)</span>,在某个邻域附近的所有样本的<spanclass="math inline">\(Y\)</span>值取平均。</p><div class="note note-info">            <p>关于期望<span class="math inline">\(E(Y|X)\)</span>和<spanclass="math inline">\(E_X(Y|X=x)\)</span>，条件期望中的角标，随机变量和事件区别，符号大小写的不同意义等细微区别，参见随笔XXX（先占坑），以帮助理解。</p><hr /><p>2021.10.08 简要补充</p><p>首先要区分随机变量和事件，比如对于随机变量<spanclass="math inline">\(X\)</span>, <spanclass="math inline">\(X=x\)</span>是一个事件，即<spanclass="math inline">\(X\)</span>取一个固定的值<strong>这件事情</strong>发生了，所以是一个事件。而随机变量<spanclass="math inline">\(X\)</span>自身是有可能取任何一个值的，只不过有不同的概率(所以为什么随机变量会有概率分布这个概念)。</p><p>所以我们说条件期望时，就是指<spanclass="math inline">\(E(Y|X=x)\)</span>，即在<spanclass="math inline">\(X=x\)</span>事件发生的条件下，<spanclass="math inline">\(Y\)</span>取各种值的可能性的期望。 <spanclass="math inline">\(E(Y|X=x) = \sum_{y \in Y}p(Y=y|X=x)y\)</span>。</p><p>此外事件的独立和变量的独立不是一回事。n个事件的独立要求n个事件两两独立，任意三个独立，任意四个独立……任意n个独立。但是随机变量的独立只需要$P(X_1, X_2, , X_n ) = _i X_i <spanclass="math inline">\(一个条件即可。因为每个随机变量可以取很多值，包含了很多\)</span>X_i= x$的事件，所以显然约束要比事件独立 要强得多。</p><p>在信息论中，信息熵和条件熵，互信息与条件互信息中，条件部分是随机变量<spanclass="math inline">\(X\)</span>还是事件<spanclass="math inline">\(X=x\)</span>是大有讲究的。比如见此图：</p><p><img src="https://i.loli.net/2021/10/08/2Hr4F1ec7hJEKoI.png" alt="image-20211008200426280" style="zoom: 33%;" /></p><p>以互信息<span class="math inline">\(I(X;Y|Z,V)\)</span>和<spanclass="math inline">\(I(X;Y|Z=z,V)\)</span>的关系为例，如下图公式所示<span class="math display">\[I(X;Y|Z,V) = \sum_Z p(Z=z)I(X;y|Z=z, V)\]</span></p><p>对于条件信息熵: $$ <span class="math display">\[\begin{align}H(X|Y) &amp;= \sum_{y \in Y} p(Y=y) H(X|Y=y) \\&amp;= \sum_{y \in Y} p(Y=y) \sum_{x \in X} p(X=x|Y=y)\log\frac{1}{p(X=x|Y=y)} \\&amp;= \sum_{X,Y} p(x,y)\log\frac{1}{p(X=x|Y=y)}\end{align}\]</span> $$</p>          </div><h3 id="对于分类问题的epe">对于分类问题的EPE</h3><p>先做一些符号约定：</p><p>分类问题的输出是离散型(随机)变量，定义其符号为<spanclass="math inline">\(G\)</span>(Group),属于同一类的值在同一个Group里，以此来表示类别信息。 <spanclass="math display">\[\begin{align}EPE &amp;= E(L(G, \hat{G}(X))) \\&amp;= E_X E_{Y|X}(L(G, \hat{G}(X))) \\&amp;= E_X \sum_k  P(Y=G_k| X=x)L(G_k, \hat{G}(X)) \\\end{align}\]</span> 同理，通过逐点最小化，只需要 <span class="math display">\[\hat{G}(X) = \underset{g}{\operatorname{arg\min}}\, {L(G_k,g)P(Y=G_k|X=x)}\]</span>原因比较显然，哪个样本属于类的概率大，并且损失函数对于分类错误的惩罚大小共同决定了EPE。对于二分类+0-1 Loss来说(分类正确L为0，否则L为1)，那么可得 <spanclass="math display">\[\hat{G}(x) = \underset{g \in {0,1}}{\operatorname{arg\max}}\,P(Y=G_g|X=x)\]</span> 这个分类方式被称为<code>贝叶斯分类器</code>（Bayesclassification）: 我们通过<spanclass="math inline">\(P(G_k|X)\)</span>把样本分类到最可能的类别中(classifyto the most probable class)。贝叶斯分配器得到的Error Rate(BayesRate)是所有分类模型中统计学理论上误差最小的。</p><div class="note note-warning">            <p>经过刚才的讲解，读者应该能较为清楚的感受到，统计学习中把<spanclass="math inline">\(X\)</span>,<spanclass="math inline">\(Y\)</span>作为两个独立的随机变量研究他们的联合分布，本质上研究的是数据与Label的<strong><u>相关性</u></strong>信息。即几乎目前绝大多数有监督学习的方法(机器学习,深度神经网络等)本质上都是学习数据与Label的分布和相关性信息。所以说这类模型是数据驱动(data-driven)的，机器并没有理解数据，只是学习到数据的分布而已。</p>          </div><p>k近邻及其MajorityVote选出类别的机制，本质上也是通过训练数据去近似<spanclass="math inline">\(P(Y=G_g|X=x)\)</span>。而当$k ,N ， <spanclass="math inline">\(时，K近邻近似得到\)</span>E(Y |X=x)$。通过这个例子，我们也发现EPE的结果在回归和分类问题上本质上是一致的。</p><h2 id="bias-variance-decomposition">Bias-Variance Decomposition</h2><p>先考虑在某个点$ X=x_0 $上的MSE拆分： <span class="math display">\[\begin{align}MSE(x_0) &amp;= E_\tau[f(x_0) - \hat{y_0}]^2 \\&amp;= E_\tau [f^2(x_0) - 2f(x_0)\hat{y}_0 + \hat{y_0}^2] \\&amp;= E_\tau [\hat{y_0}^2 - 2\hat{y_0}E_\tau(\hat{y_0}) +E_\tau^2(\hat{y_0}) + f^2(x_0) - 2f(x_0)\hat{y_0} +2\hat{y_0}E_\tau(\hat{y_0}) - E_\tau^2(\hat{y_0})] \\&amp;= E_\tau[(\hat{y_0} - E_\tau(\hat{y_0}))^2] + f^2(x_0) -2f(x_0)E_\tau(\hat{y_0}) + E_\tau^2(\hat{y_0}) \\&amp;= E_\tau[(\hat{y_0} - E_\tau(\hat{y_0}))^2] + [f(x_0) -E_\tau(\hat{y_0})]^2\end{align}\]</span></p><p>设训练集为<span class="math inline">\(\tau\)</span>, 函数<spanclass="math inline">\(f(x)\)</span>为准确的预测模型(has no bias),计算<strong><span class="math inline">\(X=x_0\)</span>这一测试点(testpoint)</strong>上的最小均方误差。由于<spanclass="math inline">\(f(x)\)</span>为精准模型，因此其变化只与输入有关，因此在<spanclass="math inline">\(\tau\)</span>的期望上可以看做是常数，就有如上变换。</p><p>其中第一项是预测模型<spanclass="math inline">\(\hat{y_0}\)</span>自身的方差，第二项是预测模型与真实模型之间的Bias。对于方差来说，是由于对计算<spanclass="math inline">\(y_0\)</span>时带来的数据方差(通常假设是<spanclass="math inline">\(N(0,1)\)</span>的高斯噪声，方差由此带来)。对于Bias而言，一方面由于在<spanclass="math inline">\(x_0\)</span>的邻域里采样，另一方面由于预测模型总会有误差(<spanclass="math inline">\(N(0,1)\)</span>高斯噪声的均值)。</p><div class="note note-warning">            <p>注意这里算MSE拆分时的<spanclass="math inline">\(f(x_0)\)</span>与数据集<spanclass="math inline">\(\tau\)</span>中的随机变量<spanclass="math inline">\(Y\)</span>的取值不等价的。因为数据集中的标签<spanclass="math inline">\(Y\)</span>往往充满噪声的，而我们假设理想数据集是满足某种分布<spanclass="math inline">\(f(x)\)</span>，然后再该分布上加上噪声<spanclass="math inline">\(\epsilon\)</span>才得到了数据集中的真实的<spanclass="math inline">\(Y\)</span>。所以算bias时 $ f(x_0) - E_()$是不能写成$ Y - E_() $的。</p><p>后面我们算EPE的拆分时，把数据集中的真实<spanclass="math inline">\(Y\)</span>放了进去，因此拆分中多了关于噪声的方差项。详细见下面介绍。</p>          </div><h3 id="对于线性回归模型-y-xhatbeta-xbeta-epsilon">对于线性回归模型<span class="math inline">\(Y = X\hat{\beta} = X\beta +\epsilon\)</span></h3><p>假设数据集 <span class="math inline">\(\tau\)</span>中<spanclass="math inline">\(Y\)</span>与<span class="math inline">\(X\)</span>成近似线性分布，假设中间差个高斯噪声<spanclass="math inline">\(N(0,I_p)\)</span></p><p>由于 <span class="math display">\[\begin{align}\hat{\beta} &amp;= (X^TX)^{-1}X^Ty \\&amp;= (X^TX)^{-1}X^T(X \beta + \epsilon) \\&amp;= \beta + (X^TX)^{-1}X^T \epsilon\end{align}\]</span></p><p>可得到 <span class="math display">\[E(\hat{\beta}) = \beta \\Var(\hat{\beta}) = Var((X^TX)^{-1}X^T \epsilon) = A^T Var(\epsilon) A =A^TA = Var(XX^T)^{-1} \, ???对吗\]</span> 故知道对于<spanclass="math inline">\(\beta\)</span>的估计<spanclass="math inline">\(\hat{\beta}\)</span>是无偏估计，因而得到在均方误差中线性回归模型满足$E(X)-E(X)=0<span class="math inline">\(即\)</span>f(x_0)-E_((x_0)) = 0$,bias 为0。</p><p>并且在<span class="math inline">\(X=x_0\)</span>这一测试点(testpoint) <span class="math display">\[\hat{y_0} = \hat{x_0}^T\beta + x_0^T\sum_{i=1}^N l_i(x_0)\epsilon_i \\where\,\, l_i(x_0) = (x_0^T(X^TX)^{-1}X^T)_i\]</span> 下面，我们对整个数据集(over training set <spanclass="math inline">\(\tau\)</span>)的<strong>EPE</strong>进行bias-variance拆分。先考虑固定<spanclass="math inline">\(x_0\)</span>, 让<spanclass="math inline">\(y_0\)</span>变 <span class="math display">\[\begin{align}EPE(x_0) &amp;= E_{y_0 | x_0} E_\tau(y_0 - \hat{y_0})^2 \\&amp;= Var(y_0|x_0) + E_\tau ((\hat{y_0} - E_\tau(\hat{y_0})^2)) +(E_\tau \hat{y_0} - x_0^T \beta)^2 \\&amp;= \sigma^2(\epsilon) + E_\tau x_0^T(X^TX)^{-1}x_0\sigma^2(\epsilon) + 0^2\end{align}\]</span></p><h2 id="维度灾难curse-of-dimensionality">维度灾难(Curse ofDimensionality)</h2><p>待填坑</p><h2 id="模型选择与误差model-selection-and-bias">模型选择与误差(ModelSelection and Bias)</h2><h2 id="参考文献">参考文献</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>《统计学习方法》第二版 李航<a href="#fnref:1" rev="footnote" class="footnote-backref">↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
      <category>统计学习理论与方法</category>
      
      <category>教材笔记</category>
      
      <category>Elements of Statistical Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>课程笔记</tag>
      
      <tag>教材笔记</tag>
      
      <tag>统计学习理论与方法</tag>
      
      <tag>ELS</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>课程笔记：强化学习(2)——Bellman方程的动态规划求解</title>
    <link href="/posts/5ee11453/"/>
    <url>/posts/5ee11453/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script><h1id="课程笔记强化学习2bellman方程的动态规划求解">课程笔记：强化学习(2)——Bellman方程的动态规划求解</h1><h2 id="动态规划">动态规划</h2><div class="note note-info">            <p>本节只是非常粗略的把动态规划的要点总结一下，是课堂笔记纲要，并非详细讲解。先占个坑，以后有空再补</p>          </div><ol type="1"><li><p>动态规划需要满足的条件：</p><ol type="i"><li>最优子结构(Optimal substructure):</li></ol><ul><li><p>原问题的最优解一定也是子问题的最优解。例如，一个输入长度为N的序列的某个Scheduling问题（记为T(1，N)) 的最优解，拆分后一定也是其子问题T(1,N/2-1), T(N/2,N)的最优解。</p></li><li><p>问题的最优解可以通过某种形式组合和计算，得到原问题的最优解</p></li></ul><ol start="2" type="i"><li>重叠子问题</li></ol><ul><li>子问题与原问题有同样的format和structure</li></ul><ol start="3" type="i"><li>无后效性</li></ol><ul><li>计算原问题最优解时，不会对子问题最优解产生影响。</li></ul></li></ol><p><div class="note note-success">            <p>马尔科夫链和马尔科夫决策过程满足上述三个条件的，因为:</p><ul><li>Bellman方程将原问题递归的分解为规模更小的子问题</li><li>子问题拥有同样的format和structure</li><li>Que？有疑问，虽然马尔科夫过程强调每个state只与上一个state有关，与其他state无关($P(x_n|x_1,x_2,,x_{n-1}) = P(x_n| x_{n-1})$)，但这是保证MDP无后效性的原因嘛？因为通过迭代的方式求解贝尔曼方程，value早晚会传播到很多时间段以后的。</li></ul>          </div></p><ol start="2" type="1"><li><p>Dynamic Planning in MDP:</p><ol type="i"><li>For prediction(Policy Evaluation):</li></ol><p>Input: MDP $ &lt;S,A,P,R,&gt; $ 和策略 <spanclass="math inline">\(\pi\)</span> or MRP <spanclass="math inline">\(&lt;S, P^\pi, R^\pi, \gamma&gt;\)</span>(没有显示的策略表示，是Implicit Policy) Output: value function <spanclass="math inline">\(v_\pi\)</span> 对PolicyEvaluation的理解：给定一个策略，通过贝尔曼方程迭代求出所有状态的价值<span class="math inline">\(v_\pi(s)\)</span></p><p>ii.For control:</p><p>Input: MDP $ &lt;S,A,P,R,&gt; $</p><p>Output: 最优价值函数 $ v_* $ 和最优策略 $ _* $</p></li><li><p>Bellman方程:</p></li></ol><p><strong>(State-) Value Function:</strong></p><p><img src="https://i.loli.net/2021/09/29/7yIKAVjkGHTbXeU.png" alt="image-20210929174143894" style="zoom:80%;" /><span class="math display">\[\begin{align}  v_\pi(s) &amp;= \sum_{a}\pi(a|s)q_\pi(s,a)\\  &amp;= \sum_a\,\pi(a|s)(R_s^a+\gamma\sum_{s&#39;}(P_{ss&#39;}^a\,v_\pi(s&#39;)))(代入q_\pi(s&#39;,a)公式)  \end{align}\]</span></p><p>求Optimal <spanclass="math inline">\(v_*(s)\)</span>(把求期望操作<spanclass="math inline">\(\rightarrow\)</span>找<spanclass="math inline">\(\operatorname {arg\,max}\)</span>操作)：</p><p><img src="https://i.loli.net/2021/09/29/x9D7TQga6GShrsi.png" alt="image-20210929203919230" style="zoom:80%;" /></p><p>$$ <span class="math display">\[\begin{align}  v_*(s) &amp;= \underset{a}\max\{R_s^a+\gamma\sum_{s&#39;}P_{ss&#39;}^a\,v_\pi(s&#39;)\}  \end{align}\]</span> $$</p><p><strong>Action Function:</strong></p><p><img src="https://i.loli.net/2021/09/29/VMlc1N2g4DPJo5I.png" alt="image-20210929174301352" style="zoom: 80%;" /><span class="math display">\[  \begin{align}  q_*(s, a) &amp;= R_{s}^a + \gamma\sum_{s&#39;}\, P_{ss&#39;}^av_\pi(s&#39;) \\  (or&amp;= \sum_{s&#39;}\, P_{ss&#39;}^a (R_{ss&#39;}^a +v_\pi(s&#39;)) ) (考虑奖励R_s^a是否与s&#39;有关)\\  &amp;=R_s^a +\gamma\sum_{s&#39;}\,P_{ss&#39;}^a\sum_{a&#39;}\pi(a&#39;|s&#39;)q_\pi(s&#39;,a&#39;) (代入v_\pi(s&#39;)公式)\\  \end{align}\]</span> 求Optimal $ q_*(s, a) $ 时(把求期望操作<spanclass="math inline">\(\rightarrow\)</span>找<spanclass="math inline">\(\operatorname {arg\,max}\)</span>操作)：</p><p><img src="https://i.loli.net/2021/09/29/xIVUrmEWBt4Nlzf.png" alt="image-20210929204248905" style="zoom:80%;" /><span class="math display">\[  q_*(s, a) = R_s^a + \gamma\sum_{s&#39;}\,P_{ss&#39;}^a{\underset{a&#39;}\max\,}q_\pi(s&#39;,a&#39;))\]</span></p><h2 id="policy-evaluation">Policy Evaluation</h2><figure><img src="https://i.loli.net/2021/09/29/ghzERuXiC1MtsQp.png"alt="image-20210929211939079" /><figcaption aria-hidden="true">image-20210929211939079</figcaption></figure><p>给定一个policy $ $，评估/衡量在 <spanclass="math inline">\(\pi\)</span> 下每个状态的value function.</p><h2 id="policy-iteration">Policy Iteration</h2><figure><img src="https://i.loli.net/2021/09/29/2THt83175YAOZRJ.png"alt="image-20210929212427612" /><figcaption aria-hidden="true">image-20210929212427612</figcaption></figure><p>这个是通过贪心的策略来迭代得到最好的policy。基本思想是<spanclass="math inline">\(\pi(s) = \underset{a}{\operatorname{arg\,max}}\,q(s, a)\)</span>，即在state function固定情况下，通过贪心的选择给自己带来最大受益的action a来作为自己的策略。可以证明经过多次迭代后，策略能收敛到贪心策略下的最优解上。</p><h2 id="value-iteration">Value Iteration</h2><figure><img src="https://i.loli.net/2021/09/29/NpwbPFi3k52uedH.png"alt="image-20210929213203328" /><figcaption aria-hidden="true">image-20210929213203328</figcaption></figure><p>这个的特点是没有给定任何显示的(explicit)策略<spanclass="math inline">\(\pi\)</span>, 而是只求解state function。</p><p>与Policy Evaluation 和Policy Iteration 不同， 前两者都出现了<spanclass="math inline">\(\pi(a|s)\)</span>这个东西，也就是说每个状态下选择某个动作的概率是被记录的，并且可能随着算法也会迭代更新；而ValueIteration 每次选择受益最大的动作来更新value。</p><p>等迭代结束，value全部求出来后，最后一遍求解<spanclass="math inline">\(\pi\)</span>。</p><h2 id="summary">Summary</h2><figure><img src="https://i.loli.net/2021/09/29/pNR9Q8biXWH2GOq.png"alt="image-20210929213757669" /><figcaption aria-hidden="true">image-20210929213757669</figcaption></figure>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
      <category>强化学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>课程笔记</tag>
      
      <tag>强化学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>课程笔记：算法分析与设计(0)</title>
    <link href="/posts/ffef3022/"/>
    <url>/posts/ffef3022/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script><h1 id="课程笔记算法设计与分析0">课程笔记：算法设计与分析(0)</h1><h2 id="notation">Notation</h2><h3 id="o-notation">O-notation</h3><ol type="1"><li><span class="math inline">\(f(n) = \Omega(g(n))\)</span>: $ c,n_0,whenn n_0, f(n) cg(n)$ (<spanclass="math inline">\(\omega(·)\)</span>严格<spanclass="math inline">\(&gt;\)</span>)</li><li><span class="math inline">\(f(n) = O(g(n))\)</span>: $ c,n_0, whennn_0, f(n) cg(n)$ (<span class="math inline">\(o(·)\)</span>严格<spanclass="math inline">\(&lt;\)</span>)</li><li><span class="math inline">\(f(n) = \Theta(g(n))\)</span>: $ c1,c2,n_0, whenn n_0, c_1g(n) f(n) c_2g(n)$</li></ol><h2 id="高中数学复习回顾">高中数学复习回顾</h2><h3 id="指数函数exponentials">指数函数(Exponentials)</h3><ol type="1"><li><p>泰勒展开(忘了的话自己再推一遍) <span class="math display">\[f(x) = f(x_0) + f&#39;(x_0)(x-x_0) + \frac{f&#39;&#39;(x_0)}{2}(x-x_0)^2+ \frac{f^{(3)}(x_0)}{3!}(x-x_0)^3 + \dots+\frac{f^{(n)}(x_0)}{n!}(x-x_0)^{(n)}\]</span> 常用形式: <span class="math display">\[e^x = 1 + x +\frac{x^2}{2} + \frac{x^3}{3!} + \dots +\frac{x^{(n)}}{n!}+ \dots\]</span> <span class="math inline">\(\ln(1+x)\)</span> 在0点展开，<span class="math inline">\(x \in (-1, 1)\)</span> <spanclass="math display">\[\ln(1+x) = x-\frac{x^2}{2} + \frac{x^3}{3} - \frac{x^4}{4} +\frac{x^5}{5} - \dots + (-1)^{n-1}\frac{x^n}{n}\]</span></p><p><strong>记忆：</strong>$ (1+x)<spanclass="math inline">\(和\)</span>e^x$对比记忆，都是标准形式，不过对数是±交替并且少了1的常数项</p><p><span class="math inline">\(\sin(x)\)</span>在0点展开， <spanclass="math inline">\(x \in R\)</span>: <span class="math display">\[\sin(x) = x- \frac{x^3}{3!} + \frac{x^5}{5!} - \dots +(-1)^{n-1}\frac{x^{(2n-1)!}}{(2n-1)!}\]</span> <strong>记忆</strong>： <spanclass="math inline">\(\sin\)</span>是“奇”函数, 所以是第“1,3,5...”项。<span class="math display">\[cos(x)=1 - \frac{x^2}{2} + \frac{x^4}{4!} - \dots +(-1)^{(n-1)}\frac{x^{2(n-1)}}{(2n-2)!}\]</span></p></li><li><p>关于指数函数的重要不等式(从Taylor直接得到) <spanclass="math display">\[1 + x \le e^x \le 1 + x + x^2(x \rightarrow 0)\]</span></p><p><span class="math display">\[e^x = 1 + x + \Theta(x^2)(x \rightarrow 0)\]</span></p></li><li><p>关于对数函数的重要不等式</p></li></ol><p><span class="math display">\[\frac{x}{1+x} \le \ln(x+1) \le x, x &gt; -1\]</span></p><p>当且仅当<span class="math inline">\(x =0\)</span>时成立</p><h3 id="对数函数logarithm">对数函数(Logarithm)</h3><ol type="1"><li><p>换底公式 <span class="math inline">\(\log_ba =\frac{\log_ca}{\log_cb}\)</span></p></li><li><p>$ a^ {_bc} = a ^ {_ba_ac } = c^{log_ba}$</p></li></ol><h2 id="阶乘factorials">阶乘(Factorials)</h2><ol type="1"><li>Stirling's Approximation</li></ol><p><span class="math display">\[n! = \sqrt{2\pi n}\left(\frac{n}{e} \right)^n\left(1 +\Theta(\frac{1}{n}) \right)\]</span></p><p>不等式(tight upper bound): <span class="math display">\[2^n &lt; n! \le n^n\]</span></p><ol start="2" type="1"><li><p><span class="math display">\[n! = \sqrt{2 \pi n}\left(\frac{n}{e} \right)^n e^{\alpha_n},\frac{1}{12n +1} &lt; \alpha_n &lt; \frac{1}{12n}\]</span></p></li><li><p><span class="math display">\[\lg(n!) = \Theta(n \lg n)\]</span></p></li></ol>]]></content>
    
    
    <categories>
      
      <category>课程笔记</category>
      
      <category>算法分析与设计</category>
      
    </categories>
    
    
    <tags>
      
      <tag>课程笔记</tag>
      
      <tag>算法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Luna--Linear Unified Nested Attention[阅读笔记]</title>
    <link href="/posts/45554f7f/"/>
    <url>/posts/45554f7f/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script><h1id="neural-ips在投2021-luna--linear-unified-nested-attention阅读笔记">(NeuralIPS在投,2021) Luna--Linear Unified Nested Attention[阅读笔记]</h1><h2 id="abstract">Abstract</h2><p>传统Transformer中的注意力机制计算量是平方级别的。本文提出了Luna，LinearUnified Nested Attention的方法，通过增加一个额外的固定长度的序列作为输入和输出，把平方级别的注意力计算拆分成两个线性时间的计算步骤来做近似，并且该固定长度的序列可以存储足够的上下文相关信息(ContexualInfomation)。</p><h2 id="motivation">Motivation</h2><ol type="1"><li>想提出一个简单有效减低计算复杂度的方法<ul><li>传统的注意力机制的计算和存储都是<spanclass="math inline">\(O(n^2)\)</span>的(<spanclass="math inline">\(n\)</span>表示序列的长度)，但是对于长序列输入，序列间相关性往往是稀疏的(不是完全图)</li><li>已有的改进方法： 1)利用稀疏性约束(Sparse Attention)，如localattention<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, andDustin Tran. Image transformer. In *International Conference on Machine Learning*, pages 4055–4064. PMLR, 2018.">[1]</span></a></sup>,strided<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. *arXiv preprint arXiv:1904.10509*, 2019.">[2]</span></a></sup><sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. *arXiv preprint arXiv:2004.05150*, 2020.">[3]</span></a></sup>,Hierarchical GlobalAttention<sup id="fnref:4" class="footnote-ref"><a href="#fn:4" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="Guo, Q., Qiu, X., Liu, P., Shao, Y., Xue, X., &amp; Zhang, Z. (2019). Star-transformer. *arXiv preprint arXiv:1902.09113*.">[4]</span></a></sup>,attentionwith learnablepatterns<sup id="fnref:5" class="footnote-ref"><a href="#fn:5" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Effificient content-based sparse attention with routing transformers. *Transactions of the Association for Computational Linguistics*,9:53–68, 2021.">[5]</span></a></sup>2)利用注意力矩阵的低秩性，如Linformer<sup id="fnref:6" class="footnote-ref"><a href="#fn:6" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="Wang, S., Li, B., Khabsa, M., Fang, H., &amp; Ma, H. L. (2020). Self-attention with linear complexity. *arXiv preprint arXiv:2006.04768*.">[6]</span></a></sup>3)kernel 方法，如LinearTransformer<sup id="fnref:7" class="footnote-ref"><a href="#fn:7" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In *International Conference on Machine Learning*, pages 5156–5165. PMLR, 2020.">[7]</span></a></sup>,Performer<sup id="fnref:8" class="footnote-ref"><a href="#fn:8" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. *arXiv preprint arXiv:2009.14794*, 2020.">[8]</span></a></sup>, Random FeatureAttention<sup id="fnref:9" class="footnote-ref"><a href="#fn:9" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah Smith, and Lingpeng Kong. Random feature attention. In *International Conference on Learning Representations*, 2021. URL https://openreview.net/forum?id=QtTKTdVrFBB.">[9]</span></a></sup></li><li>本文所提出的引入额外固定长度序列的方法简洁优美，且能有效将计算复杂度降低到线性级别。</li></ul></li></ol><h2 id="theoremmodel">Theorem&amp;Model</h2><p><img src="https://i.loli.net/2021/08/20/LA4bgRN5s2cFiqG.png" /> <imgsrc="https://i.loli.net/2021/08/20/FfWyYXRlCiumvqZ.png" /> <imgsrc="https://i.loli.net/2021/08/20/jJdZAOwCgLXbWH8.png" /> <imgsrc="https://i.loli.net/2021/08/20/h8w5AaLgQjYMNDo.png" /> <imgsrc="https://i.loli.net/2021/08/20/pgj62btJTOqGYlZ.png" /></p><h2 id="contributioninnovation">Contribution(Innovation)</h2><p>Under review，暂不分析innovation如何。</p><h2 id="evaluation">Evaluation</h2><p>暂略，待后续自己实验后分析。</p><h2 id="其他">其他</h2><p>不过，本篇的idea和几乎同一时间(2021.06)放到Archiv上的ExternalAttention using Two Linear Layers for VisualTasks<sup id="fnref:10" class="footnote-ref"><a href="#fn:10" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="M. H., Liu, Z. N., Mu, T. J., &amp; Hu, S. M. (2021). Beyond self-attention: External attention using two linear layers for visual tasks. *arXiv preprint arXiv:2105.02358*.URL https://arxiv.org/abs/2105.02358">[10]</span></a></sup>有同工异曲之处，详细可参考知乎<ahref="https://zhuanlan.zhihu.com/p/382961255">介绍</a>。</p><h2 id="参考文献">参考文献</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>Niki Parmar, Ashish Vaswani,Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, andDustinTran. Image transformer. In <em>International Conference on MachineLearning</em>, pages 4055–4064. PMLR, 2018.<a href="#fnref:1" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>Rewon Child, Scott Gray,Alec Radford, and Ilya Sutskever. Generating long sequences with sparsetransformers. <em>arXiv preprint arXiv:1904.10509</em>, 2019.<a href="#fnref:2" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:3" class="footnote-text"><span>Iz Beltagy, Matthew EPeters, and Arman Cohan. Longformer: The long-document transformer.<em>arXiv preprint arXiv:2004.05150</em>, 2020.<a href="#fnref:3" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:4" class="footnote-text"><span>Guo, Q., Qiu, X., Liu, P.,Shao, Y., Xue, X., &amp; Zhang, Z. (2019). Star-transformer. <em>arXivpreprint arXiv:1902.09113</em>.<a href="#fnref:4" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:5" class="footnote-text"><span>Aurko Roy, Mohammad Saffar,Ashish Vaswani, and David Grangier. Effificient content-based sparseattention with routing transformers. <em>Transactions of the Associationfor Computational Linguistics</em>,9:53–68, 2021.<a href="#fnref:5" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:6" class="footnote-text"><span>Wang, S., Li, B., Khabsa,M., Fang, H., &amp; Ma, H. L. (2020). Self-attention with linearcomplexity. <em>arXiv preprint arXiv:2006.04768</em>.<a href="#fnref:6" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:7" class="footnote-text"><span>Angelos Katharopoulos,Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers arernns: Fast autoregressive transformers with linear attention. In<em>International Conference on Machine Learning</em>, pages 5156–5165.PMLR, 2020. <a href="#fnref:7" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:8" class="footnote-text"><span>Krzysztof Choromanski,Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, TamasSarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, etal. Rethinking attention with performers. <em>arXiv preprintarXiv:2009.14794</em>, 2020.<a href="#fnref:8" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:9" class="footnote-text"><span>Hao Peng, Nikolaos Pappas,Dani Yogatama, Roy Schwartz, Noah Smith, and Lingpeng Kong. Randomfeature attention. In <em>International Conference on LearningRepresentations</em>, 2021. URLhttps://openreview.net/forum?id=QtTKTdVrFBB.<a href="#fnref:9" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:10" class="footnote-text"><span>M. H., Liu, Z. N., Mu, T.J., &amp; Hu, S. M. (2021). Beyond self-attention: External attentionusing two linear layers for visual tasks. <em>arXiv preprintarXiv:2105.02358</em>.URL https://arxiv.org/abs/2105.02358<a href="#fnref:10" rev="footnote" class="footnote-backref">↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
      <category>Transformer</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Attention</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Partial FC--Training 10 Million Identities on a Single Machine[阅读笔记]</title>
    <link href="/posts/a35429c/"/>
    <url>/posts/a35429c/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="https://cdn.jsdelivr.net/npm/meting/dist/Meting.min.js"></script><h1id="partial-fc--training-10-million-identities-on-a-single-machine阅读笔记">PartialFC--Training 10 Million Identities on a Single Machine[阅读笔记]</h1><p>[TOC]</p><h2 id="motivation">Motivation</h2><ol type="1"><li><p>传统的DataParallel模式，无法解决大数据集(包含几十万至千万ID,千万至亿个训练样本)训练时，分类器的矩阵参数<spanclass="math inline">\(W_{NK \times d}\)</span>爆显存的问题(<spanclass="math inline">\(N\)</span>是Batch数量，<spanclass="math inline">\(K\)</span>是GPU个数，d是FeatureMap的维度)</p></li><li><p>ModelParallel模式能够有效解决1中问题：通过将矩阵<spanclass="math inline">\(W\)</span>按<spanclass="math inline">\(N\)</span>的维度拆分成多个子矩阵放在不同GPU上即可。但是该模式无法解决<spanclass="math inline">\(logits_{N \times C}\)</span>当<spanclass="math inline">\(C\)</span>很大时爆显存的问题</p></li><li><p>本文提出的解决方式：保留一个Batch中所有的Positive Class，随机采样NegativeClass，并证明采样率较低情况下，仍能保持performance几乎无损。</p></li><li><p>Related Work(只是听作者介绍，没看):</p></li></ol><ul><li><p>HF-softmax(Goodman,2001)<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="Goodman, J. 2001. Classes for fast maximum entropy training. In *IEEE International Conference on Acoustics, Speech,* *and Signal Processing. Proceedings (Cat. No.01CH37221)*, volume 1, 561–564.">[1]</span></a></sup>通过对FeatureMap构建随机hash森林，每次检索最近的class center来获得activeclass subset 缺点： ①class center 存储在RAM中 ②计算featureretrieval也要耗时</p></li><li><p>Softmax Dissection(He etal.2020)<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="He, L.; Wang, Z.; Li, Y.; and Wang, S. 2020. Softmax Dissection: Towards Understanding Intra- and Inter-class Objective for Embedding Learning. In *AAAI Conference on* *Artifificial Intelligence*, 10957–10964.">[2]</span></a></sup>将softmax分成了intra-class objective和inter-classobjective两部分，并且通过减低intra-class objective的计算冗余</p><p>缺点：①难以拓展到其他softmax based方法</p></li></ul><h2 id="theoremmodel">Theorem&amp;Model</h2><ol type="1"><li><p>softmax公式 <span class="math display">\[\sigma (X, i) = \frac{e^{w_i^T X}}{\sum_{j=1}^C e^{w_j^T X}}\]</span>分子可以在GPU-i上算，只要batch大小不会导致爆显存。分母的话每个GPU只需要提供一个scalar，代表一个求和。</p></li><li><p>Model Parallel 在第i块GPU上的算法：</p></li></ol><p><img src="https://i.loli.net/2021/08/15/dgfnUjzxpqQGVke.png" alt="image-20210717092711902" style="zoom:50%;" /></p><p>注解:</p><ul><li>Line 2,<code>allgather</code>是因为同时要使用DataParallel来训练模型</li><li>Line7, <code>allreduce</code>是因为对参数<spanclass="math inline">\(W\)</span>实行ModelParallel,要从不同GPU上reduce<span class="math inline">\(\sum e^{logtits_i}\)</span></li></ul><ol start="3" type="1"><li><p>随机选取Negative Class的方法：</p><p><img src="https://i.loli.net/2021/08/15/MYaywVFGzf9IeEW.png" alt="image-20210717093753863" style="zoom:50%;" /><img src="https://i.loli.net/2021/08/15/1tEZPeDjrLW8Gwi.png" alt="image-20210717093818660" style="zoom:50%;" /></p></li><li></li></ol><h2 id="contribution">Contribution</h2><p>暂略(以后想到了再写)</p><h2 id="evaluation">Evaluation</h2><ol type="1"><li><p>Training Dataset, Validation Dataset, TestingDataset分别是啥？Backbone? 损失函数？优化器？参数设置？ Training DatasetCASIA, MS1MV2, Celab-500k Testing Dataset LFW(CPLFW, CFLFW), CFP-FP,AgeDB30, Backbone: Resnet-50, 100 Mini-batch size = 512, 8 x 2080Ti LR:0.1起步，后面有衰减(不同数据集不一样)</p><p>训练次数：CASIA： 32K ； MS！MV2 180K； Glint360K 600K.</p></li><li><p>自身Ablation</p><p>比较不同的sample rate, 以及all-sample 和只对negative class sample评价指标：Average Cos Diatnce，因为使用了CosFace和ArcFace的损失函数，度量<code>$x_i$</code>,<code>$W_&#123;y_i&#125;$</code>间的余弦距离<img src="https://i.loli.net/2021/08/15/43dKpjCLtfSJ16v.png" alt="image-20210717094807189" style="zoom:50%;" /></p><p><img src="https://i.loli.net/2021/08/15/r8xdvhPjUKLzR7O.png" alt="image-20210717094939172" style="zoom:50%;" />评估在不同的分类数量情况下的内存使用情况 <imgsrc="https://i.loli.net/2021/08/15/ywXRMfK24xJtrHc.png"alt="image-20210717095831255" /></p></li><li><p>和其他同类方法比较</p></li></ol><p><img src="https://i.loli.net/2021/08/15/1GxVTFIYbv9m5s3.png" alt="image-20210717095513744" style="zoom:50%;" /></p><p><img src="https://i.loli.net/2021/08/15/nfsvX9QTZkBRDqa.png" alt="image-20210717095601380" style="zoom:50%;" /></p><ol start="4" type="1"><li><p>和其他不同类方法比较(看总榜)<img src="https://i.loli.net/2021/08/15/vXkL96DZQta4wbH.png" alt="image-20210717095254291" style="zoom:50%;" /></p></li><li><p>该文章的长处和不足 长处： ①在<strong>ID量</strong>特别大的时候，到达十几万甚至百万时，开一个较大的batch-size也不用担心爆GPU显存② 扔掉大部分negative class确实很节约训练时间，而且loss损失不大</p><p>不足: ① 在ID较少的数据集上用处不大(比赛9万ID用不到，sample rate=1)②只是随机的扔掉负样本比较粗糙，能不能有一些Mining HardNegative？(但是要保证时间成本，保证存储Hard Negative Center不需要耗费太多内存等)</p></li><li><p>自己感觉可以改进的地方①其实结合分布式原语操作(primitive)中的ReduceAll，想要得到<spanclass="math inline">\(logits\)</span>只需要统计部分信息，比如Max,Sum等，所以不扔掉负样本也能做。但是可能有些特殊情况还是要用到完整的<spanclass="math inline">\(logits\)</span></p></li></ol><h2 id="其他">其他</h2><p>搜一搜知乎评论，CSDN博客，查一查OpenReview；这些讨论，质疑对我的思维启发非常大的</p><h2 id="参考文献">参考文献</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>Goodman, J. 2001. Classesfor fast maximum entropy training. In <em>IEEE International Conferenceon Acoustics, Speech,</em> <em>and Signal Processing. Proceedings (Cat.No.01CH37221)</em>, volume 1, 561–564.<a href="#fnref:1" rev="footnote" class="footnote-backref">↩︎</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>He, L.; Wang, Z.; Li, Y.;and Wang, S. 2020. Softmax Dissection: Towards Understanding Intra- andInter-class Objective for Embedding Learning. In <em>AAAI Conferenceon</em> <em>Artifificial Intelligence</em>, 10957–10964.<a href="#fnref:2" rev="footnote" class="footnote-backref">↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
      <category>分类器</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Model Parallel</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
